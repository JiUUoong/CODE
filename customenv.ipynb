{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiUUoong/CODE/blob/main/customenv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlPRuBigEhpJ"
      },
      "outputs": [],
      "source": [
        "# 커스텀 환경\n",
        "# 여태는 만들어져있는 환경 가져다씀 (FrozenLake, CartPole, Taxi, PandaReachDense)\n",
        "# 내가 직접 만든 환경에서 알고리즘 학습시킬수 있게\n",
        "# MDP로 그림 그려놓은 내용을 코드로 옮겨적기\n",
        "# 상태가 어떻고, 어떤 액션이 들어오면, 무슨 확률로 보상 주고, 다음 상태 이동"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12번 슬라이드 재활용 로봇이 상호작용할 환경 만들기"
      ],
      "metadata": {
        "id": "WyiubmBjFKX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# +에이전트 가져다 써보기\n",
        "# tfagents; rl baselines"
      ],
      "metadata": {
        "id": "yaHCkyLCFbY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-agents[reverb]"
      ],
      "metadata": {
        "id": "FJZzpTJdFiVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import abc                  # Abstract Base Class (추상적 피상속 클래스) -> abstractmethod\n",
        "# 골뱅이abstractmethod\n",
        "# 밑줄에 함수 이름 적으면\n",
        "# 클래스 상속할 때 해당 함수는 반드시 구현되게 만들 수 있\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# tf-agents\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym    # tf_agents랑 gym 연결\n",
        "\n",
        "from tf_agents.trajectories import time_step as ts  # (s, a, r, s')\n",
        "from tf_agents.trajectories import trajectory       # time_step들의 연속\n",
        "from tf_agents.specs import array_spec        # state 배열, action 배열\n",
        "\n",
        "import random"
      ],
      "metadata": {
        "id": "O7ZGiipAFk5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RecyclerEnv(py_environment.PyEnvironment):\n",
        "  def __init__(self):\n",
        "    # state, action은 어떤 shape를 가진 배열인지 정해주기\n",
        "    # 예를 들어 DQN 만들 때 입력층의 폭을 env.observation_space.n\n",
        "    # bound 제한된\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name=\"action\"\n",
        "    )\n",
        "    # shape=()    # 스칼라\n",
        "    # dtype       # 정수\n",
        "    # 0, 1, 2의 값을 가질 수 있음: 0=search, 1=wait, 2=recharge\n",
        "    self._state_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=0, maximum=1, name=\"observation\"\n",
        "    )\n",
        "    # 배터리 기준 0=low, 1=high\n",
        "    # (1,)        # 스칼라\n",
        "    # 초기 상태\n",
        "    self._state = 1\n",
        "    # 태스크 종료 여부\n",
        "    self._done = False\n",
        "    # discount rate\n",
        "    self._gamma = 0.99\n",
        "    # 12번 슬라이드 알파, 베타 -> 배터리가 얼마나 빨리 소모되는지 반영시킬 값\n",
        "    self._alpha = 0.7\n",
        "    self._beta = 0.95\n",
        "    # 보상\n",
        "    self._reward = 0\n",
        "    return\n",
        "\n",
        "  def _reset(self):       # state = env.reset()\n",
        "    self._state = 1       # 배터리 완충\n",
        "    self._done = False\n",
        "    self._reward = 0\n",
        "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
        "\n",
        "  def _step(self, action):  # 환경이름._step(action)\n",
        "    # 태스크 종료 조건 구현하는 방법\n",
        "    # 누적 합산 보상 (self._reward)에 조건 걸어서 done=True로 두기\n",
        "    # 트레이닝 루프에서 스텝 수 최대 제한 걸어서 알아서 루프에서 나오기\n",
        "\n",
        "    # MDP 그림을 환경으로 구현할때\n",
        "    # [action 선택지 갯수 * state 갯수]\n",
        "    # state 0이고, action이 0일때\n",
        "    # 0, 1, 2 ; search, wait, recharge\n",
        "    if self._state == 0 and action == 0:  # 배터리 얼마 없을 때 search를 하면?\n",
        "      coin = random.random()              # 0~1 사이 난수\n",
        "      # 이 값이랑 베타랑 비교해서 어느 상태로 이동할지 (어떤 보상이 주어질지) 결정\n",
        "      if coin < self._beta:               # 베타의 확률로\n",
        "        # 상태는 안 바뀌고 그대로\n",
        "        self._state = 0\n",
        "        # 보상은 r_search (예. 10)\n",
        "        reward = 10\n",
        "        self._reward += reward\n",
        "        # 아래 transition 코드는 MDP 상에서 한 화살표를 나타냄\n",
        "        return ts.transition(np.array([self._state], dtype=np.int32), reward, discount=self._gamma)\n",
        "      else:                               # 1-베타의 확률로 방전\n",
        "        self._state = 0\n",
        "        reward = -10\n",
        "        self._reward += reward\n",
        "        return ts.transition(np.array([self._state], dtype=np.int32), reward, discount=self._gamma)\n",
        "    # state 0이고, action이 1일때\n",
        "    elif self._state == 0 and action == 1:\n",
        "      self._state = 0\n",
        "      reward = 1\n",
        "      self._reward += reward\n",
        "      return ts.transition(np.array([self._state], dtype=np.int32), reward, discount=self._gamma)\n",
        "    elif self._state == 0 and action == 2:    # 충전\n",
        "      self._state = 1\n",
        "      reward = 0\n",
        "      self._reward += reward\n",
        "      return ts.transition(np.array([self._state], dtype=np.int32), reward, discount=self._gamma)\n",
        "    elif self._state == 1 and action == 0:\n",
        "      coin = random.random()              # 0~1 사이 난수\n",
        "      # 이 값이랑 베타랑 비교해서 어느 상태로 이동할지 (어떤 보상이 주어질지) 결정\n",
        "      if coin < self._alpha:               # 알파의 확률로\n",
        "        # 상태는 안 바뀌고 그대로\n",
        "        self._state = 1\n",
        "        # 보상은 r_search (예. 10)\n",
        "        reward = 10\n",
        "        self._reward += reward\n",
        "        # 아래 transition 코드는 MDP 상에서 한 화살표를 나타냄\n",
        "        return ts.transition(np.array([self._state], dtype=np.int32), reward, discount=self._gamma)\n",
        "      else:                               # 1-알의 확률로 방전\n",
        "        self._state = 0\n",
        "        reward = 10\n",
        "        self._reward += reward\n",
        "        return ts.transition(np.array([self._state], dtype=np.int32), reward, discount=self._gamma)"
      ],
      "metadata": {
        "id": "tpDVYwRdKITs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_env = RecyclerEnv()        # 클래스 인스턴스\n",
        "_env._action_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "dB6QT-dzg2Rt",
        "outputId": "5099ff09-0cdf-429f-9498-eb4356399d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d4e26275f9fe>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecyclerEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# 클래스 인스턴스\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class RecyclerEnv with abstract methods action_spec, observation_spec"
          ]
        }
      ]
    }
  ]
}