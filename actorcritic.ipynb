{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiUUoong/CODE/blob/main/actorcritic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voNfE_Vux4MJ"
      },
      "outputs": [],
      "source": [
        "# actor critic\n",
        "# 하이브리드 알고리즘\n",
        "# 정책 기반으로 개선되는 actor\n",
        "# 가치 함수 기반으로 개선되는 critic\n",
        "# critic의 비평 (Q(s, a)): 방금 actor에 의해 선택된 행동은 얼만큼의 가치가 있다\n",
        "# actor의 행동 선정은 정책을 따름: 이 정책의 개선은 policy gradient theorem을 따름"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym # CartPole 환경 이번엔 AC 알고리즘으로\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers   # import torch.nn as nn; 예를 들면 layers.Dense가 nn.Linear"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv-2XwCFyUEC",
        "outputId": "cf50ebd1-690a-4221-b294-e780508eaff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/dtypes.py:35: DeprecationWarning: ml_dtypes.float8_e4m3b11 is deprecated. Use ml_dtypes.float8_e4m3b11fnuz\n",
            "  from tensorflow.tsl.python.lib.core import pywrap_ml_dtypes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "state = env.reset(seed=seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAbBdg9wzCAv",
        "outputId": "5aa8e46e-52e9-416e-81fe-b7308f1c4e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터\n",
        "GAMMA = 0.99\n",
        "EPS = np.finfo(np.float32).eps.item()   # numpy 상에서 정해져있는 가장 작은 수: 1.0 + eps != 1.0\n",
        "LR = 0.01"
      ],
      "metadata": {
        "id": "URuui-QazXHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한 에피소드에서 최대 버틸 수 있는 스텝 수\n",
        "max_steps = 1000\n",
        "state = env.reset()\n",
        "state.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c-RnZfFziXB",
        "outputId": "2a027dc1-ad43-49f5-8950-4bcb5bb34420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_observations = state.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "# 신경망 모델\n",
        "# actor, critic 각각 신경망 만들기\n",
        "# 입력 층\n",
        "inputs = layers.Input(shape=(n_observations,))\n",
        "# 공통 층\n",
        "n_hidden = 128\n",
        "common = layers.Dense(n_hidden, activation=\"relu\")(inputs)\n",
        "common2 = layers.Dense(n_hidden, activation=\"relu\")(common)\n",
        "# actor critic 각각의 마지막 층\n",
        "# actor\n",
        "actor = layers.Dense(n_actions, activation=\"softmax\")(common)\n",
        "# critic\n",
        "critic = layers.Dense(1)(common)\n",
        "model = keras.Model(inputs=inputs, outputs=[actor, critic])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k4E6WQQ0Qw_",
        "outputId": "07a61676-fa81-4ad5-8d52-7c707c05162b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTH-oPmA3GGT",
        "outputId": "4e1a38e0-d344-4a0d-a952-8efc6f302389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)        [(None, 4)]                  0         []                            \n",
            "                                                                                                  \n",
            " dense_20 (Dense)            (None, 128)                  640       ['input_6[0][0]']             \n",
            "                                                                                                  \n",
            " dense_22 (Dense)            (None, 2)                    258       ['dense_20[0][0]']            \n",
            "                                                                                                  \n",
            " dense_23 (Dense)            (None, 1)                    129       ['dense_20[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1027 (4.01 KB)\n",
            "Trainable params: 1027 (4.01 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=LR)     # torch.optim.Adam()\n",
        "huber_loss = keras.losses.Huber()                       # torch.nn.HuberLoss()"
      ],
      "metadata": {
        "id": "m8IA25CW6bsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keras에서 모델 만들고 학습시키기 준비하는 과정(방법)\n",
        "# 1. 모델 정의, optimizer 정의, loss 정의 -> 이후 model.compile(optimizer, loss)\n",
        "# 2. GradientTape 스코프 안에서 학습 with gradienTape as gt:"
      ],
      "metadata": {
        "id": "Pu-xQ9Hw6xc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기록을 위한 몇개 빈 리스트\n",
        "action_probs = []     # 행동을 고를 확률\n",
        "critic_value = []     # 행동 가치\n",
        "rewards = []          # 보상\n",
        "reward_ma = 0         # reward Moving Average\n",
        "ep = 0                # 지금 몇번째 에피소드 학습 중인지\n",
        "\n",
        "while True:           # 풀릴 때까지 (180스텝)\n",
        "  state = env.reset()\n",
        "  ep_reward = 0       # 이번 에피소드에 얻은 보상\n",
        "  with tf.GradientTape() as tape:\n",
        "    for timestep in range(1, max_steps):\n",
        "      state = tf.convert_to_tensor(state) # torch.tensor(배열이름)\n",
        "      state = tf.expand_dims(state, 0)    # 텐서이름.unsqueeze(0)\n",
        "      # 모델에게 state를 주고 출력값 받기\n",
        "      # 103번 슬라이드\n",
        "      action_dist, q_value = model(state)\n",
        "      # 출력된 정책 분포에 따라 액션 고르기\n",
        "      # action_dist는 0번째 축이 추가되어있음 (1, 2) -> (2,)\n",
        "      # n_actios개의 선택지 중에 샘플링하겠다 샘플링 확률은 p에 따라\n",
        "      action = np.random.choice(n_actions, p=np.squeeze(action_dist))\n",
        "      # 96번 슬라이드 액터 부분 (log (pi))\n",
        "      action_probs.append(tf.math.log(action_dist[0, action]))\n",
        "      # (1, 1)\n",
        "      critic_value.append(q_value[0, 0])\n",
        "      # 104번 슬라이드\n",
        "      state, reward, terminated, truncated = env.step(action)\n",
        "      rewards.append(reward)\n",
        "      ep_reward += reward\n",
        "      # 태스크 종료되면 이번 에피소드 (안쪽 루프)에서 나오기\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "    reward_ma = 0.01 * ep_reward + (1-0.01) * reward_ma\n",
        "    # 에피소드 단위의 내용 return\n",
        "    returns = []\n",
        "    discounted_sum = 0\n",
        "    for r in rewards[::-1]:   # rewards 리스트 거꾸로\n",
        "      discounted_sum = r + GAMMA * discounted_sum\n",
        "      returns.insert(0, discounted_sum)\n",
        "    # return(G_t) 값 정규화 (normalize) sklearn.preprocessing StandardScaler()\n",
        "    returns = np.array(returns)   # list를 numpy 배열로\n",
        "    returns = (returns - np.mean(returns)) / np.std(returns) + EPS\n",
        "    returns = returns.tolist()    # 다시 list로\n",
        "\n",
        "    # 손실 값 계산\n",
        "    history = zip(action_probs, critic_value, returns)\n",
        "    actor_losses = []\n",
        "    critic_losses = []\n",
        "    for log_prob, value, ret in history:\n",
        "      # 오차\n",
        "      diff = ret - value    # G_t - Q(s, a)\n",
        "      actor_losses.append(-log_prob * diff)\n",
        "      critic_losses.append(\n",
        "          huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
        "      )\n",
        "    loss_value = sum(actor_losses) + sum(critic_losses)\n",
        "    grads = tape.gradient(loss_value, model.trainable_variables)          # loss.backward()\n",
        "    # 기울기 계산: 손실 값에 기여하는 변수가 여럿임 : 모델의 파라미터들 (model.trainable_variables)\n",
        "    # 이 각 변수에 대해 미분 (편미분)해서 각 변수의 기여도를 수치화하고\n",
        "    # 그 수치에 비례하게 보정\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))      # optimizer.step()\n",
        "\n",
        "    # 이번 에피소드 관련 모든 기록 초기화\n",
        "    action_probs.clear()\n",
        "    critic_value.clear()\n",
        "    rewards.clear()\n",
        "  # GradienTape 스코프에서 나오기\n",
        "  ep += 1\n",
        "  if ep % 10 == 0:\n",
        "    print(\"보상 이동평균: {:.2f}. 에피소드 {} 학습 중\".format(reward_ma, ep))\n",
        "  if reward_ma > 180:\n",
        "    print(\"학습 완료\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XDB0eXG7LMl",
        "outputId": "a2e31076-29f4-48df-f68b-793dd379cc3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "보상 이동평균: 3.70. 에피소드 10 학습 중\n",
            "보상 이동평균: 7.61. 에피소드 20 학습 중\n",
            "보상 이동평균: 11.06. 에피소드 30 학습 중\n",
            "보상 이동평균: 15.93. 에피소드 40 학습 중\n",
            "보상 이동평균: 18.00. 에피소드 50 학습 중\n",
            "보상 이동평균: 20.83. 에피소드 60 학습 중\n",
            "보상 이동평균: 24.42. 에피소드 70 학습 중\n",
            "보상 이동평균: 25.43. 에피소드 80 학습 중\n",
            "보상 이동평균: 29.31. 에피소드 90 학습 중\n",
            "보상 이동평균: 30.33. 에피소드 100 학습 중\n",
            "보상 이동평균: 38.13. 에피소드 110 학습 중\n",
            "보상 이동평균: 50.34. 에피소드 120 학습 중\n",
            "보상 이동평균: 64.26. 에피소드 130 학습 중\n",
            "보상 이동평균: 87.15. 에피소드 140 학습 중\n",
            "보상 이동평균: 100.66. 에피소드 150 학습 중\n",
            "보상 이동평균: 116.74. 에피소드 160 학습 중\n",
            "보상 이동평균: 121.34. 에피소드 170 학습 중\n",
            "보상 이동평균: 144.75. 에피소드 180 학습 중\n",
            "보상 이동평균: 154.11. 에피소드 190 학습 중\n",
            "보상 이동평균: 145.46. 에피소드 200 학습 중\n",
            "보상 이동평균: 148.91. 에피소드 210 학습 중\n",
            "보상 이동평균: 178.44. 에피소드 220 학습 중\n",
            "학습 완료\n"
          ]
        }
      ]
    }
  ]
}