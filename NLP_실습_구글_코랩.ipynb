{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "g-SIqd41fsmS",
        "nl7r3h21m8Jm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiUUoong/CODE/blob/main/NLP_%EC%8B%A4%EC%8A%B5_%EA%B5%AC%EA%B8%80_%EC%BD%94%EB%9E%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP"
      ],
      "metadata": {
        "id": "g-SIqd41fsmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOshavE7fjVZ",
        "outputId": "ef25d1f2-4cc2-4b03-daab-4cead9f14ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-learning-from-scratch-2'...\n",
            "remote: Enumerating objects: 598, done.\u001b[K\n",
            "remote: Counting objects: 100% (301/301), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 598 (delta 259), reused 247 (delta 247), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (598/598), 29.67 MiB | 21.16 MiB/s, done.\n",
            "Resolving deltas: 100% (372/372), done.\n"
          ]
        }
      ],
      "source": [
        "# 밑바닥부터 시작하는 딥러닝2(한빛미디어)에서 제공하는 깃헙 코드 클론하기\n",
        "!git clone https://github.com/yonghyunk1m/deep-learning-from-scratch-2.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab 한글 폰트 지원 (Matplotlib 등 한글 깨짐현상 방지)\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "# 셀 실행 후 '런타임 - 런타임 다시 시작'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poklQtIY5gPP",
        "outputId": "49be3be5-39d8-4668-f226-fd09f7a3cc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 1s (9,526 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 해당 디렉토리로 이동하기\n",
        "%cd \"/content/deep-learning-from-scratch-2/ch01\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqPhMbKWgqA8",
        "outputId": "4a080774-b2b3-403c-cafe-7180de3a06d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deep-learning-from-scratch-2/ch01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 현재 디렉토리 확인하기\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LteO-7Yrgtlc",
        "outputId": "6bc82559-4c4c-4dfc-b74b-b1607184eaa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deep-learning-from-scratch-2/ch01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CH 01"
      ],
      "metadata": {
        "id": "nl7r3h21m8Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"*** x ***\")\n",
        "x = np.array([0,1,2])\n",
        "print(x)\n",
        "print(f\"class: {x.__class__}\") # 클래스 이름 표시\n",
        "print(f\"shape: {x.shape}\") # 형상\n",
        "print(f\"ndim: {x.ndim}\", end=\"\\n\\n\") # 차원\n",
        "\n",
        "print(\"*** W ***\")\n",
        "W = np.array([[0, 1, 2], [3,4,5]])\n",
        "print(W)\n",
        "print(f\"shape: {W.shape}\\nndim: {W.ndim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPnWEyAys9In",
        "outputId": "e50a0fd4-4cd5-40a1-da98-826bc991f4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** x ***\n",
            "[0 1 2]\n",
            "class: <class 'numpy.ndarray'>\n",
            "shape: (3,)\n",
            "ndim: 1\n",
            "\n",
            "*** W ***\n",
            "[[0 1 2]\n",
            " [3 4 5]]\n",
            "shape: (2, 3)\n",
            "ndim: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Element-wise Operation\n",
        "import numpy as np\n",
        "\n",
        "W = np.array([[0, 1, 2], [3, 4, 5]]) # W.shape == (2,3)\n",
        "X = np.array([[1, 1, 1], [2, 2, 2]]) # Likewise\n",
        "print(W + X) # Element-wise Addition\n",
        "print(W * X, end=\"\\n\"*2) # Element-wise Multiplication (Hadamard Product)\n",
        "\n",
        "# 뺼셈(-), 나눗셈(/), 몫(//), 모듈로(%) 연산 모두 가능합니다.\n",
        "print(W - X)\n",
        "print(W / X)\n",
        "print(W // X)\n",
        "print(W % X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig-YZUCusTKL",
        "outputId": "e609ce84-13d4-46a8-ea39-8e86ed41b0f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [5 6 7]]\n",
            "[[ 0  1  2]\n",
            " [ 6  8 10]]\n",
            "\n",
            "[[-1  0  1]\n",
            " [ 1  2  3]]\n",
            "[[0.  1.  2. ]\n",
            " [1.5 2.  2.5]]\n",
            "[[0 1 2]\n",
            " [1 2 2]]\n",
            "[[0 0 0]\n",
            " [1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array((2, 3, 4))\n",
        "print(np.dot(a,b))\n",
        "print(a*b)\n",
        "print(a@b)\n",
        "A = np.array(((1, 2), (3, 4)))\n",
        "B = np.array([[1, 1], [2, 2]])\n",
        "print(np.matmul(A, B))\n",
        "print(A@B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8XED7cjyJp5",
        "outputId": "a6254dc0-71ba-401b-f004-f0cbf629769b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "[ 2  6 12]\n",
            "20\n",
            "[[ 5  5]\n",
            " [11 11]]\n",
            "[[ 5  5]\n",
            " [11 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망 구현\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1+ np.exp(-x))\n",
        "\n",
        "N = 10 # Batch Size\n",
        "D = 2 # Input data Dimension\n",
        "H = 4 # Hidden Layer Dimension\n",
        "C = 3 # Output Layer Dimension (# of Class)\n",
        "\n",
        "## Initialization\n",
        "x = np.random.randn(N, D) # Input\n",
        "W1 = np.random.randn(D, H) # Weight for Input layer - Hidden Layer\n",
        "b1 = np.random.randn(H) # Bias for Hidden Layer\n",
        "W2 = np.random.randn(H, C) # Weight for Hidden Layer - Output Layer\n",
        "b2 = np.random.randn(C) # Bias for Output Layer\n",
        "\n",
        "## Calculation (Feed Forward)\n",
        "h = np.matmul(x, W1) + b1\n",
        "a = sigmoid(h)\n",
        "y = np.matmul(a, W2) + b2\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCfcq1mtBsjJ",
        "outputId": "31ac8ef7-8096-4464-c7d7-ad0de29441ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.46358604  0.04551072 -0.97566835]\n",
            " [-1.15431081  0.32379967 -1.00469801]\n",
            " [-1.54828477 -0.03972186 -0.72620183]\n",
            " [-0.7926285   0.70483317 -0.02465118]\n",
            " [-0.16274608  1.17760898 -0.55540396]\n",
            " [-0.99218446  0.58545701  0.10612933]\n",
            " [-1.32180563  0.17265955 -0.93298811]\n",
            " [-1.06605818  0.314883   -1.63018241]\n",
            " [-0.96414016  0.5226594  -0.30808656]\n",
            " [-0.07863553  1.41634239  0.2369875 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax 구현\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "  tmp = np.exp(x)\n",
        "  sum = np.sum(tmp)\n",
        "  return tmp/sum\n",
        "\n",
        "x = np.array([2.560, 0.817, -2.332])\n",
        "y = softmax(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLb3VHHiziEu",
        "outputId": "bc40cff5-e91a-4e52-c5e8-9efe4fa66c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.84566523 0.14798687 0.0063479 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid와 Affine 계층 구현\n",
        "\n",
        "import numpy as n\n",
        "\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.params = []\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.params = [W, b]\n",
        "\n",
        "  def forward(self, x):\n",
        "    W, b = self.params\n",
        "    output = x @ W + b\n",
        "    return output"
      ],
      "metadata": {
        "id": "O9EvAZD35W3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax2D(x): # 2차원 행렬에서 각 행마다 소프트맥스 함수를 가함\n",
        "  y = np.zeros_like(x)\n",
        "  for idx, sample in enumerate(x):\n",
        "    y[idx] = softmax(sample)\n",
        "  return y"
      ],
      "metadata": {
        "id": "7V-VkTVXACbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SampleNet 구현\n",
        "import numpy as np\n",
        "\n",
        "class SampleNet:\n",
        "\n",
        "  # Constructor (생성자)\n",
        "  def __init__(self, I, H, O):\n",
        "    I, H, O = I, H, O # The size of Input/Hidden/Output layer\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    W1 = np.random.randn(I, H) # Input to Hidden\n",
        "    b1 = np.random.randn(H) # Hidden\n",
        "    W2 = np.random.randn(H, O) # Hidden to Output\n",
        "    b2 = np.random.randn(O) # Output\n",
        "\n",
        "    # Layers Construction\n",
        "    self.layers =[Affine(W1,b1), Sigmoid(), Affine(W2, b2)]\n",
        "\n",
        "    # Gathering Parameters\n",
        "    self.params = []\n",
        "    for layer in self.layers:\n",
        "      self.params += layer.params\n",
        "\n",
        "  # Forward Propagation (순전파)\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer.forward(x) # 각 클래스의 forward method를 호출\n",
        "    return x\n",
        "\n",
        "# SampleNet 예시 실행\n",
        "x = np.random.randn(10, 2) # N, I\n",
        "model = SampleNet(2, 4, 3) # I, H, O\n",
        "y = softmax2D(model.predict(x)) # 구현한 softmax 함수의 활용\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQa2CHUg74lS",
        "outputId": "14a0a39e-2c02-45b3-dd42-e548e785eb70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8.17683237e-03 9.90918464e-01 9.04703366e-04]\n",
            " [7.24731498e-03 9.91905833e-01 8.46851750e-04]\n",
            " [4.69394669e-03 9.94355082e-01 9.50971554e-04]\n",
            " [9.90920504e-03 9.89096129e-01 9.94666149e-04]\n",
            " [8.71877226e-03 9.90333522e-01 9.47705403e-04]\n",
            " [1.24799108e-02 9.86409439e-01 1.11065066e-03]\n",
            " [5.64476655e-03 9.93467059e-01 8.88174468e-04]\n",
            " [7.29276146e-03 9.91828896e-01 8.78342596e-04]\n",
            " [5.01867832e-03 9.94042373e-01 9.38949158e-04]\n",
            " [1.04196426e-02 9.88519890e-01 1.06046743e-03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MatMul 노드\n",
        "# x = (N, D), W = (D, H), y = (N, H)\n",
        "\n",
        "class MatMul:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.x = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    W, _ = self.params\n",
        "    out = np.matmul(x, W)\n",
        "    self.x = x\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    W, _ = self.params\n",
        "    dx = np.matmul(dout, W.T)\n",
        "    dW = np.matmul(self.x.T, dout)\n",
        "    self.grads[0][...] = dW # 생략기호(...)를 통한 넘파이 배열의 덮어쓰기.\n",
        "                            #(깊은 복사) 메모리 주소는 미변경, 값이 복제됨.\n",
        "    return dx"
      ],
      "metadata": {
        "id": "T2WtcZy2QjMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 분기노드의 순전파와 역전파 구현 예시\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "N, D = 10, 3\n",
        "\n",
        "x = np.random.randn(1, D) # 입력\n",
        "y = np.repeat(x, N, axis=0) # 순전파 (분기)\n",
        "\n",
        "dy = np.random.randn(N, D) # 예시로 무작위의 y에 대한 기울기 생성\n",
        "dx = np.sum(dy, axis=0, keepdims=True) # 역전파 (분기)\n",
        "\n",
        "print(dy) # (N, D)\n",
        "print(dx) # (1, D)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDv9wUg-OseO",
        "outputId": "399af092-f41c-4e8c-c2c6-060074f3dde9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.67181025  0.32543582 -0.05223489]\n",
            " [-0.50946011 -1.47095469 -0.96766139]\n",
            " [ 0.24706563 -0.62562215  1.2942793 ]\n",
            " [ 1.16147928 -1.51564546  1.95354537]\n",
            " [-0.43546271 -0.44178456 -0.37072699]\n",
            " [-1.16086449 -1.18972719  0.98111669]\n",
            " [ 0.52689056 -0.20720351 -0.46264796]\n",
            " [-0.00507831  0.32987059  0.04617314]\n",
            " [-1.05818905  0.38628594 -0.3003998 ]\n",
            " [-0.4691678   0.49204841  0.91496874]]\n",
            "[[-2.37459724 -3.91729681  3.03641221]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads, self.out = [], [], None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.out = 1 / (1 + np.exp(-x))\n",
        "    return self.out\n",
        "\n",
        "  def backward(self, dout): # 시그모이드의 미분을 반영: y(1-y)\n",
        "    return dout * self.out * (1.0 - self.out)"
      ],
      "metadata": {
        "id": "dMgSb0ocVymn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.params = [W, b]\n",
        "    self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "    self.x = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    W, b = self.params\n",
        "    output = x @ W + b\n",
        "    self.x = x\n",
        "    return output\n",
        "\n",
        "  def backward(self, dout):\n",
        "    W, b = self.params\n",
        "    dx = np.matmul(dout, W.T) # dout @ W.T\n",
        "    dW = np.matmul(self.x.T, dout)\n",
        "    db = np.sum(dout, axis=0)\n",
        "\n",
        "    self.grads[0][...] = dW\n",
        "    self.grads[1][...] = db\n",
        "    return dx"
      ],
      "metadata": {
        "id": "vHb9LkSPkGoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD(Stochastic Gradient Descent; 확률적경사하강법)\n",
        "\n",
        "class SGD:\n",
        "  def __init__(self, lr=0.001):\n",
        "    self.lr = lr\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    for i in range(len(params)): params[i] -= self.lr * grads[i]"
      ],
      "metadata": {
        "id": "fK1Qh8GkpnPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spiral Dataset으로 Multi-class 분류신경망을 학습시켜보자.\n",
        "\n",
        "import sys\n",
        "sys.path.append('..') # 부모 디렉토리도 시스템이 바라볼 수 있게 지정\n",
        "from dataset import spiral # 데이터셋 폴더 안에 있는 spiral.py을 가져오기\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x, t = spiral.load_data()\n",
        "print(f\"x.shape: {x.shape}\") # (300, 2): 2차원 데이터를 300개 가져옴\n",
        "print(f\"t.shape: {t.shape}\") # (300, 3): 3차원 데이터를 300개 가져옴\n",
        "print(f'x[0]: {x[0]}, t[0]: {t[0]}') # 0번째 인덱스의 데이터 값과 클래스 정답 레이블 (원핫벡터) 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "RHbliG1-pnnf",
        "outputId": "6c0ca6ad-e682-443f-a9a2-7aba229250ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b99c48215f85>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 부모 디렉토리도 시스템이 바라볼 수 있게 지정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspiral\u001b[0m \u001b[0;31m# 데이터셋 폴더 안에 있는 spiral.py을 가져오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터점 플롯\n",
        "N = 100\n",
        "CLS_NUM = 3\n",
        "markers = ['o', 'x', '^']\n",
        "for i in range(CLS_NUM):\n",
        "    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jNzi2ScspnpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스파이럴 데이터셋을 분류하는 은닉층 한 개의 신경망 구현\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.layers import Affine, Sigmoid, SoftmaxWithLoss\n",
        "\n",
        "class TwoLayerNN:\n",
        "  def __init__(self, I, H, O):\n",
        "    W1 = 0.01 * np.random.randn(I, H); b1 = np.zeros(H)\n",
        "    W2 = 0.01 * np.random.randn(H, O); b2 = np.zeros(O)\n",
        "\n",
        "    self.layers = [Affine(W1, b1), Sigmoid(), Affine(W2, b2)]\n",
        "    self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in self.layers:\n",
        "      self.params += layer.params; self.grads += layer.grads;\n",
        "\n",
        "  def predict(self, x): # Score를 예측\n",
        "    for layer in self.layers: x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x, t): # Score로부터 Softmax와 CEE를 거쳐 Loss 구하기\n",
        "    score = self.predict(x) # predict 메소드 사용\n",
        "    loss = self.loss_layer.forward(score, t) # score와 정답 레이블을 넣어 loss 구하기\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    dout = self.loss_layer.backward(dout)\n",
        "    for layer in reversed(self.layers):\n",
        "      dout = layer.backward(dout)\n",
        "    return dout"
      ],
      "metadata": {
        "id": "8kDu0vrutcyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 해당 코드롤 학습을 수행\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.optimizer import SGD\n",
        "from dataset import spiral\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameter Setting\n",
        "max_ep = 3000\n",
        "N = 30 # Batch Size\n",
        "H = 10 # Hidden Size\n",
        "lr = 0.1\n",
        "\n",
        "# Data import, Generate model and optimizer\n",
        "x, t = spiral.load_data() # Input data, answer (label)\n",
        "\n",
        "model = TwoLayerNN(I=2, H=H, O=3)\n",
        "optimizer = SGD(lr=lr)\n",
        "\n",
        "# Initialize the variables using in the training stage\n",
        "data_size = len(x)\n",
        "max_iters = data_size // N\n",
        "total_loss, loss_count = 0, 0\n",
        "loss_list = []\n",
        "\n",
        "for epoch in range(max_ep): # 에폭: 학습 단위 (1 에폭이 지나면 학습 데이터를 한번 다 살펴본 것)\n",
        "  # Data shuffling\n",
        "  idx = np.random.permutation(data_size) # 인덱스가 뒤섞인 리스트\n",
        "  x, t = x[idx], t[idx] # 뒤섞인 인덱스로 x와 t를 재배열하기\n",
        "\n",
        "  for i in range(max_iters):\n",
        "    batch_x, batch_t = x[i*N:(i+1)*N], t[i*N:(i+1)*N] # (N,2), (N,3)\n",
        "    loss = model.forward(batch_x, batch_t) # 순전파 (손실 구하기)\n",
        "    model.backward() # 역전파 (기울기 구하기)\n",
        "    optimizer.update(model.params, model.grads) # Gradient Descent (가중치 갱신)\n",
        "\n",
        "    total_loss += loss\n",
        "    loss_count += 1\n",
        "\n",
        "    # 정기적으로 학습 경과를 출력하기\n",
        "    if (epoch+1) % 50 == (i+1) % 10 == 0: # 10개의 배치를 통해 얻은 로스의 평균을 활용\n",
        "      avg_loss = total_loss / loss_count\n",
        "      print(f\"epoch: {epoch+1}, iteration: {i+1}/{max_iters}, loss: {avg_loss:.2f}\")\n",
        "      loss_list.append(avg_loss) # 이제까지의 로스를 기록하기\n",
        "      total_loss, loss_count = 0, 0 # 초기화!"
      ],
      "metadata": {
        "id": "Q0LxhMHYpnqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 결과 플롯\n",
        "plt.rcParams['font.family'] = 'NanumBarunGothic' # 나눔바른고딕 적용하기\n",
        "\n",
        "plt.plot(np.arange(len(loss_list)), loss_list, label='train')\n",
        "plt.xlabel('반복 (x10)')\n",
        "plt.ylabel('손실')\n",
        "plt.show()\n",
        "\n",
        "# 경계 영역 플롯\n",
        "h = 0.001\n",
        "x_min, x_max = x[:, 0].min() - .1, x[:, 0].max() + .1\n",
        "y_min, y_max = x[:, 1].min() - .1, x[:, 1].max() + .1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "X = np.c_[xx.ravel(), yy.ravel()]\n",
        "score = model.predict(X)\n",
        "predict_cls = np.argmax(score, axis=1)\n",
        "Z = predict_cls.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z)\n",
        "plt.axis('off')\n",
        "\n",
        "# 데이터점 플롯\n",
        "x, t = spiral.load_data()\n",
        "N = 100\n",
        "CLS_NUM = 3\n",
        "markers = ['o', 'x', '^']\n",
        "for i in range(CLS_NUM):\n",
        "    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X4xEDu1r5fK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# common/trainer.py의 Trainer 클래스를 활용\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.optimizer import SGD\n",
        "from common.trainer import Trainer # Trainer 클래스 불러오기\n",
        "from dataset import spiral\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameter Setting\n",
        "max_ep = 3000\n",
        "N = 30 # Batch Size\n",
        "H = 10 # Hidden Size\n",
        "lr = 0.1\n",
        "\n",
        "x, t = spiral.load_data()\n",
        "model = TwoLayerNN(I=2, H=H, O=3)\n",
        "optimizer = SGD(lr=lr)\n",
        "\n",
        "trainer = Trainer(model, optimizer)\n",
        "trainer.fit(x, t, max_ep, N, eval_interval=50)\n",
        "trainer.plot()"
      ],
      "metadata": {
        "id": "kVpN5YFJ5LCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 비트 정밀도의 변경\n",
        "\n",
        "import numpy as np\n",
        "original = np.random.randn(3)\n",
        "print(original.dtype) # 64비트로 float 자료형을 나타내고 있다. (비효율적)\n",
        "\n",
        "new = original.astype(np.float32) # 이렇게 32비트로 자료형을 변경\n",
        "print(new.dtype)\n",
        "\n",
        "new2 = np.random.randn(3).astype('f') # 혹은 'f'로 인자를 넣어줄 수 있다.\n",
        "print(new2.dtype)"
      ],
      "metadata": {
        "id": "Ee8BBSjhmqQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CH 02"
      ],
      "metadata": {
        "id": "1-6EQQwsuqDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 아주 간단한 텍스트 전처리 과정 (Text -> Word -> Word ID List)\n",
        "\n",
        "text = \"I am a boy and you are a girl.\" # 주어진 텍스트\n",
        "\n",
        "text = text.lower() # 대문자를 모두 소문자로 취급\n",
        "text = text.replace('.', ' .') # 문장 끝에 위치하는 온점을 단어 girl과 공백으로써 분리시키기 위함\n",
        "print(text)\n",
        "\n",
        "words = text.split(' ') # 공백을 기준으로 텍스트를 단어로 쪼개서 리스트의 아이템으로 넣음\n",
        "print(words)\n",
        "\n",
        "word2id, id2word = {}, {}\n",
        "\n",
        "for word in words:\n",
        "  if word not in word2id: # word2id가 새로 발견한 word라면 -> word2id와 id2word에 모두 등록\n",
        "    new_id = len(word2id) # 새로운 word에 새로운 integer id를 부여\n",
        "    word2id[word] = new_id\n",
        "    id2word[new_id] = word\n",
        "\n",
        "print(word2id)\n",
        "print(id2word)\n",
        "\n",
        "import numpy as np\n",
        "corpus = list(word2id[word] for word in words) # 단어 목록 -> 단어 id 목록\n",
        "corpus = np.array(corpus)\n",
        "print(corpus)"
      ],
      "metadata": {
        "id": "m5gWxsX3uvRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess 함수로 제작하기\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def preprocess(text):\n",
        "  word2id, id2word = {}, {}\n",
        "\n",
        "  text = text.lower()\n",
        "  text = text.replace('.', ' .')\n",
        "  words = text.split(' ')\n",
        "\n",
        "  for word in words:\n",
        "    if word not in word2id:\n",
        "      new_id = len(word2id)\n",
        "      word2id[word] = new_id\n",
        "      id2word[new_id] = word\n",
        "\n",
        "  corpus = np.array(list(word2id[word] for word in words))\n",
        "  return corpus, word2id, id2word\n",
        "\n",
        "text = \"I am a boy and you are a girl.\"\n",
        "corpus, word2id, id2word = preprocess(text)\n",
        "\n",
        "print(f\"text: {text}\")\n",
        "print(f\"corpus: {corpus}\")\n",
        "print(f\"word2id: {word2id}\")\n",
        "print(f\"id2word: {id2word}\")"
      ],
      "metadata": {
        "id": "K3h99NcAxSp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a427ae38-08f6-4a26-a173-f0d11ad63f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: I am a boy and you are a girl.\n",
            "corpus: [0 1 2 3 4 5 6 2 7 8]\n",
            "word2id: {'i': 0, 'am': 1, 'a': 2, 'boy': 3, 'and': 4, 'you': 5, 'are': 6, 'girl': 7, '.': 8}\n",
            "id2word: {0: 'i', 1: 'am', 2: 'a', 3: 'boy', 4: 'and', 5: 'you', 6: 'are', 7: 'girl', 8: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def corpus2comatrix(corpus, vocab_size, window_size = 1):\n",
        "  corpus_size = len(corpus)\n",
        "  comatrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "  print(f\"corpus: {corpus}\\n\")\n",
        "\n",
        "  for idx, word_id in enumerate(corpus):\n",
        "    for i in range(1, window_size+1):\n",
        "      left_idx, right_idx = idx-i, idx+i\n",
        "\n",
        "      if left_idx >=0:\n",
        "        left_word_id = corpus[left_idx]\n",
        "        comatrix[word_id, left_word_id] += 1\n",
        "\n",
        "      if right_idx < corpus_size:\n",
        "        right_word_id = corpus[right_idx]\n",
        "        comatrix[word_id, right_word_id] += 1\n",
        "\n",
        "  #print(\"* Co-matrix *\")\n",
        "  return comatrix\n",
        "\n",
        "print(corpus2comatrix(preprocess(\"I like Nubzuki and you like Kumdori.\")[0], 7))"
      ],
      "metadata": {
        "id": "bg5eXUh17ubu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c4dad8-6908-47af-dd17-8deb505cf635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus: [0 1 2 3 4 1 5 6]\n",
            "\n",
            "[[0 1 0 0 0 0 0]\n",
            " [1 0 1 0 1 1 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_similarity(x, y, eps=1e-8):\n",
        "  return np.dot(x, y) / ((np.sqrt(np.sum(x**2))+eps) * (np.sqrt(np.sum(y**2))+eps))\n",
        "\n",
        "def cos_similarity2(x, y, eps=1e-8):\n",
        "  nx = x / (np.sqrt(np.sum(x**2)) + eps) # Normalized x\n",
        "  ny = y / (np.sqrt(np.sum(y**2)) + eps) # Normalized y\n",
        "  return np.dot(nx, ny)\n",
        "\n",
        "x = np.array([1, 2, 3])\n",
        "y = np.array([0.5, 4, 3])\n",
        "\n",
        "print(cos_similarity(x, y))\n",
        "print(cos_similarity2(x, y))"
      ],
      "metadata": {
        "id": "ObuoE72UC7ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I like Nubzuki and you like Kumdori.\"\n",
        "corpus, word2id, id2word = preprocess(text)\n",
        "vocab_size = len(word2id)\n",
        "C = corpus2comatrix(corpus, vocab_size)\n",
        "\n",
        "c0 = C[word2id['i']] # I의 단어 벡터\n",
        "c1 = C[word2id['you']] # you의 단어 벡터\n",
        "print(\"\\b\"*16)\n",
        "print(f\"cosine similarity between 'i' and 'you': {cos_similarity(c0, c1)}\") # I와 you 단어 벡터 간의 코사인 유사도"
      ],
      "metadata": {
        "id": "Rs01nUi1EdAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# most_similar 함수\n",
        "def most_similar(query, word2id, id2word, word_matrix, top = 5):\n",
        "\n",
        "  if query not in word2id: print(f\"{query}를 찾을 수 없습니다.\"); return\n",
        "\n",
        "  query_id = word2id[query]\n",
        "  query_vector = word_matrix[query_id]\n",
        "\n",
        "  vocab_size = len(id2word)\n",
        "  similarity = np.zeros(vocab_size)\n",
        "  for i in range(vocab_size):\n",
        "    similarity[i] = cos_similarity(word_matrix[i], query_vector)\n",
        "\n",
        "  count = 0\n",
        "  for i in (-1 * similarity).argsort(): # argsort()는 오름차순\n",
        "    if id2word[i] == query: continue\n",
        "    print(f\"{id2word[i]}: {similarity[i]}\")\n",
        "    count += 1\n",
        "    if count >= top: return\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "text = \"I like Nubzuki and You like Kumdori.\"\n",
        "corpus, word2id, id2word = preprocess(text)\n",
        "vocab_size = len(word2id)\n",
        "C = corpus2comatrix(corpus, vocab_size)\n",
        "\n",
        "most_similar(\"you\", word2id, id2word, C, top=5)"
      ],
      "metadata": {
        "id": "fgiDVXo7JbeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ppmi(C, verbose=False, eps=1e-8): # C: 동시발생행렬 인풋, verbose: 진행상황 출력 여부 플래그\n",
        "  M = np.zeros_like(C, dtype=np.float32) # PPMI 행렬\n",
        "  N = np.sum(C)\n",
        "  S = np.sum(C, axis=0)\n",
        "  total = C.shape[0] * C.shape[1]\n",
        "  count = 0\n",
        "\n",
        "  for i in range(C.shape[0]): # 동시발생행렬의 각 행에 대해서\n",
        "    for j in range(C.shape[1]): # 동시발생행렬의 각 열에 대해서\n",
        "        pmi = np.log2( C[i,j]*N/ (S[i]*S[j]) + eps) # log0을 prevent\n",
        "        M[i, j] = max(0, pmi)\n",
        "        if verbose:\n",
        "          count += 1\n",
        "          if count % (total//100) == 0:\n",
        "            print(f\"{100*count/total}% 완료\")\n",
        "  return M\n",
        "\n",
        "text = \"I like Nubzuki and You like Kumdori.\"\n",
        "corpus, word2id, id2word = preprocess(text)\n",
        "vocab_size = len(word2id)\n",
        "C = corpus2comatrix(corpus, vocab_size)\n",
        "W = ppmi(C)\n",
        "\n",
        "np.set_printoptions(precision=3) # 유효 자릿수를 세자리로\n",
        "print(\"Co-occurrence Matrix\")\n",
        "print(C)\n",
        "print(\"=\"*45)\n",
        "print(\"Positive Pointwise Mutual Information \")\n",
        "print(W)"
      ],
      "metadata": {
        "id": "zeYPJ633TgII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "U, S, Vt = np.linalg.svd(W) # 희소행렬 W -> 밀집행렬 U\n",
        "\n",
        "print(f\"Original U.shape: {U.shape}\")\n",
        "print(f\"Original S.shape: {S.shape}\")\n",
        "print(f\"Original Vt.shape: {Vt.shape}\")\n",
        "print(\"=\"*70)\n",
        "print(\"** 7 -> 4 dimension **\")\n",
        "dim = 4\n",
        "U_ = U[:, :dim]\n",
        "S_ = S[:dim]\n",
        "Vt_ = Vt[:dim, :]\n",
        "\n",
        "print(f\"Reducted U.shape: {U_.shape}\")\n",
        "print(f\"Reducted S.shape: {S_.shape}\")\n",
        "print(f\"Reducted Vt.shape: {Vt_.shape}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"Original: {np.dot(U, S)@Vt}\")\n",
        "print(f\"Restored: {np.dot(U_, S_)@Vt}\")"
      ],
      "metadata": {
        "id": "7ivqKR1VdtR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from dataset import ptb # PTB 데이터셋\n",
        "\n",
        "corpus, word2id, id2word = ptb.load_data('train') # train 데이터 임포트\n",
        "\n",
        "print('코퍼스 크기:', len(corpus))\n",
        "print('corpus[:15]:', corpus[:15])\n",
        "print(f'\\nid2word[0]: {id2word[0]}\\nid2word[1]: {id2word[1]}\\n')\n",
        "print(f\"word2id['car']: {word2id['car']}\\nword2id['like']: {word2id['like']}\")"
      ],
      "metadata": {
        "id": "6KMGXgGEbSnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.extmath import randomized_svd\n",
        "import time\n",
        "\n",
        "window_size = 2\n",
        "wordvec_size = 100 # 100차원의 단어벡터\n",
        "\n",
        "corpus, word2id, id2word = ptb.load_data('train')\n",
        "vocab_size = len(word2id)\n",
        "C = corpus2comatrix(corpus, vocab_size, window_size) # 동시발생행렬 계산\n",
        "W = ppmi(C, verbose=False) # PPMI 계산\n",
        "\n",
        "start = time.time()\n",
        "# Truncated SVD scipy.sparse.linalg의 svds를 활용한 SVD\n",
        "U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=None)\n",
        "print(f\"소요시간: {time.time()-start}\")\n",
        "\n",
        "start = time.time()\n",
        "# 일반 SVD (매우 시간이 오래 걸림)\n",
        "U2, S2, V2 = np.linalg.svd(W)\n",
        "print(f\"소요시간: {time.time()-start}\")"
      ],
      "metadata": {
        "id": "Tm1QJbB8jsNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vecs = U[:, :wordvec_size]\n",
        "word_vecs2 = U2[:, :wordvec_size]\n",
        "\n",
        "querys = [\"i\", 'like', 'car']\n",
        "\n",
        "print(\"[Truncated SVD]\")\n",
        "start = time.time()\n",
        "for query in querys:\n",
        "  print(f\"({query})\")\n",
        "  most_similar(query, word2id, id2word, word_vecs, top=5)\n",
        "\n",
        "print(\"\\n[Original SVD]\")\n",
        "start = time.time()\n",
        "for query in querys:\n",
        "  print(f\"({query})\")\n",
        "  most_similar(query, word2id, id2word, word_vecs2, top=5)"
      ],
      "metadata": {
        "id": "DqK0N3uRmiRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CH 03"
      ],
      "metadata": {
        "id": "sEARufJ8RXQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력층의 9개 노드에서 은닉층의 3개 노드로 변환 (FCNN W/o bias)\n",
        "import numpy as np\n",
        "\n",
        "# text: \"I like Nubzuki and you like it too.\"\n",
        "x = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0]]) # text의 원핫 벡터 입력; shape = (1,9)\n",
        "W = np.random.randn(9,3) # 가중치\n",
        "h = np.matmul(x, W) # 은닉 노드\n",
        "print(h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lmtnj9-RTfB",
        "outputId": "eb2336a1-7b1e-434f-a352-2d7873749a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.26221641 -1.00796001  0.00548217]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CH 01에서 다룬 MatMul 계층\n",
        "# x = (N, D), W = (D, H), y = (N, H)\n",
        "\n",
        "class MatMul:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.x = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    W, = self.params\n",
        "    out = np.matmul(x, W)\n",
        "    self.x = x\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    W, = self.params\n",
        "    dx = np.matmul(dout, W.T)\n",
        "    dW = np.matmul(self.x.T, dout)\n",
        "    self.grads[0][...] = dW # 생략기호(...)를 통한 넘파이 배열의 덮어쓰기.\n",
        "                            #(깊은 복사) 메모리 주소는 미변경, 값이 복제됨.\n",
        "    return dx"
      ],
      "metadata": {
        "id": "gbJnvCwaan1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MatMul 계층과 softmax 함수를 활용한 CBOW 모델의 추론 처리\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# Context Examples\n",
        "c0 = np.array([[1, 0, 0, 0, 0, 0]])\n",
        "c1 = np.array([[0, 0, 1, 0, 0, 0]])\n",
        "\n",
        "# Weights Initialization\n",
        "W_in = np.random.randn(6, 3)\n",
        "W_out = np.random.randn(3, 6)\n",
        "\n",
        "# Layer construction\n",
        "layer_in_0 = MatMul(W_in)\n",
        "layer_in_1 = MatMul(W_in)\n",
        "layer_out = MatMul(W_out)\n",
        "\n",
        "# Forward Propagation\n",
        "h0 = layer_in_0.forward(c0)\n",
        "h1 = layer_in_1.forward(c1)\n",
        "h = 0.5 * (h0+h1)\n",
        "score = layer_out.forward(h)\n",
        "print(score)\n",
        "\n",
        "def softmax(x):\n",
        "  tmp = np.exp(x)\n",
        "  sum = np.sum(tmp)\n",
        "  return tmp/sum\n",
        "\n",
        "probability = softmax(score)\n",
        "print(probability)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yjdw7khVarcN",
        "outputId": "9844ee9f-fb8c-43b9-9872-6e1085e0d24a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.76015896 -0.30222942  0.56581549 -1.76161468  1.27363606 -1.46090573]]\n",
            "[[0.47297594 0.06013885 0.14326571 0.01397501 0.29076681 0.01887767]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CBOW 모델의 학습을 위한 데이터 준비\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.util import preprocess, convert_one_hot\n",
        "\n",
        "text = \"I like Nubzuki and you like it too.\"\n",
        "corpus, word2id, id2word = preprocess(text)\n",
        "print(f\"corpus: {corpus}\")\n",
        "\n",
        "def corpus2contexts_target(corpus, window_size=1):\n",
        "  contexts, target = [], corpus[window_size:-window_size]\n",
        "  for idx in range(window_size, len(corpus)-window_size): # idx가 곧 타깃 단어\n",
        "    cs = []\n",
        "    for t in range(-window_size, window_size+1): # 맥락 구하기\n",
        "      if t == 0: continue # 타깃 단어이므로 스킵\n",
        "      cs.append(corpus[idx+t])\n",
        "    contexts.append(cs)\n",
        "  return np.array(contexts), np.array(target)\n",
        "\n",
        "contexts, target = corpus2contexts_target(corpus, window_size = 1)\n",
        "print(f\"contexts: {contexts}\")\n",
        "print(f\"target: {target}\")\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "contexts = convert_one_hot(contexts, vocab_size)\n",
        "target = convert_one_hot(target, vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6yMUupnarmt",
        "outputId": "9f2cb694-ec71-4fbf-94a7-24bb064c8eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus: [0 1 2 3 4 1 5 6 7]\n",
            "contexts: [[0 2]\n",
            " [1 3]\n",
            " [2 4]\n",
            " [3 1]\n",
            " [4 5]\n",
            " [1 6]\n",
            " [5 7]]\n",
            "target: [1 2 3 4 1 5 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple한 CBOW 신경망 만들기\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.layers import MatMul, SoftmaxWithLoss\n",
        "\n",
        "class SimpleCBOW:\n",
        "  def __init__(self, vocab_size, hidden_size):\n",
        "    V, H = vocab_size, hidden_size\n",
        "    W_in = 0.01 * np.random.randn(V, H).astype(np.float32)\n",
        "    W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
        "\n",
        "    self.layer_in_0, self.layer_in_1 = MatMul(W_in), MatMul(W_in)\n",
        "    self.layer_out = MatMul(W_out)\n",
        "    self.layer_loss = SoftmaxWithLoss() # Score에서 Loss를 만드는 계층\n",
        "\n",
        "    layers = [self.layer_in_0, self.layer_in_1, self.layer_out]\n",
        "\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in layers:\n",
        "      self.params += layer.params; self.grads += layer.grads\n",
        "\n",
        "    self.word_vecs = W_in # 단어의 분산 표현은 W_in이다.\n",
        "\n",
        "  def forward(self, contexts, target):\n",
        "    h0 = self.layer_in_0.forward(contexts[:, 0])\n",
        "    h1 = self.layer_in_1.forward(contexts[:, 1])\n",
        "    h = 0.5 * (h0 + h1)\n",
        "    score = self.layer_out.forward(h)\n",
        "    loss = self.layer_loss.forward(score, target)\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    ds = self.layer_loss.backward(dout)\n",
        "    da = self.layer_out.backward(ds)\n",
        "    da *= 0.5\n",
        "    self.layer_in_0.backward(da)\n",
        "    self.layer_in_1.backward(da)\n",
        "    return None"
      ],
      "metadata": {
        "id": "bvLqs3xJarph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구현한 SimpleCBOW 모델로 학습 시키기\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
        "\n",
        "W = 1 # window size\n",
        "H = 5 # Hidden size\n",
        "N = 3 # Batch size\n",
        "max_epoch = 1000\n",
        "\n",
        "text = \"I like Nubzuki and you like it too.\"\n",
        "corpus, word2id, id2word = preprocess(text)\n",
        "\n",
        "V = len(word2id) # Vocab size\n",
        "contexts, target = create_contexts_target(corpus, W)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)\n",
        "\n",
        "model = SimpleCBOW(V, H)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "trainer.fit(contexts, target, max_epoch, N)\n",
        "trainer.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WpXpKh4en9SH",
        "outputId": "8621a0cf-069d-498d-9980-2a6f0bc635dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| 에폭 1 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 2 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 3 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 4 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 5 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 6 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 7 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 8 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 9 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 10 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 11 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 12 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 13 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 14 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 15 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 16 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 17 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 18 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 19 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 20 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 21 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 22 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 23 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 24 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 25 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 26 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 27 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 28 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 29 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 30 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 31 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 32 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 33 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 34 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 35 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 36 |  반복 1 / 2 | 시간 0[s] | 손실 2.06\n",
            "| 에폭 37 |  반복 1 / 2 | 시간 0[s] | 손실 2.06\n",
            "| 에폭 38 |  반복 1 / 2 | 시간 0[s] | 손실 2.06\n",
            "| 에폭 39 |  반복 1 / 2 | 시간 0[s] | 손실 2.06\n",
            "| 에폭 40 |  반복 1 / 2 | 시간 0[s] | 손실 2.06\n",
            "| 에폭 41 |  반복 1 / 2 | 시간 0[s] | 손실 2.06\n",
            "| 에폭 42 |  반복 1 / 2 | 시간 0[s] | 손실 2.05\n",
            "| 에폭 43 |  반복 1 / 2 | 시간 0[s] | 손실 2.05\n",
            "| 에폭 44 |  반복 1 / 2 | 시간 0[s] | 손실 2.05\n",
            "| 에폭 45 |  반복 1 / 2 | 시간 0[s] | 손실 2.05\n",
            "| 에폭 46 |  반복 1 / 2 | 시간 0[s] | 손실 2.05\n",
            "| 에폭 47 |  반복 1 / 2 | 시간 0[s] | 손실 2.05\n",
            "| 에폭 48 |  반복 1 / 2 | 시간 0[s] | 손실 2.05\n",
            "| 에폭 49 |  반복 1 / 2 | 시간 0[s] | 손실 2.04\n",
            "| 에폭 50 |  반복 1 / 2 | 시간 0[s] | 손실 2.04\n",
            "| 에폭 51 |  반복 1 / 2 | 시간 0[s] | 손실 2.04\n",
            "| 에폭 52 |  반복 1 / 2 | 시간 0[s] | 손실 2.04\n",
            "| 에폭 53 |  반복 1 / 2 | 시간 0[s] | 손실 2.04\n",
            "| 에폭 54 |  반복 1 / 2 | 시간 0[s] | 손실 2.03\n",
            "| 에폭 55 |  반복 1 / 2 | 시간 0[s] | 손실 2.03\n",
            "| 에폭 56 |  반복 1 / 2 | 시간 0[s] | 손실 2.03\n",
            "| 에폭 57 |  반복 1 / 2 | 시간 0[s] | 손실 2.03\n",
            "| 에폭 58 |  반복 1 / 2 | 시간 0[s] | 손실 2.02\n",
            "| 에폭 59 |  반복 1 / 2 | 시간 0[s] | 손실 2.02\n",
            "| 에폭 60 |  반복 1 / 2 | 시간 0[s] | 손실 2.02\n",
            "| 에폭 61 |  반복 1 / 2 | 시간 0[s] | 손실 2.01\n",
            "| 에폭 62 |  반복 1 / 2 | 시간 0[s] | 손실 2.02\n",
            "| 에폭 63 |  반복 1 / 2 | 시간 0[s] | 손실 2.01\n",
            "| 에폭 64 |  반복 1 / 2 | 시간 0[s] | 손실 2.00\n",
            "| 에폭 65 |  반복 1 / 2 | 시간 0[s] | 손실 2.01\n",
            "| 에폭 66 |  반복 1 / 2 | 시간 0[s] | 손실 2.00\n",
            "| 에폭 67 |  반복 1 / 2 | 시간 0[s] | 손실 2.00\n",
            "| 에폭 68 |  반복 1 / 2 | 시간 0[s] | 손실 1.99\n",
            "| 에폭 69 |  반복 1 / 2 | 시간 0[s] | 손실 2.00\n",
            "| 에폭 70 |  반복 1 / 2 | 시간 0[s] | 손실 1.99\n",
            "| 에폭 71 |  반복 1 / 2 | 시간 0[s] | 손실 1.98\n",
            "| 에폭 72 |  반복 1 / 2 | 시간 0[s] | 손실 1.99\n",
            "| 에폭 73 |  반복 1 / 2 | 시간 0[s] | 손실 1.98\n",
            "| 에폭 74 |  반복 1 / 2 | 시간 0[s] | 손실 1.97\n",
            "| 에폭 75 |  반복 1 / 2 | 시간 0[s] | 손실 1.97\n",
            "| 에폭 76 |  반복 1 / 2 | 시간 0[s] | 손실 1.97\n",
            "| 에폭 77 |  반복 1 / 2 | 시간 0[s] | 손실 1.96\n",
            "| 에폭 78 |  반복 1 / 2 | 시간 0[s] | 손실 1.97\n",
            "| 에폭 79 |  반복 1 / 2 | 시간 0[s] | 손실 1.97\n",
            "| 에폭 80 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 81 |  반복 1 / 2 | 시간 0[s] | 손실 1.96\n",
            "| 에폭 82 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 83 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 84 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 85 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 86 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 87 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 88 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 89 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 90 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 91 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 92 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 93 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 94 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 95 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 96 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 97 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 98 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 99 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 100 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 101 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 102 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 103 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 104 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 105 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 106 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
            "| 에폭 107 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
            "| 에폭 108 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
            "| 에폭 109 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 110 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
            "| 에폭 111 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
            "| 에폭 112 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
            "| 에폭 113 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
            "| 에폭 114 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
            "| 에폭 115 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
            "| 에폭 116 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
            "| 에폭 117 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
            "| 에폭 118 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
            "| 에폭 119 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
            "| 에폭 120 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
            "| 에폭 121 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
            "| 에폭 122 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
            "| 에폭 123 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
            "| 에폭 124 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
            "| 에폭 125 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
            "| 에폭 126 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
            "| 에폭 127 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
            "| 에폭 128 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
            "| 에폭 129 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
            "| 에폭 130 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
            "| 에폭 131 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
            "| 에폭 132 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
            "| 에폭 133 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
            "| 에폭 134 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
            "| 에폭 135 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
            "| 에폭 136 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
            "| 에폭 137 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
            "| 에폭 138 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
            "| 에폭 139 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
            "| 에폭 140 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
            "| 에폭 141 |  반복 1 / 2 | 시간 0[s] | 손실 1.63\n",
            "| 에폭 142 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
            "| 에폭 143 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
            "| 에폭 144 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
            "| 에폭 145 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
            "| 에폭 146 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
            "| 에폭 147 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
            "| 에폭 148 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
            "| 에폭 149 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
            "| 에폭 150 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
            "| 에폭 151 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
            "| 에폭 152 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
            "| 에폭 153 |  반복 1 / 2 | 시간 0[s] | 손실 1.64\n",
            "| 에폭 154 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
            "| 에폭 155 |  반복 1 / 2 | 시간 0[s] | 손실 1.63\n",
            "| 에폭 156 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
            "| 에폭 157 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
            "| 에폭 158 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
            "| 에폭 159 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
            "| 에폭 160 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
            "| 에폭 161 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
            "| 에폭 162 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
            "| 에폭 163 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
            "| 에폭 164 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
            "| 에폭 165 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
            "| 에폭 166 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
            "| 에폭 167 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
            "| 에폭 168 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
            "| 에폭 169 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
            "| 에폭 170 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
            "| 에폭 171 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
            "| 에폭 172 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
            "| 에폭 173 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
            "| 에폭 174 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
            "| 에폭 175 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 176 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
            "| 에폭 177 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
            "| 에폭 178 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
            "| 에폭 179 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
            "| 에폭 180 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
            "| 에폭 181 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
            "| 에폭 182 |  반복 1 / 2 | 시간 0[s] | 손실 1.40\n",
            "| 에폭 183 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
            "| 에폭 184 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
            "| 에폭 185 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
            "| 에폭 186 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
            "| 에폭 187 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
            "| 에폭 188 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 189 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
            "| 에폭 190 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
            "| 에폭 191 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
            "| 에폭 192 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
            "| 에폭 193 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 194 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 195 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
            "| 에폭 196 |  반복 1 / 2 | 시간 0[s] | 손실 1.40\n",
            "| 에폭 197 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 198 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
            "| 에폭 199 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
            "| 에폭 200 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
            "| 에폭 201 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
            "| 에폭 202 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
            "| 에폭 203 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 204 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
            "| 에폭 205 |  반복 1 / 2 | 시간 0[s] | 손실 1.44\n",
            "| 에폭 206 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 207 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
            "| 에폭 208 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
            "| 에폭 209 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
            "| 에폭 210 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
            "| 에폭 211 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
            "| 에폭 212 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
            "| 에폭 213 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
            "| 에폭 214 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
            "| 에폭 215 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
            "| 에폭 216 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
            "| 에폭 217 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
            "| 에폭 218 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
            "| 에폭 219 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 220 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
            "| 에폭 221 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
            "| 에폭 222 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
            "| 에폭 223 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
            "| 에폭 224 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
            "| 에폭 225 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
            "| 에폭 226 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
            "| 에폭 227 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
            "| 에폭 228 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
            "| 에폭 229 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
            "| 에폭 230 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 231 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
            "| 에폭 232 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
            "| 에폭 233 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
            "| 에폭 234 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
            "| 에폭 235 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
            "| 에폭 236 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 237 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
            "| 에폭 238 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 239 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
            "| 에폭 240 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
            "| 에폭 241 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
            "| 에폭 242 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
            "| 에폭 243 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
            "| 에폭 244 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
            "| 에폭 245 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
            "| 에폭 246 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 247 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
            "| 에폭 248 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
            "| 에폭 249 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 250 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
            "| 에폭 251 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 252 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
            "| 에폭 253 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
            "| 에폭 254 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
            "| 에폭 255 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 256 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 257 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 258 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 259 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
            "| 에폭 260 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
            "| 에폭 261 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
            "| 에폭 262 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
            "| 에폭 263 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
            "| 에폭 264 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 265 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
            "| 에폭 266 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
            "| 에폭 267 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
            "| 에폭 268 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 269 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
            "| 에폭 270 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
            "| 에폭 271 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 272 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
            "| 에폭 273 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 274 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 275 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 276 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 277 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 278 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 279 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
            "| 에폭 280 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 281 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
            "| 에폭 282 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 283 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
            "| 에폭 284 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 285 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 286 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 287 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 288 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 289 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 290 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
            "| 에폭 291 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 292 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 293 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 294 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
            "| 에폭 295 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 296 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 297 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 298 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
            "| 에폭 299 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
            "| 에폭 300 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 301 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
            "| 에폭 302 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 303 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
            "| 에폭 304 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 305 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 306 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 307 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 308 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 309 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 310 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
            "| 에폭 311 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 312 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 313 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 314 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 315 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 316 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 317 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
            "| 에폭 318 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 319 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 320 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 321 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 322 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 323 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 324 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 325 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 326 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 327 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 328 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
            "| 에폭 329 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 330 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 331 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
            "| 에폭 332 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 333 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 334 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 335 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 336 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 337 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 338 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
            "| 에폭 339 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 340 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 341 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 342 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 343 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 344 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 345 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 346 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 347 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 348 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 349 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 350 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 351 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 352 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 353 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 354 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 355 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 356 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 357 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 358 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 359 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 360 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 361 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
            "| 에폭 362 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 363 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 364 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 365 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 366 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 367 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 368 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 369 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 370 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 371 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 372 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 373 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 374 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 375 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 376 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 377 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 378 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 379 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 380 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 381 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 382 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 383 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 384 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 385 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 386 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 387 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 388 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 389 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 390 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 391 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 392 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 393 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 394 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 395 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 396 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 397 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 398 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 399 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 400 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 401 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 402 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 403 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 404 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 405 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 406 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 407 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 408 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 409 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 410 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 411 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 412 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 413 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 414 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 415 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 416 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 417 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 418 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 419 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 420 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 421 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 422 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 423 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 424 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 425 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 426 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 427 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 428 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 429 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 430 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 431 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 432 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 433 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 434 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 435 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 436 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 437 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 438 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 439 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 440 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 441 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 442 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 443 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 444 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 445 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 446 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 447 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 448 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 449 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 450 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 451 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 452 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 453 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 454 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 455 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 456 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 457 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 458 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 459 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 460 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 461 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 462 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 463 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 464 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 465 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 466 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 467 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 468 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 469 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 470 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 471 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 472 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 473 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 474 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 475 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 476 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 477 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 478 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 479 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 480 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 481 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 482 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 483 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 484 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 485 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 486 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 487 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 488 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 489 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 490 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 491 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 492 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 493 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 494 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 495 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 496 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 497 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 498 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 499 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 500 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 501 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 502 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 503 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 504 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 505 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 506 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 507 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 508 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 509 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 510 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 511 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 512 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 513 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 514 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 515 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 516 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 517 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 518 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 519 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 520 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 521 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 522 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 523 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 524 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 525 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 526 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 527 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 528 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 529 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 530 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 531 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 532 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 533 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 534 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 535 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 536 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 537 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 538 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 539 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 540 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 541 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 542 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 543 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 544 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 545 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 546 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 547 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 548 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 549 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 550 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 551 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 552 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 553 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 554 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 555 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 556 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 557 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 558 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 559 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 560 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 561 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 562 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 563 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 564 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 565 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 566 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 567 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 568 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 569 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 570 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 571 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 572 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 573 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 574 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 575 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 576 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 577 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 578 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 579 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 580 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 581 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 582 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 583 |  반복 1 / 2 | 시간 1[s] | 손실 0.77\n",
            "| 에폭 584 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 585 |  반복 1 / 2 | 시간 1[s] | 손실 0.89\n",
            "| 에폭 586 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 587 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 588 |  반복 1 / 2 | 시간 1[s] | 손실 0.83\n",
            "| 에폭 589 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
            "| 에폭 590 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
            "| 에폭 591 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 592 |  반복 1 / 2 | 시간 1[s] | 손실 0.82\n",
            "| 에폭 593 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
            "| 에폭 594 |  반복 1 / 2 | 시간 1[s] | 손실 0.77\n",
            "| 에폭 595 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 596 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 597 |  반복 1 / 2 | 시간 1[s] | 손실 0.87\n",
            "| 에폭 598 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
            "| 에폭 599 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 600 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 601 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
            "| 에폭 602 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 603 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 604 |  반복 1 / 2 | 시간 1[s] | 손실 0.82\n",
            "| 에폭 605 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 606 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
            "| 에폭 607 |  반복 1 / 2 | 시간 1[s] | 손실 0.89\n",
            "| 에폭 608 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 609 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 610 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
            "| 에폭 611 |  반복 1 / 2 | 시간 1[s] | 손실 0.77\n",
            "| 에폭 612 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 613 |  반복 1 / 2 | 시간 1[s] | 손실 0.84\n",
            "| 에폭 614 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 615 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
            "| 에폭 616 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 617 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
            "| 에폭 618 |  반복 1 / 2 | 시간 1[s] | 손실 0.86\n",
            "| 에폭 619 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 620 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
            "| 에폭 621 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 622 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 623 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 624 |  반복 1 / 2 | 시간 1[s] | 손실 0.85\n",
            "| 에폭 625 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 626 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 627 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
            "| 에폭 628 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 629 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
            "| 에폭 630 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
            "| 에폭 631 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 632 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 633 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 634 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 635 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 636 |  반복 1 / 2 | 시간 1[s] | 손실 0.82\n",
            "| 에폭 637 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 638 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 639 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 640 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 641 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 642 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
            "| 에폭 643 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 644 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 645 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
            "| 에폭 646 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 647 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 648 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 649 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 650 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
            "| 에폭 651 |  반복 1 / 2 | 시간 1[s] | 손실 0.86\n",
            "| 에폭 652 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 653 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
            "| 에폭 654 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 655 |  반복 1 / 2 | 시간 1[s] | 손실 0.85\n",
            "| 에폭 656 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 657 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 658 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 659 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 660 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
            "| 에폭 661 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 662 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 663 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
            "| 에폭 664 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
            "| 에폭 665 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 666 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 667 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 668 |  반복 1 / 2 | 시간 1[s] | 손실 0.92\n",
            "| 에폭 669 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 670 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
            "| 에폭 671 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 672 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
            "| 에폭 673 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 674 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
            "| 에폭 675 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 676 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 677 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
            "| 에폭 678 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
            "| 에폭 679 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 680 |  반복 1 / 2 | 시간 1[s] | 손실 0.90\n",
            "| 에폭 681 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
            "| 에폭 682 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
            "| 에폭 683 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 684 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 685 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
            "| 에폭 686 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
            "| 에폭 687 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
            "| 에폭 688 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
            "| 에폭 689 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
            "| 에폭 690 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
            "| 에폭 691 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 692 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 693 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 694 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
            "| 에폭 695 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 696 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
            "| 에폭 697 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
            "| 에폭 698 |  반복 1 / 2 | 시간 1[s] | 손실 0.90\n",
            "| 에폭 699 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 700 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 701 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 702 |  반복 1 / 2 | 시간 1[s] | 손실 0.87\n",
            "| 에폭 703 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 704 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 705 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
            "| 에폭 706 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
            "| 에폭 707 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 708 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 709 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
            "| 에폭 710 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 711 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 712 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
            "| 에폭 713 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 714 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
            "| 에폭 715 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 716 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
            "| 에폭 717 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 718 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
            "| 에폭 719 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 720 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
            "| 에폭 721 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 722 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
            "| 에폭 723 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 724 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
            "| 에폭 725 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 726 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 727 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 728 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 729 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
            "| 에폭 730 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
            "| 에폭 731 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
            "| 에폭 732 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 733 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 734 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 735 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 736 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 737 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 738 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
            "| 에폭 739 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 740 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
            "| 에폭 741 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
            "| 에폭 742 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
            "| 에폭 743 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 744 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
            "| 에폭 745 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
            "| 에폭 746 |  반복 1 / 2 | 시간 1[s] | 손실 0.77\n",
            "| 에폭 747 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 748 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 749 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
            "| 에폭 750 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
            "| 에폭 751 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 752 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 753 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 754 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 755 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 756 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 757 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 758 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 759 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
            "| 에폭 760 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 761 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 762 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
            "| 에폭 763 |  반복 1 / 2 | 시간 1[s] | 손실 0.83\n",
            "| 에폭 764 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 765 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 766 |  반복 1 / 2 | 시간 1[s] | 손실 0.87\n",
            "| 에폭 767 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 768 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 769 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 770 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 771 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 772 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 773 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 774 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
            "| 에폭 775 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 776 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 777 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 778 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
            "| 에폭 779 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 780 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 781 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
            "| 에폭 782 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
            "| 에폭 783 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 784 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 785 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 786 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 787 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 788 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
            "| 에폭 789 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 790 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
            "| 에폭 791 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 792 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
            "| 에폭 793 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 794 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
            "| 에폭 795 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 796 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 797 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
            "| 에폭 798 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 799 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 800 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 801 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
            "| 에폭 802 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 803 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 804 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 805 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
            "| 에폭 806 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 807 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
            "| 에폭 808 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 809 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
            "| 에폭 810 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 811 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 812 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
            "| 에폭 813 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
            "| 에폭 814 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 815 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 816 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 817 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 818 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 819 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 820 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 821 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
            "| 에폭 822 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 823 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 824 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
            "| 에폭 825 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 826 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 827 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 828 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 829 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 830 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 831 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 832 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 833 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
            "| 에폭 834 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
            "| 에폭 835 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 836 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
            "| 에폭 837 |  반복 1 / 2 | 시간 1[s] | 손실 0.36\n",
            "| 에폭 838 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 839 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
            "| 에폭 840 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 841 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 842 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
            "| 에폭 843 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 844 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
            "| 에폭 845 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 846 |  반복 1 / 2 | 시간 1[s] | 손실 0.83\n",
            "| 에폭 847 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
            "| 에폭 848 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 849 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 850 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 851 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 852 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 853 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 854 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 855 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
            "| 에폭 856 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
            "| 에폭 857 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 858 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 859 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 860 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 861 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
            "| 에폭 862 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 863 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 864 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 865 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 866 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 867 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 868 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 869 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 870 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 871 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 872 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 873 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 874 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 875 |  반복 1 / 2 | 시간 1[s] | 손실 0.40\n",
            "| 에폭 876 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 877 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 878 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 879 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 880 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
            "| 에폭 881 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 882 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
            "| 에폭 883 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
            "| 에폭 884 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
            "| 에폭 885 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
            "| 에폭 886 |  반복 1 / 2 | 시간 1[s] | 손실 0.40\n",
            "| 에폭 887 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
            "| 에폭 888 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 889 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 890 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
            "| 에폭 891 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 892 |  반복 1 / 2 | 시간 1[s] | 손실 0.25\n",
            "| 에폭 893 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 894 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 895 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
            "| 에폭 896 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 897 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 898 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
            "| 에폭 899 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
            "| 에폭 900 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 901 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 902 |  반복 1 / 2 | 시간 1[s] | 손실 0.35\n",
            "| 에폭 903 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 904 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 905 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
            "| 에폭 906 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 907 |  반복 1 / 2 | 시간 1[s] | 손실 0.41\n",
            "| 에폭 908 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
            "| 에폭 909 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
            "| 에폭 910 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
            "| 에폭 911 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 912 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 913 |  반복 1 / 2 | 시간 1[s] | 손실 0.34\n",
            "| 에폭 914 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 915 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 916 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 917 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 918 |  반복 1 / 2 | 시간 1[s] | 손실 0.34\n",
            "| 에폭 919 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
            "| 에폭 920 |  반복 1 / 2 | 시간 1[s] | 손실 0.77\n",
            "| 에폭 921 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 922 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 923 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 924 |  반복 1 / 2 | 시간 1[s] | 손실 0.33\n",
            "| 에폭 925 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 926 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 927 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 928 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
            "| 에폭 929 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 930 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 931 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 932 |  반복 1 / 2 | 시간 1[s] | 손실 0.41\n",
            "| 에폭 933 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 934 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 935 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 936 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 937 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
            "| 에폭 938 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 939 |  반복 1 / 2 | 시간 1[s] | 손실 0.34\n",
            "| 에폭 940 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 941 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 942 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 943 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 944 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 945 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 946 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 947 |  반복 1 / 2 | 시간 1[s] | 손실 0.34\n",
            "| 에폭 948 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
            "| 에폭 949 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 950 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 951 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 952 |  반복 1 / 2 | 시간 1[s] | 손실 0.33\n",
            "| 에폭 953 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 954 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 955 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
            "| 에폭 956 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 957 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 958 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 959 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 960 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 961 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
            "| 에폭 962 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 963 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 964 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 965 |  반복 1 / 2 | 시간 1[s] | 손실 0.40\n",
            "| 에폭 966 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 967 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 968 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
            "| 에폭 969 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 970 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 971 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 972 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
            "| 에폭 973 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 974 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 975 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 976 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 977 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
            "| 에폭 978 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 979 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 980 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
            "| 에폭 981 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 982 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 983 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 984 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 985 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 986 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 987 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
            "| 에폭 988 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 989 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 990 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
            "| 에폭 991 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 992 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
            "| 에폭 993 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 994 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
            "| 에폭 995 |  반복 1 / 2 | 시간 1[s] | 손실 0.32\n",
            "| 에폭 996 |  반복 1 / 2 | 시간 1[s] | 손실 0.36\n",
            "| 에폭 997 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 998 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 999 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
            "| 에폭 1000 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MatMul 계층의 가중치를 꺼내어 내용을 확인해보기\n",
        "\n",
        "word_vecs = model.word_vecs\n",
        "for word_id, word in id2word.items():\n",
        "  print(word, word_vecs[word_id])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fTkGgAjn9Ud",
        "outputId": "03fe7fa5-0c48-4e06-a3d8-bc3ecce30588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i [ 1.3099738 -1.2070829  1.1296748  0.9779411  1.2318969]\n",
            "like [-1.2148529   1.1633682  -1.2152407  -0.21331532 -1.1852255 ]\n",
            "nubzuki [ 0.4783465 -0.7001424  0.7960252  1.4543121  0.6970033]\n",
            "and [-0.93465024  0.9306229  -0.96775067 -1.7975717  -0.9266825 ]\n",
            "you [ 0.4123881 -0.6536685  0.7223608  1.7135993  0.6817525]\n",
            "it [ 1.3750037 -1.3083295  1.2798731 -0.4029655  1.3190441]\n",
            "too [-1.0875041  1.0117518 -1.0561371  1.5979116 -1.062342 ]\n",
            ". [ 0.8763476  -0.86544174  0.86302066 -1.5337248   0.81785667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.layers import MatMul, SoftmaxWithLoss\n",
        "\n",
        "class SimpleSkipGram:\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        V, H = vocab_size, hidden_size\n",
        "\n",
        "        # 가중치 초기화\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layer_in = MatMul(W_in)\n",
        "        self.layer_out = MatMul(W_out)\n",
        "        self.layer_loss1 = SoftmaxWithLoss()\n",
        "        self.layer_loss2 = SoftmaxWithLoss()\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        layers = [self.layer_in, self.layer_out]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h = self.layer_in.forward(target)\n",
        "        s = self.layer_out.forward(h)\n",
        "        l1 = self.layer_loss1.forward(s, contexts[:, 0])\n",
        "        l2 = self.layer_loss2.forward(s, contexts[:, 1])\n",
        "        loss = l1 + l2\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dl1 = self.layer_loss1.backward(dout)\n",
        "        dl2 = self.layer_loss2.backward(dout)\n",
        "        ds = dl1 + dl2\n",
        "        dh = self.layer_out.backward(ds)\n",
        "        self.layer_in.backward(dh)\n",
        "        return None"
      ],
      "metadata": {
        "id": "v9wzYjp1yxT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구현한 SimpleSkipGram 모델로 학습 시키기\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
        "\n",
        "W = 1 # window size\n",
        "H = 5 # Hidden size\n",
        "N = 3 # Batch size\n",
        "max_epoch = 1000\n",
        "\n",
        "text = \"I like Nubzuki and you like it too.\"\n",
        "corpus, word2id, id2word = preprocess(text)\n",
        "\n",
        "V = len(word2id) # Vocab size\n",
        "contexts, target = create_contexts_target(corpus, W)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)\n",
        "\n",
        "model = SimpleSkipGram(V, H)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "trainer.fit(contexts, target, max_epoch, N)\n",
        "trainer.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cL3O15BPy1Td",
        "outputId": "be85c25c-ecb7-4a36-9c32-82bd3b9c5b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| 에폭 1 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 2 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 3 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 4 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 5 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 6 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 7 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 8 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 9 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 10 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 11 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 12 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 13 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 14 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 15 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 16 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 17 |  반복 1 / 2 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 18 |  반복 1 / 2 | 시간 0[s] | 손실 4.15\n",
            "| 에폭 19 |  반복 1 / 2 | 시간 0[s] | 손실 4.15\n",
            "| 에폭 20 |  반복 1 / 2 | 시간 0[s] | 손실 4.15\n",
            "| 에폭 21 |  반복 1 / 2 | 시간 0[s] | 손실 4.15\n",
            "| 에폭 22 |  반복 1 / 2 | 시간 0[s] | 손실 4.15\n",
            "| 에폭 23 |  반복 1 / 2 | 시간 0[s] | 손실 4.15\n",
            "| 에폭 24 |  반복 1 / 2 | 시간 0[s] | 손실 4.15\n",
            "| 에폭 25 |  반복 1 / 2 | 시간 0[s] | 손실 4.15\n",
            "| 에폭 26 |  반복 1 / 2 | 시간 0[s] | 손실 4.15\n",
            "| 에폭 27 |  반복 1 / 2 | 시간 0[s] | 손실 4.15\n",
            "| 에폭 28 |  반복 1 / 2 | 시간 0[s] | 손실 4.15\n",
            "| 에폭 29 |  반복 1 / 2 | 시간 0[s] | 손실 4.14\n",
            "| 에폭 30 |  반복 1 / 2 | 시간 0[s] | 손실 4.14\n",
            "| 에폭 31 |  반복 1 / 2 | 시간 0[s] | 손실 4.14\n",
            "| 에폭 32 |  반복 1 / 2 | 시간 0[s] | 손실 4.14\n",
            "| 에폭 33 |  반복 1 / 2 | 시간 0[s] | 손실 4.14\n",
            "| 에폭 34 |  반복 1 / 2 | 시간 0[s] | 손실 4.14\n",
            "| 에폭 35 |  반복 1 / 2 | 시간 0[s] | 손실 4.14\n",
            "| 에폭 36 |  반복 1 / 2 | 시간 0[s] | 손실 4.14\n",
            "| 에폭 37 |  반복 1 / 2 | 시간 0[s] | 손실 4.13\n",
            "| 에폭 38 |  반복 1 / 2 | 시간 0[s] | 손실 4.12\n",
            "| 에폭 39 |  반복 1 / 2 | 시간 0[s] | 손실 4.13\n",
            "| 에폭 40 |  반복 1 / 2 | 시간 0[s] | 손실 4.13\n",
            "| 에폭 41 |  반복 1 / 2 | 시간 0[s] | 손실 4.12\n",
            "| 에폭 42 |  반복 1 / 2 | 시간 0[s] | 손실 4.12\n",
            "| 에폭 43 |  반복 1 / 2 | 시간 0[s] | 손실 4.11\n",
            "| 에폭 44 |  반복 1 / 2 | 시간 0[s] | 손실 4.12\n",
            "| 에폭 45 |  반복 1 / 2 | 시간 0[s] | 손실 4.11\n",
            "| 에폭 46 |  반복 1 / 2 | 시간 0[s] | 손실 4.12\n",
            "| 에폭 47 |  반복 1 / 2 | 시간 0[s] | 손실 4.10\n",
            "| 에폭 48 |  반복 1 / 2 | 시간 0[s] | 손실 4.10\n",
            "| 에폭 49 |  반복 1 / 2 | 시간 0[s] | 손실 4.10\n",
            "| 에폭 50 |  반복 1 / 2 | 시간 0[s] | 손실 4.09\n",
            "| 에폭 51 |  반복 1 / 2 | 시간 0[s] | 손실 4.10\n",
            "| 에폭 52 |  반복 1 / 2 | 시간 0[s] | 손실 4.09\n",
            "| 에폭 53 |  반복 1 / 2 | 시간 0[s] | 손실 4.08\n",
            "| 에폭 54 |  반복 1 / 2 | 시간 0[s] | 손실 4.09\n",
            "| 에폭 55 |  반복 1 / 2 | 시간 0[s] | 손실 4.08\n",
            "| 에폭 56 |  반복 1 / 2 | 시간 0[s] | 손실 4.07\n",
            "| 에폭 57 |  반복 1 / 2 | 시간 0[s] | 손실 4.07\n",
            "| 에폭 58 |  반복 1 / 2 | 시간 0[s] | 손실 4.07\n",
            "| 에폭 59 |  반복 1 / 2 | 시간 0[s] | 손실 4.06\n",
            "| 에폭 60 |  반복 1 / 2 | 시간 0[s] | 손실 4.06\n",
            "| 에폭 61 |  반복 1 / 2 | 시간 0[s] | 손실 4.05\n",
            "| 에폭 62 |  반복 1 / 2 | 시간 0[s] | 손실 4.06\n",
            "| 에폭 63 |  반복 1 / 2 | 시간 0[s] | 손실 4.04\n",
            "| 에폭 64 |  반복 1 / 2 | 시간 0[s] | 손실 4.04\n",
            "| 에폭 65 |  반복 1 / 2 | 시간 0[s] | 손실 4.04\n",
            "| 에폭 66 |  반복 1 / 2 | 시간 0[s] | 손실 4.04\n",
            "| 에폭 67 |  반복 1 / 2 | 시간 0[s] | 손실 4.02\n",
            "| 에폭 68 |  반복 1 / 2 | 시간 0[s] | 손실 4.04\n",
            "| 에폭 69 |  반복 1 / 2 | 시간 0[s] | 손실 4.01\n",
            "| 에폭 70 |  반복 1 / 2 | 시간 0[s] | 손실 4.01\n",
            "| 에폭 71 |  반복 1 / 2 | 시간 0[s] | 손실 3.98\n",
            "| 에폭 72 |  반복 1 / 2 | 시간 0[s] | 손실 4.02\n",
            "| 에폭 73 |  반복 1 / 2 | 시간 0[s] | 손실 3.99\n",
            "| 에폭 74 |  반복 1 / 2 | 시간 0[s] | 손실 3.99\n",
            "| 에폭 75 |  반복 1 / 2 | 시간 0[s] | 손실 3.98\n",
            "| 에폭 76 |  반복 1 / 2 | 시간 0[s] | 손실 3.98\n",
            "| 에폭 77 |  반복 1 / 2 | 시간 0[s] | 손실 4.00\n",
            "| 에폭 78 |  반복 1 / 2 | 시간 0[s] | 손실 3.96\n",
            "| 에폭 79 |  반복 1 / 2 | 시간 0[s] | 손실 3.97\n",
            "| 에폭 80 |  반복 1 / 2 | 시간 0[s] | 손실 3.98\n",
            "| 에폭 81 |  반복 1 / 2 | 시간 0[s] | 손실 3.95\n",
            "| 에폭 82 |  반복 1 / 2 | 시간 0[s] | 손실 3.94\n",
            "| 에폭 83 |  반복 1 / 2 | 시간 0[s] | 손실 3.92\n",
            "| 에폭 84 |  반복 1 / 2 | 시간 0[s] | 손실 3.94\n",
            "| 에폭 85 |  반복 1 / 2 | 시간 0[s] | 손실 3.94\n",
            "| 에폭 86 |  반복 1 / 2 | 시간 0[s] | 손실 3.89\n",
            "| 에폭 87 |  반복 1 / 2 | 시간 0[s] | 손실 3.93\n",
            "| 에폭 88 |  반복 1 / 2 | 시간 0[s] | 손실 3.90\n",
            "| 에폭 89 |  반복 1 / 2 | 시간 0[s] | 손실 3.91\n",
            "| 에폭 90 |  반복 1 / 2 | 시간 0[s] | 손실 3.93\n",
            "| 에폭 91 |  반복 1 / 2 | 시간 0[s] | 손실 3.91\n",
            "| 에폭 92 |  반복 1 / 2 | 시간 0[s] | 손실 3.88\n",
            "| 에폭 93 |  반복 1 / 2 | 시간 0[s] | 손실 3.87\n",
            "| 에폭 94 |  반복 1 / 2 | 시간 0[s] | 손실 3.83\n",
            "| 에폭 95 |  반복 1 / 2 | 시간 0[s] | 손실 3.87\n",
            "| 에폭 96 |  반복 1 / 2 | 시간 0[s] | 손실 3.88\n",
            "| 에폭 97 |  반복 1 / 2 | 시간 0[s] | 손실 3.87\n",
            "| 에폭 98 |  반복 1 / 2 | 시간 0[s] | 손실 3.81\n",
            "| 에폭 99 |  반복 1 / 2 | 시간 0[s] | 손실 3.88\n",
            "| 에폭 100 |  반복 1 / 2 | 시간 0[s] | 손실 3.79\n",
            "| 에폭 101 |  반복 1 / 2 | 시간 0[s] | 손실 3.86\n",
            "| 에폭 102 |  반복 1 / 2 | 시간 0[s] | 손실 3.77\n",
            "| 에폭 103 |  반복 1 / 2 | 시간 0[s] | 손실 3.86\n",
            "| 에폭 104 |  반복 1 / 2 | 시간 0[s] | 손실 3.82\n",
            "| 에폭 105 |  반복 1 / 2 | 시간 0[s] | 손실 3.80\n",
            "| 에폭 106 |  반복 1 / 2 | 시간 0[s] | 손실 3.78\n",
            "| 에폭 107 |  반복 1 / 2 | 시간 0[s] | 손실 3.79\n",
            "| 에폭 108 |  반복 1 / 2 | 시간 0[s] | 손실 3.76\n",
            "| 에폭 109 |  반복 1 / 2 | 시간 0[s] | 손실 3.76\n",
            "| 에폭 110 |  반복 1 / 2 | 시간 0[s] | 손실 3.76\n",
            "| 에폭 111 |  반복 1 / 2 | 시간 0[s] | 손실 3.80\n",
            "| 에폭 112 |  반복 1 / 2 | 시간 0[s] | 손실 3.77\n",
            "| 에폭 113 |  반복 1 / 2 | 시간 0[s] | 손실 3.68\n",
            "| 에폭 114 |  반복 1 / 2 | 시간 0[s] | 손실 3.74\n",
            "| 에폭 115 |  반복 1 / 2 | 시간 0[s] | 손실 3.72\n",
            "| 에폭 116 |  반복 1 / 2 | 시간 0[s] | 손실 3.70\n",
            "| 에폭 117 |  반복 1 / 2 | 시간 0[s] | 손실 3.76\n",
            "| 에폭 118 |  반복 1 / 2 | 시간 0[s] | 손실 3.68\n",
            "| 에폭 119 |  반복 1 / 2 | 시간 0[s] | 손실 3.70\n",
            "| 에폭 120 |  반복 1 / 2 | 시간 0[s] | 손실 3.66\n",
            "| 에폭 121 |  반복 1 / 2 | 시간 0[s] | 손실 3.70\n",
            "| 에폭 122 |  반복 1 / 2 | 시간 0[s] | 손실 3.65\n",
            "| 에폭 123 |  반복 1 / 2 | 시간 0[s] | 손실 3.64\n",
            "| 에폭 124 |  반복 1 / 2 | 시간 0[s] | 손실 3.69\n",
            "| 에폭 125 |  반복 1 / 2 | 시간 0[s] | 손실 3.62\n",
            "| 에폭 126 |  반복 1 / 2 | 시간 0[s] | 손실 3.62\n",
            "| 에폭 127 |  반복 1 / 2 | 시간 0[s] | 손실 3.65\n",
            "| 에폭 128 |  반복 1 / 2 | 시간 0[s] | 손실 3.62\n",
            "| 에폭 129 |  반복 1 / 2 | 시간 0[s] | 손실 3.66\n",
            "| 에폭 130 |  반복 1 / 2 | 시간 0[s] | 손실 3.56\n",
            "| 에폭 131 |  반복 1 / 2 | 시간 0[s] | 손실 3.63\n",
            "| 에폭 132 |  반복 1 / 2 | 시간 0[s] | 손실 3.59\n",
            "| 에폭 133 |  반복 1 / 2 | 시간 0[s] | 손실 3.56\n",
            "| 에폭 134 |  반복 1 / 2 | 시간 0[s] | 손실 3.51\n",
            "| 에폭 135 |  반복 1 / 2 | 시간 0[s] | 손실 3.61\n",
            "| 에폭 136 |  반복 1 / 2 | 시간 0[s] | 손실 3.59\n",
            "| 에폭 137 |  반복 1 / 2 | 시간 0[s] | 손실 3.51\n",
            "| 에폭 138 |  반복 1 / 2 | 시간 0[s] | 손실 3.52\n",
            "| 에폭 139 |  반복 1 / 2 | 시간 0[s] | 손실 3.54\n",
            "| 에폭 140 |  반복 1 / 2 | 시간 0[s] | 손실 3.48\n",
            "| 에폭 141 |  반복 1 / 2 | 시간 0[s] | 손실 3.52\n",
            "| 에폭 142 |  반복 1 / 2 | 시간 0[s] | 손실 3.51\n",
            "| 에폭 143 |  반복 1 / 2 | 시간 0[s] | 손실 3.52\n",
            "| 에폭 144 |  반복 1 / 2 | 시간 0[s] | 손실 3.52\n",
            "| 에폭 145 |  반복 1 / 2 | 시간 0[s] | 손실 3.53\n",
            "| 에폭 146 |  반복 1 / 2 | 시간 0[s] | 손실 3.43\n",
            "| 에폭 147 |  반복 1 / 2 | 시간 0[s] | 손실 3.53\n",
            "| 에폭 148 |  반복 1 / 2 | 시간 0[s] | 손실 3.49\n",
            "| 에폭 149 |  반복 1 / 2 | 시간 0[s] | 손실 3.44\n",
            "| 에폭 150 |  반복 1 / 2 | 시간 0[s] | 손실 3.43\n",
            "| 에폭 151 |  반복 1 / 2 | 시간 0[s] | 손실 3.42\n",
            "| 에폭 152 |  반복 1 / 2 | 시간 0[s] | 손실 3.47\n",
            "| 에폭 153 |  반복 1 / 2 | 시간 0[s] | 손실 3.34\n",
            "| 에폭 154 |  반복 1 / 2 | 시간 0[s] | 손실 3.39\n",
            "| 에폭 155 |  반복 1 / 2 | 시간 0[s] | 손실 3.34\n",
            "| 에폭 156 |  반복 1 / 2 | 시간 0[s] | 손실 3.48\n",
            "| 에폭 157 |  반복 1 / 2 | 시간 0[s] | 손실 3.40\n",
            "| 에폭 158 |  반복 1 / 2 | 시간 0[s] | 손실 3.31\n",
            "| 에폭 159 |  반복 1 / 2 | 시간 0[s] | 손실 3.34\n",
            "| 에폭 160 |  반복 1 / 2 | 시간 0[s] | 손실 3.28\n",
            "| 에폭 161 |  반복 1 / 2 | 시간 0[s] | 손실 3.44\n",
            "| 에폭 162 |  반복 1 / 2 | 시간 0[s] | 손실 3.34\n",
            "| 에폭 163 |  반복 1 / 2 | 시간 0[s] | 손실 3.33\n",
            "| 에폭 164 |  반복 1 / 2 | 시간 0[s] | 손실 3.34\n",
            "| 에폭 165 |  반복 1 / 2 | 시간 0[s] | 손실 3.33\n",
            "| 에폭 166 |  반복 1 / 2 | 시간 0[s] | 손실 3.42\n",
            "| 에폭 167 |  반복 1 / 2 | 시간 0[s] | 손실 3.39\n",
            "| 에폭 168 |  반복 1 / 2 | 시간 0[s] | 손실 3.27\n",
            "| 에폭 169 |  반복 1 / 2 | 시간 0[s] | 손실 3.33\n",
            "| 에폭 170 |  반복 1 / 2 | 시간 0[s] | 손실 3.24\n",
            "| 에폭 171 |  반복 1 / 2 | 시간 0[s] | 손실 3.35\n",
            "| 에폭 172 |  반복 1 / 2 | 시간 0[s] | 손실 3.17\n",
            "| 에폭 173 |  반복 1 / 2 | 시간 0[s] | 손실 3.30\n",
            "| 에폭 174 |  반복 1 / 2 | 시간 0[s] | 손실 3.37\n",
            "| 에폭 175 |  반복 1 / 2 | 시간 0[s] | 손실 3.12\n",
            "| 에폭 176 |  반복 1 / 2 | 시간 0[s] | 손실 3.25\n",
            "| 에폭 177 |  반복 1 / 2 | 시간 0[s] | 손실 3.13\n",
            "| 에폭 178 |  반복 1 / 2 | 시간 0[s] | 손실 3.27\n",
            "| 에폭 179 |  반복 1 / 2 | 시간 0[s] | 손실 3.24\n",
            "| 에폭 180 |  반복 1 / 2 | 시간 0[s] | 손실 3.20\n",
            "| 에폭 181 |  반복 1 / 2 | 시간 0[s] | 손실 3.21\n",
            "| 에폭 182 |  반복 1 / 2 | 시간 0[s] | 손실 3.34\n",
            "| 에폭 183 |  반복 1 / 2 | 시간 0[s] | 손실 3.19\n",
            "| 에폭 184 |  반복 1 / 2 | 시간 0[s] | 손실 3.08\n",
            "| 에폭 185 |  반복 1 / 2 | 시간 0[s] | 손실 3.30\n",
            "| 에폭 186 |  반복 1 / 2 | 시간 0[s] | 손실 3.14\n",
            "| 에폭 187 |  반복 1 / 2 | 시간 0[s] | 손실 3.26\n",
            "| 에폭 188 |  반복 1 / 2 | 시간 0[s] | 손실 3.06\n",
            "| 에폭 189 |  반복 1 / 2 | 시간 0[s] | 손실 3.25\n",
            "| 에폭 190 |  반복 1 / 2 | 시간 0[s] | 손실 3.00\n",
            "| 에폭 191 |  반복 1 / 2 | 시간 0[s] | 손실 3.19\n",
            "| 에폭 192 |  반복 1 / 2 | 시간 0[s] | 손실 3.25\n",
            "| 에폭 193 |  반복 1 / 2 | 시간 0[s] | 손실 2.98\n",
            "| 에폭 194 |  반복 1 / 2 | 시간 0[s] | 손실 3.15\n",
            "| 에폭 195 |  반복 1 / 2 | 시간 0[s] | 손실 2.93\n",
            "| 에폭 196 |  반복 1 / 2 | 시간 0[s] | 손실 3.22\n",
            "| 에폭 197 |  반복 1 / 2 | 시간 0[s] | 손실 3.22\n",
            "| 에폭 198 |  반복 1 / 2 | 시간 0[s] | 손실 3.00\n",
            "| 에폭 199 |  반복 1 / 2 | 시간 0[s] | 손실 3.03\n",
            "| 에폭 200 |  반복 1 / 2 | 시간 0[s] | 손실 3.14\n",
            "| 에폭 201 |  반복 1 / 2 | 시간 0[s] | 손실 3.02\n",
            "| 에폭 202 |  반복 1 / 2 | 시간 0[s] | 손실 3.18\n",
            "| 에폭 203 |  반복 1 / 2 | 시간 0[s] | 손실 3.04\n",
            "| 에폭 204 |  반복 1 / 2 | 시간 0[s] | 손실 3.10\n",
            "| 에폭 205 |  반복 1 / 2 | 시간 0[s] | 손실 3.05\n",
            "| 에폭 206 |  반복 1 / 2 | 시간 0[s] | 손실 3.03\n",
            "| 에폭 207 |  반복 1 / 2 | 시간 0[s] | 손실 3.15\n",
            "| 에폭 208 |  반복 1 / 2 | 시간 0[s] | 손실 3.01\n",
            "| 에폭 209 |  반복 1 / 2 | 시간 0[s] | 손실 2.87\n",
            "| 에폭 210 |  반복 1 / 2 | 시간 0[s] | 손실 3.16\n",
            "| 에폭 211 |  반복 1 / 2 | 시간 0[s] | 손실 2.95\n",
            "| 에폭 212 |  반복 1 / 2 | 시간 0[s] | 손실 3.04\n",
            "| 에폭 213 |  반복 1 / 2 | 시간 0[s] | 손실 3.13\n",
            "| 에폭 214 |  반복 1 / 2 | 시간 0[s] | 손실 3.00\n",
            "| 에폭 215 |  반복 1 / 2 | 시간 0[s] | 손실 3.07\n",
            "| 에폭 216 |  반복 1 / 2 | 시간 0[s] | 손실 2.92\n",
            "| 에폭 217 |  반복 1 / 2 | 시간 0[s] | 손실 2.99\n",
            "| 에폭 218 |  반복 1 / 2 | 시간 0[s] | 손실 2.91\n",
            "| 에폭 219 |  반복 1 / 2 | 시간 0[s] | 손실 3.06\n",
            "| 에폭 220 |  반복 1 / 2 | 시간 0[s] | 손실 3.00\n",
            "| 에폭 221 |  반복 1 / 2 | 시간 0[s] | 손실 2.79\n",
            "| 에폭 222 |  반복 1 / 2 | 시간 0[s] | 손실 3.14\n",
            "| 에폭 223 |  반복 1 / 2 | 시간 0[s] | 손실 3.00\n",
            "| 에폭 224 |  반복 1 / 2 | 시간 0[s] | 손실 2.88\n",
            "| 에폭 225 |  반복 1 / 2 | 시간 0[s] | 손실 2.94\n",
            "| 에폭 226 |  반복 1 / 2 | 시간 0[s] | 손실 2.91\n",
            "| 에폭 227 |  반복 1 / 2 | 시간 0[s] | 손실 2.77\n",
            "| 에폭 228 |  반복 1 / 2 | 시간 0[s] | 손실 2.90\n",
            "| 에폭 229 |  반복 1 / 2 | 시간 0[s] | 손실 2.83\n",
            "| 에폭 230 |  반복 1 / 2 | 시간 0[s] | 손실 2.95\n",
            "| 에폭 231 |  반복 1 / 2 | 시간 0[s] | 손실 2.88\n",
            "| 에폭 232 |  반복 1 / 2 | 시간 0[s] | 손실 2.94\n",
            "| 에폭 233 |  반복 1 / 2 | 시간 0[s] | 손실 2.91\n",
            "| 에폭 234 |  반복 1 / 2 | 시간 0[s] | 손실 2.71\n",
            "| 에폭 235 |  반복 1 / 2 | 시간 0[s] | 손실 3.03\n",
            "| 에폭 236 |  반복 1 / 2 | 시간 0[s] | 손실 2.82\n",
            "| 에폭 237 |  반복 1 / 2 | 시간 0[s] | 손실 2.86\n",
            "| 에폭 238 |  반복 1 / 2 | 시간 0[s] | 손실 2.78\n",
            "| 에폭 239 |  반복 1 / 2 | 시간 0[s] | 손실 2.86\n",
            "| 에폭 240 |  반복 1 / 2 | 시간 0[s] | 손실 3.02\n",
            "| 에폭 241 |  반복 1 / 2 | 시간 0[s] | 손실 2.86\n",
            "| 에폭 242 |  반복 1 / 2 | 시간 0[s] | 손실 2.74\n",
            "| 에폭 243 |  반복 1 / 2 | 시간 0[s] | 손실 2.88\n",
            "| 에폭 244 |  반복 1 / 2 | 시간 0[s] | 손실 2.89\n",
            "| 에폭 245 |  반복 1 / 2 | 시간 0[s] | 손실 2.76\n",
            "| 에폭 246 |  반복 1 / 2 | 시간 0[s] | 손실 2.83\n",
            "| 에폭 247 |  반복 1 / 2 | 시간 0[s] | 손실 2.76\n",
            "| 에폭 248 |  반복 1 / 2 | 시간 0[s] | 손실 2.74\n",
            "| 에폭 249 |  반복 1 / 2 | 시간 0[s] | 손실 2.82\n",
            "| 에폭 250 |  반복 1 / 2 | 시간 0[s] | 손실 2.76\n",
            "| 에폭 251 |  반복 1 / 2 | 시간 0[s] | 손실 2.88\n",
            "| 에폭 252 |  반복 1 / 2 | 시간 0[s] | 손실 2.71\n",
            "| 에폭 253 |  반복 1 / 2 | 시간 0[s] | 손실 2.84\n",
            "| 에폭 254 |  반복 1 / 2 | 시간 0[s] | 손실 2.78\n",
            "| 에폭 255 |  반복 1 / 2 | 시간 0[s] | 손실 2.63\n",
            "| 에폭 256 |  반복 1 / 2 | 시간 0[s] | 손실 2.82\n",
            "| 에폭 257 |  반복 1 / 2 | 시간 0[s] | 손실 2.66\n",
            "| 에폭 258 |  반복 1 / 2 | 시간 0[s] | 손실 2.77\n",
            "| 에폭 259 |  반복 1 / 2 | 시간 0[s] | 손실 2.76\n",
            "| 에폭 260 |  반복 1 / 2 | 시간 0[s] | 손실 2.66\n",
            "| 에폭 261 |  반복 1 / 2 | 시간 0[s] | 손실 2.95\n",
            "| 에폭 262 |  반복 1 / 2 | 시간 0[s] | 손실 2.65\n",
            "| 에폭 263 |  반복 1 / 2 | 시간 0[s] | 손실 2.74\n",
            "| 에폭 264 |  반복 1 / 2 | 시간 0[s] | 손실 2.63\n",
            "| 에폭 265 |  반복 1 / 2 | 시간 0[s] | 손실 2.74\n",
            "| 에폭 266 |  반복 1 / 2 | 시간 0[s] | 손실 2.73\n",
            "| 에폭 267 |  반복 1 / 2 | 시간 0[s] | 손실 2.76\n",
            "| 에폭 268 |  반복 1 / 2 | 시간 0[s] | 손실 2.72\n",
            "| 에폭 269 |  반복 1 / 2 | 시간 0[s] | 손실 2.67\n",
            "| 에폭 270 |  반복 1 / 2 | 시간 0[s] | 손실 2.49\n",
            "| 에폭 271 |  반복 1 / 2 | 시간 0[s] | 손실 2.86\n",
            "| 에폭 272 |  반복 1 / 2 | 시간 0[s] | 손실 2.69\n",
            "| 에폭 273 |  반복 1 / 2 | 시간 0[s] | 손실 2.75\n",
            "| 에폭 274 |  반복 1 / 2 | 시간 0[s] | 손실 2.65\n",
            "| 에폭 275 |  반복 1 / 2 | 시간 0[s] | 손실 2.70\n",
            "| 에폭 276 |  반복 1 / 2 | 시간 0[s] | 손실 2.68\n",
            "| 에폭 277 |  반복 1 / 2 | 시간 0[s] | 손실 2.63\n",
            "| 에폭 278 |  반복 1 / 2 | 시간 0[s] | 손실 2.79\n",
            "| 에폭 279 |  반복 1 / 2 | 시간 0[s] | 손실 2.78\n",
            "| 에폭 280 |  반복 1 / 2 | 시간 0[s] | 손실 2.67\n",
            "| 에폭 281 |  반복 1 / 2 | 시간 0[s] | 손실 2.43\n",
            "| 에폭 282 |  반복 1 / 2 | 시간 0[s] | 손실 2.83\n",
            "| 에폭 283 |  반복 1 / 2 | 시간 0[s] | 손실 2.54\n",
            "| 에폭 284 |  반복 1 / 2 | 시간 0[s] | 손실 2.54\n",
            "| 에폭 285 |  반복 1 / 2 | 시간 0[s] | 손실 2.70\n",
            "| 에폭 286 |  반복 1 / 2 | 시간 0[s] | 손실 2.54\n",
            "| 에폭 287 |  반복 1 / 2 | 시간 0[s] | 손실 2.53\n",
            "| 에폭 288 |  반복 1 / 2 | 시간 0[s] | 손실 2.64\n",
            "| 에폭 289 |  반복 1 / 2 | 시간 0[s] | 손실 2.86\n",
            "| 에폭 290 |  반복 1 / 2 | 시간 0[s] | 손실 2.50\n",
            "| 에폭 291 |  반복 1 / 2 | 시간 0[s] | 손실 2.63\n",
            "| 에폭 292 |  반복 1 / 2 | 시간 0[s] | 손실 2.57\n",
            "| 에폭 293 |  반복 1 / 2 | 시간 0[s] | 손실 2.78\n",
            "| 에폭 294 |  반복 1 / 2 | 시간 0[s] | 손실 2.73\n",
            "| 에폭 295 |  반복 1 / 2 | 시간 0[s] | 손실 2.51\n",
            "| 에폭 296 |  반복 1 / 2 | 시간 0[s] | 손실 2.77\n",
            "| 에폭 297 |  반복 1 / 2 | 시간 0[s] | 손실 2.55\n",
            "| 에폭 298 |  반복 1 / 2 | 시간 0[s] | 손실 2.65\n",
            "| 에폭 299 |  반복 1 / 2 | 시간 0[s] | 손실 2.65\n",
            "| 에폭 300 |  반복 1 / 2 | 시간 0[s] | 손실 2.53\n",
            "| 에폭 301 |  반복 1 / 2 | 시간 0[s] | 손실 2.70\n",
            "| 에폭 302 |  반복 1 / 2 | 시간 0[s] | 손실 2.62\n",
            "| 에폭 303 |  반복 1 / 2 | 시간 0[s] | 손실 2.49\n",
            "| 에폭 304 |  반복 1 / 2 | 시간 0[s] | 손실 2.45\n",
            "| 에폭 305 |  반복 1 / 2 | 시간 0[s] | 손실 2.74\n",
            "| 에폭 306 |  반복 1 / 2 | 시간 0[s] | 손실 2.55\n",
            "| 에폭 307 |  반복 1 / 2 | 시간 0[s] | 손실 2.73\n",
            "| 에폭 308 |  반복 1 / 2 | 시간 0[s] | 손실 2.38\n",
            "| 에폭 309 |  반복 1 / 2 | 시간 0[s] | 손실 2.48\n",
            "| 에폭 310 |  반복 1 / 2 | 시간 0[s] | 손실 2.42\n",
            "| 에폭 311 |  반복 1 / 2 | 시간 0[s] | 손실 2.81\n",
            "| 에폭 312 |  반복 1 / 2 | 시간 0[s] | 손실 2.37\n",
            "| 에폭 313 |  반복 1 / 2 | 시간 0[s] | 손실 2.69\n",
            "| 에폭 314 |  반복 1 / 2 | 시간 0[s] | 손실 2.45\n",
            "| 에폭 315 |  반복 1 / 2 | 시간 0[s] | 손실 2.48\n",
            "| 에폭 316 |  반복 1 / 2 | 시간 0[s] | 손실 2.64\n",
            "| 에폭 317 |  반복 1 / 2 | 시간 0[s] | 손실 2.35\n",
            "| 에폭 318 |  반복 1 / 2 | 시간 0[s] | 손실 2.37\n",
            "| 에폭 319 |  반복 1 / 2 | 시간 0[s] | 손실 2.42\n",
            "| 에폭 320 |  반복 1 / 2 | 시간 0[s] | 손실 2.36\n",
            "| 에폭 321 |  반복 1 / 2 | 시간 0[s] | 손실 2.46\n",
            "| 에폭 322 |  반복 1 / 2 | 시간 0[s] | 손실 2.59\n",
            "| 에폭 323 |  반복 1 / 2 | 시간 0[s] | 손실 2.59\n",
            "| 에폭 324 |  반복 1 / 2 | 시간 0[s] | 손실 2.25\n",
            "| 에폭 325 |  반복 1 / 2 | 시간 0[s] | 손실 2.58\n",
            "| 에폭 326 |  반복 1 / 2 | 시간 0[s] | 손실 2.38\n",
            "| 에폭 327 |  반복 1 / 2 | 시간 0[s] | 손실 2.48\n",
            "| 에폭 328 |  반복 1 / 2 | 시간 0[s] | 손실 2.32\n",
            "| 에폭 329 |  반복 1 / 2 | 시간 0[s] | 손실 2.48\n",
            "| 에폭 330 |  반복 1 / 2 | 시간 0[s] | 손실 2.32\n",
            "| 에폭 331 |  반복 1 / 2 | 시간 0[s] | 손실 2.52\n",
            "| 에폭 332 |  반복 1 / 2 | 시간 0[s] | 손실 2.57\n",
            "| 에폭 333 |  반복 1 / 2 | 시간 0[s] | 손실 2.33\n",
            "| 에폭 334 |  반복 1 / 2 | 시간 0[s] | 손실 2.35\n",
            "| 에폭 335 |  반복 1 / 2 | 시간 0[s] | 손실 2.52\n",
            "| 에폭 336 |  반복 1 / 2 | 시간 0[s] | 손실 2.49\n",
            "| 에폭 337 |  반복 1 / 2 | 시간 0[s] | 손실 2.25\n",
            "| 에폭 338 |  반복 1 / 2 | 시간 0[s] | 손실 2.36\n",
            "| 에폭 339 |  반복 1 / 2 | 시간 0[s] | 손실 2.24\n",
            "| 에폭 340 |  반복 1 / 2 | 시간 0[s] | 손실 2.24\n",
            "| 에폭 341 |  반복 1 / 2 | 시간 0[s] | 손실 2.42\n",
            "| 에폭 342 |  반복 1 / 2 | 시간 0[s] | 손실 2.55\n",
            "| 에폭 343 |  반복 1 / 2 | 시간 0[s] | 손실 2.56\n",
            "| 에폭 344 |  반복 1 / 2 | 시간 0[s] | 손실 2.24\n",
            "| 에폭 345 |  반복 1 / 2 | 시간 0[s] | 손실 2.43\n",
            "| 에폭 346 |  반복 1 / 2 | 시간 0[s] | 손실 2.41\n",
            "| 에폭 347 |  반복 1 / 2 | 시간 0[s] | 손실 2.41\n",
            "| 에폭 348 |  반복 1 / 2 | 시간 0[s] | 손실 2.52\n",
            "| 에폭 349 |  반복 1 / 2 | 시간 0[s] | 손실 2.34\n",
            "| 에폭 350 |  반복 1 / 2 | 시간 0[s] | 손실 2.45\n",
            "| 에폭 351 |  반복 1 / 2 | 시간 0[s] | 손실 2.44\n",
            "| 에폭 352 |  반복 1 / 2 | 시간 0[s] | 손실 2.31\n",
            "| 에폭 353 |  반복 1 / 2 | 시간 0[s] | 손실 2.59\n",
            "| 에폭 354 |  반복 1 / 2 | 시간 0[s] | 손실 2.27\n",
            "| 에폭 355 |  반복 1 / 2 | 시간 0[s] | 손실 2.22\n",
            "| 에폭 356 |  반복 1 / 2 | 시간 0[s] | 손실 2.40\n",
            "| 에폭 357 |  반복 1 / 2 | 시간 0[s] | 손실 2.57\n",
            "| 에폭 358 |  반복 1 / 2 | 시간 0[s] | 손실 2.12\n",
            "| 에폭 359 |  반복 1 / 2 | 시간 0[s] | 손실 2.53\n",
            "| 에폭 360 |  반복 1 / 2 | 시간 0[s] | 손실 2.65\n",
            "| 에폭 361 |  반복 1 / 2 | 시간 0[s] | 손실 2.28\n",
            "| 에폭 362 |  반복 1 / 2 | 시간 0[s] | 손실 2.09\n",
            "| 에폭 363 |  반복 1 / 2 | 시간 0[s] | 손실 2.40\n",
            "| 에폭 364 |  반복 1 / 2 | 시간 0[s] | 손실 2.54\n",
            "| 에폭 365 |  반복 1 / 2 | 시간 0[s] | 손실 2.61\n",
            "| 에폭 366 |  반복 1 / 2 | 시간 0[s] | 손실 2.14\n",
            "| 에폭 367 |  반복 1 / 2 | 시간 0[s] | 손실 2.25\n",
            "| 에폭 368 |  반복 1 / 2 | 시간 0[s] | 손실 2.22\n",
            "| 에폭 369 |  반복 1 / 2 | 시간 0[s] | 손실 2.38\n",
            "| 에폭 370 |  반복 1 / 2 | 시간 0[s] | 손실 2.44\n",
            "| 에폭 371 |  반복 1 / 2 | 시간 0[s] | 손실 2.24\n",
            "| 에폭 372 |  반복 1 / 2 | 시간 0[s] | 손실 2.20\n",
            "| 에폭 373 |  반복 1 / 2 | 시간 0[s] | 손실 2.23\n",
            "| 에폭 374 |  반복 1 / 2 | 시간 0[s] | 손실 2.46\n",
            "| 에폭 375 |  반복 1 / 2 | 시간 0[s] | 손실 1.97\n",
            "| 에폭 376 |  반복 1 / 2 | 시간 0[s] | 손실 2.35\n",
            "| 에폭 377 |  반복 1 / 2 | 시간 0[s] | 손실 2.57\n",
            "| 에폭 378 |  반복 1 / 2 | 시간 0[s] | 손실 1.96\n",
            "| 에폭 379 |  반복 1 / 2 | 시간 0[s] | 손실 2.42\n",
            "| 에폭 380 |  반복 1 / 2 | 시간 0[s] | 손실 2.21\n",
            "| 에폭 381 |  반복 1 / 2 | 시간 0[s] | 손실 2.41\n",
            "| 에폭 382 |  반복 1 / 2 | 시간 0[s] | 손실 2.33\n",
            "| 에폭 383 |  반복 1 / 2 | 시간 0[s] | 손실 2.38\n",
            "| 에폭 384 |  반복 1 / 2 | 시간 0[s] | 손실 2.28\n",
            "| 에폭 385 |  반복 1 / 2 | 시간 0[s] | 손실 2.55\n",
            "| 에폭 386 |  반복 1 / 2 | 시간 0[s] | 손실 2.01\n",
            "| 에폭 387 |  반복 1 / 2 | 시간 0[s] | 손실 2.78\n",
            "| 에폭 388 |  반복 1 / 2 | 시간 0[s] | 손실 2.16\n",
            "| 에폭 389 |  반복 1 / 2 | 시간 0[s] | 손실 2.26\n",
            "| 에폭 390 |  반복 1 / 2 | 시간 0[s] | 손실 2.45\n",
            "| 에폭 391 |  반복 1 / 2 | 시간 0[s] | 손실 2.59\n",
            "| 에폭 392 |  반복 1 / 2 | 시간 0[s] | 손실 2.13\n",
            "| 에폭 393 |  반복 1 / 2 | 시간 0[s] | 손실 2.44\n",
            "| 에폭 394 |  반복 1 / 2 | 시간 0[s] | 손실 2.32\n",
            "| 에폭 395 |  반복 1 / 2 | 시간 0[s] | 손실 2.04\n",
            "| 에폭 396 |  반복 1 / 2 | 시간 0[s] | 손실 2.64\n",
            "| 에폭 397 |  반복 1 / 2 | 시간 0[s] | 손실 2.10\n",
            "| 에폭 398 |  반복 1 / 2 | 시간 0[s] | 손실 2.00\n",
            "| 에폭 399 |  반복 1 / 2 | 시간 0[s] | 손실 2.74\n",
            "| 에폭 400 |  반복 1 / 2 | 시간 0[s] | 손실 1.96\n",
            "| 에폭 401 |  반복 1 / 2 | 시간 0[s] | 손실 2.56\n",
            "| 에폭 402 |  반복 1 / 2 | 시간 0[s] | 손실 2.24\n",
            "| 에폭 403 |  반복 1 / 2 | 시간 0[s] | 손실 2.46\n",
            "| 에폭 404 |  반복 1 / 2 | 시간 0[s] | 손실 2.35\n",
            "| 에폭 405 |  반복 1 / 2 | 시간 0[s] | 손실 2.45\n",
            "| 에폭 406 |  반복 1 / 2 | 시간 0[s] | 손실 2.05\n",
            "| 에폭 407 |  반복 1 / 2 | 시간 0[s] | 손실 2.14\n",
            "| 에폭 408 |  반복 1 / 2 | 시간 0[s] | 손실 2.34\n",
            "| 에폭 409 |  반복 1 / 2 | 시간 0[s] | 손실 2.33\n",
            "| 에폭 410 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 411 |  반복 1 / 2 | 시간 0[s] | 손실 2.38\n",
            "| 에폭 412 |  반복 1 / 2 | 시간 0[s] | 손실 2.24\n",
            "| 에폭 413 |  반복 1 / 2 | 시간 0[s] | 손실 2.34\n",
            "| 에폭 414 |  반복 1 / 2 | 시간 0[s] | 손실 2.30\n",
            "| 에폭 415 |  반복 1 / 2 | 시간 0[s] | 손실 2.14\n",
            "| 에폭 416 |  반복 1 / 2 | 시간 0[s] | 손실 2.06\n",
            "| 에폭 417 |  반복 1 / 2 | 시간 0[s] | 손실 2.19\n",
            "| 에폭 418 |  반복 1 / 2 | 시간 0[s] | 손실 2.27\n",
            "| 에폭 419 |  반복 1 / 2 | 시간 0[s] | 손실 2.20\n",
            "| 에폭 420 |  반복 1 / 2 | 시간 0[s] | 손실 2.12\n",
            "| 에폭 421 |  반복 1 / 2 | 시간 0[s] | 손실 2.15\n",
            "| 에폭 422 |  반복 1 / 2 | 시간 0[s] | 손실 2.28\n",
            "| 에폭 423 |  반복 1 / 2 | 시간 0[s] | 손실 2.23\n",
            "| 에폭 424 |  반복 1 / 2 | 시간 0[s] | 손실 2.37\n",
            "| 에폭 425 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 426 |  반복 1 / 2 | 시간 0[s] | 손실 2.52\n",
            "| 에폭 427 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
            "| 에폭 428 |  반복 1 / 2 | 시간 0[s] | 손실 2.03\n",
            "| 에폭 429 |  반복 1 / 2 | 시간 0[s] | 손실 2.29\n",
            "| 에폭 430 |  반복 1 / 2 | 시간 0[s] | 손실 2.29\n",
            "| 에폭 431 |  반복 1 / 2 | 시간 0[s] | 손실 2.55\n",
            "| 에폭 432 |  반복 1 / 2 | 시간 0[s] | 손실 2.00\n",
            "| 에폭 433 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 434 |  반복 1 / 2 | 시간 0[s] | 손실 2.54\n",
            "| 에폭 435 |  반복 1 / 2 | 시간 0[s] | 손실 2.09\n",
            "| 에폭 436 |  반복 1 / 2 | 시간 0[s] | 손실 2.18\n",
            "| 에폭 437 |  반복 1 / 2 | 시간 0[s] | 손실 2.36\n",
            "| 에폭 438 |  반복 1 / 2 | 시간 0[s] | 손실 1.99\n",
            "| 에폭 439 |  반복 1 / 2 | 시간 0[s] | 손실 2.46\n",
            "| 에폭 440 |  반복 1 / 2 | 시간 0[s] | 손실 2.41\n",
            "| 에폭 441 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 442 |  반복 1 / 2 | 시간 0[s] | 손실 2.26\n",
            "| 에폭 443 |  반복 1 / 2 | 시간 0[s] | 손실 2.39\n",
            "| 에폭 444 |  반복 1 / 2 | 시간 0[s] | 손실 2.02\n",
            "| 에폭 445 |  반복 1 / 2 | 시간 0[s] | 손실 2.11\n",
            "| 에폭 446 |  반복 1 / 2 | 시간 0[s] | 손실 2.27\n",
            "| 에폭 447 |  반복 1 / 2 | 시간 0[s] | 손실 2.05\n",
            "| 에폭 448 |  반복 1 / 2 | 시간 0[s] | 손실 2.43\n",
            "| 에폭 449 |  반복 1 / 2 | 시간 0[s] | 손실 2.06\n",
            "| 에폭 450 |  반복 1 / 2 | 시간 0[s] | 손실 2.04\n",
            "| 에폭 451 |  반복 1 / 2 | 시간 0[s] | 손실 2.01\n",
            "| 에폭 452 |  반복 1 / 2 | 시간 0[s] | 손실 2.19\n",
            "| 에폭 453 |  반복 1 / 2 | 시간 0[s] | 손실 2.25\n",
            "| 에폭 454 |  반복 1 / 2 | 시간 0[s] | 손실 1.99\n",
            "| 에폭 455 |  반복 1 / 2 | 시간 0[s] | 손실 2.04\n",
            "| 에폭 456 |  반복 1 / 2 | 시간 0[s] | 손실 2.23\n",
            "| 에폭 457 |  반복 1 / 2 | 시간 0[s] | 손실 2.26\n",
            "| 에폭 458 |  반복 1 / 2 | 시간 0[s] | 손실 2.14\n",
            "| 에폭 459 |  반복 1 / 2 | 시간 0[s] | 손실 2.09\n",
            "| 에폭 460 |  반복 1 / 2 | 시간 0[s] | 손실 2.08\n",
            "| 에폭 461 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 462 |  반복 1 / 2 | 시간 0[s] | 손실 2.07\n",
            "| 에폭 463 |  반복 1 / 2 | 시간 0[s] | 손실 2.25\n",
            "| 에폭 464 |  반복 1 / 2 | 시간 0[s] | 손실 2.25\n",
            "| 에폭 465 |  반복 1 / 2 | 시간 0[s] | 손실 2.25\n",
            "| 에폭 466 |  반복 1 / 2 | 시간 0[s] | 손실 2.48\n",
            "| 에폭 467 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 468 |  반복 1 / 2 | 시간 0[s] | 손실 2.17\n",
            "| 에폭 469 |  반복 1 / 2 | 시간 1[s] | 손실 2.46\n",
            "| 에폭 470 |  반복 1 / 2 | 시간 1[s] | 손실 2.00\n",
            "| 에폭 471 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 472 |  반복 1 / 2 | 시간 1[s] | 손실 2.24\n",
            "| 에폭 473 |  반복 1 / 2 | 시간 1[s] | 손실 2.41\n",
            "| 에폭 474 |  반복 1 / 2 | 시간 1[s] | 손실 1.98\n",
            "| 에폭 475 |  반복 1 / 2 | 시간 1[s] | 손실 2.45\n",
            "| 에폭 476 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 477 |  반복 1 / 2 | 시간 1[s] | 손실 2.24\n",
            "| 에폭 478 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 479 |  반복 1 / 2 | 시간 1[s] | 손실 2.27\n",
            "| 에폭 480 |  반복 1 / 2 | 시간 1[s] | 손실 1.98\n",
            "| 에폭 481 |  반복 1 / 2 | 시간 1[s] | 손실 2.22\n",
            "| 에폭 482 |  반복 1 / 2 | 시간 1[s] | 손실 2.24\n",
            "| 에폭 483 |  반복 1 / 2 | 시간 1[s] | 손실 2.04\n",
            "| 에폭 484 |  반복 1 / 2 | 시간 1[s] | 손실 2.05\n",
            "| 에폭 485 |  반복 1 / 2 | 시간 1[s] | 손실 2.06\n",
            "| 에폭 486 |  반복 1 / 2 | 시간 1[s] | 손실 2.43\n",
            "| 에폭 487 |  반복 1 / 2 | 시간 1[s] | 손실 2.18\n",
            "| 에폭 488 |  반복 1 / 2 | 시간 1[s] | 손실 2.34\n",
            "| 에폭 489 |  반복 1 / 2 | 시간 1[s] | 손실 1.74\n",
            "| 에폭 490 |  반복 1 / 2 | 시간 1[s] | 손실 2.21\n",
            "| 에폭 491 |  반복 1 / 2 | 시간 1[s] | 손실 2.43\n",
            "| 에폭 492 |  반복 1 / 2 | 시간 1[s] | 손실 2.10\n",
            "| 에폭 493 |  반복 1 / 2 | 시간 1[s] | 손실 1.98\n",
            "| 에폭 494 |  반복 1 / 2 | 시간 1[s] | 손실 2.37\n",
            "| 에폭 495 |  반복 1 / 2 | 시간 1[s] | 손실 1.80\n",
            "| 에폭 496 |  반복 1 / 2 | 시간 1[s] | 손실 2.39\n",
            "| 에폭 497 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 498 |  반복 1 / 2 | 시간 1[s] | 손실 2.10\n",
            "| 에폭 499 |  반복 1 / 2 | 시간 1[s] | 손실 2.01\n",
            "| 에폭 500 |  반복 1 / 2 | 시간 1[s] | 손실 2.13\n",
            "| 에폭 501 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 502 |  반복 1 / 2 | 시간 1[s] | 손실 2.22\n",
            "| 에폭 503 |  반복 1 / 2 | 시간 1[s] | 손실 1.89\n",
            "| 에폭 504 |  반복 1 / 2 | 시간 1[s] | 손실 2.21\n",
            "| 에폭 505 |  반복 1 / 2 | 시간 1[s] | 손실 1.70\n",
            "| 에폭 506 |  반복 1 / 2 | 시간 1[s] | 손실 2.60\n",
            "| 에폭 507 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 508 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 509 |  반복 1 / 2 | 시간 1[s] | 손실 2.43\n",
            "| 에폭 510 |  반복 1 / 2 | 시간 1[s] | 손실 2.10\n",
            "| 에폭 511 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 512 |  반복 1 / 2 | 시간 1[s] | 손실 2.12\n",
            "| 에폭 513 |  반복 1 / 2 | 시간 1[s] | 손실 2.46\n",
            "| 에폭 514 |  반복 1 / 2 | 시간 1[s] | 손실 1.90\n",
            "| 에폭 515 |  반복 1 / 2 | 시간 1[s] | 손실 2.37\n",
            "| 에폭 516 |  반복 1 / 2 | 시간 1[s] | 손실 1.89\n",
            "| 에폭 517 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 518 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 519 |  반복 1 / 2 | 시간 1[s] | 손실 2.39\n",
            "| 에폭 520 |  반복 1 / 2 | 시간 1[s] | 손실 1.64\n",
            "| 에폭 521 |  반복 1 / 2 | 시간 1[s] | 손실 2.14\n",
            "| 에폭 522 |  반복 1 / 2 | 시간 1[s] | 손실 2.39\n",
            "| 에폭 523 |  반복 1 / 2 | 시간 1[s] | 손실 2.01\n",
            "| 에폭 524 |  반복 1 / 2 | 시간 1[s] | 손실 1.88\n",
            "| 에폭 525 |  반복 1 / 2 | 시간 1[s] | 손실 2.10\n",
            "| 에폭 526 |  반복 1 / 2 | 시간 1[s] | 손실 1.72\n",
            "| 에폭 527 |  반복 1 / 2 | 시간 1[s] | 손실 2.17\n",
            "| 에폭 528 |  반복 1 / 2 | 시간 1[s] | 손실 2.16\n",
            "| 에폭 529 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 530 |  반복 1 / 2 | 시간 1[s] | 손실 1.75\n",
            "| 에폭 531 |  반복 1 / 2 | 시간 1[s] | 손실 2.29\n",
            "| 에폭 532 |  반복 1 / 2 | 시간 1[s] | 손실 1.96\n",
            "| 에폭 533 |  반복 1 / 2 | 시간 1[s] | 손실 2.13\n",
            "| 에폭 534 |  반복 1 / 2 | 시간 1[s] | 손실 2.13\n",
            "| 에폭 535 |  반복 1 / 2 | 시간 1[s] | 손실 2.35\n",
            "| 에폭 536 |  반복 1 / 2 | 시간 1[s] | 손실 1.61\n",
            "| 에폭 537 |  반복 1 / 2 | 시간 1[s] | 손실 2.34\n",
            "| 에폭 538 |  반복 1 / 2 | 시간 1[s] | 손실 1.90\n",
            "| 에폭 539 |  반복 1 / 2 | 시간 1[s] | 손실 2.02\n",
            "| 에폭 540 |  반복 1 / 2 | 시간 1[s] | 손실 1.89\n",
            "| 에폭 541 |  반복 1 / 2 | 시간 1[s] | 손실 2.12\n",
            "| 에폭 542 |  반복 1 / 2 | 시간 1[s] | 손실 2.34\n",
            "| 에폭 543 |  반복 1 / 2 | 시간 1[s] | 손실 2.03\n",
            "| 에폭 544 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 545 |  반복 1 / 2 | 시간 1[s] | 손실 2.14\n",
            "| 에폭 546 |  반복 1 / 2 | 시간 1[s] | 손실 2.17\n",
            "| 에폭 547 |  반복 1 / 2 | 시간 1[s] | 손실 2.06\n",
            "| 에폭 548 |  반복 1 / 2 | 시간 1[s] | 손실 1.89\n",
            "| 에폭 549 |  반복 1 / 2 | 시간 1[s] | 손실 2.14\n",
            "| 에폭 550 |  반복 1 / 2 | 시간 1[s] | 손실 1.91\n",
            "| 에폭 551 |  반복 1 / 2 | 시간 1[s] | 손실 2.11\n",
            "| 에폭 552 |  반복 1 / 2 | 시간 1[s] | 손실 2.16\n",
            "| 에폭 553 |  반복 1 / 2 | 시간 1[s] | 손실 1.82\n",
            "| 에폭 554 |  반복 1 / 2 | 시간 1[s] | 손실 2.05\n",
            "| 에폭 555 |  반복 1 / 2 | 시간 1[s] | 손실 2.30\n",
            "| 에폭 556 |  반복 1 / 2 | 시간 1[s] | 손실 1.91\n",
            "| 에폭 557 |  반복 1 / 2 | 시간 1[s] | 손실 1.96\n",
            "| 에폭 558 |  반복 1 / 2 | 시간 1[s] | 손실 2.07\n",
            "| 에폭 559 |  반복 1 / 2 | 시간 1[s] | 손실 1.73\n",
            "| 에폭 560 |  반복 1 / 2 | 시간 1[s] | 손실 2.53\n",
            "| 에폭 561 |  반복 1 / 2 | 시간 1[s] | 손실 1.85\n",
            "| 에폭 562 |  반복 1 / 2 | 시간 1[s] | 손실 2.30\n",
            "| 에폭 563 |  반복 1 / 2 | 시간 1[s] | 손실 1.82\n",
            "| 에폭 564 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 565 |  반복 1 / 2 | 시간 1[s] | 손실 2.06\n",
            "| 에폭 566 |  반복 1 / 2 | 시간 1[s] | 손실 2.10\n",
            "| 에폭 567 |  반복 1 / 2 | 시간 1[s] | 손실 1.61\n",
            "| 에폭 568 |  반복 1 / 2 | 시간 1[s] | 손실 2.29\n",
            "| 에폭 569 |  반복 1 / 2 | 시간 1[s] | 손실 1.89\n",
            "| 에폭 570 |  반복 1 / 2 | 시간 1[s] | 손실 2.29\n",
            "| 에폭 571 |  반복 1 / 2 | 시간 1[s] | 손실 2.08\n",
            "| 에폭 572 |  반복 1 / 2 | 시간 1[s] | 손실 2.40\n",
            "| 에폭 573 |  반복 1 / 2 | 시간 1[s] | 손실 1.63\n",
            "| 에폭 574 |  반복 1 / 2 | 시간 1[s] | 손실 2.26\n",
            "| 에폭 575 |  반복 1 / 2 | 시간 1[s] | 손실 2.14\n",
            "| 에폭 576 |  반복 1 / 2 | 시간 1[s] | 손실 2.11\n",
            "| 에폭 577 |  반복 1 / 2 | 시간 1[s] | 손실 2.08\n",
            "| 에폭 578 |  반복 1 / 2 | 시간 1[s] | 손실 1.88\n",
            "| 에폭 579 |  반복 1 / 2 | 시간 1[s] | 손실 1.89\n",
            "| 에폭 580 |  반복 1 / 2 | 시간 1[s] | 손실 2.28\n",
            "| 에폭 581 |  반복 1 / 2 | 시간 1[s] | 손실 2.11\n",
            "| 에폭 582 |  반복 1 / 2 | 시간 1[s] | 손실 2.30\n",
            "| 에폭 583 |  반복 1 / 2 | 시간 1[s] | 손실 1.83\n",
            "| 에폭 584 |  반복 1 / 2 | 시간 1[s] | 손실 2.10\n",
            "| 에폭 585 |  반복 1 / 2 | 시간 1[s] | 손실 1.85\n",
            "| 에폭 586 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 587 |  반복 1 / 2 | 시간 1[s] | 손실 1.83\n",
            "| 에폭 588 |  반복 1 / 2 | 시간 1[s] | 손실 1.85\n",
            "| 에폭 589 |  반복 1 / 2 | 시간 1[s] | 손실 2.07\n",
            "| 에폭 590 |  반복 1 / 2 | 시간 1[s] | 손실 2.35\n",
            "| 에폭 591 |  반복 1 / 2 | 시간 1[s] | 손실 1.64\n",
            "| 에폭 592 |  반복 1 / 2 | 시간 1[s] | 손실 2.27\n",
            "| 에폭 593 |  반복 1 / 2 | 시간 1[s] | 손실 1.67\n",
            "| 에폭 594 |  반복 1 / 2 | 시간 1[s] | 손실 2.28\n",
            "| 에폭 595 |  반복 1 / 2 | 시간 1[s] | 손실 2.14\n",
            "| 에폭 596 |  반복 1 / 2 | 시간 1[s] | 손실 1.99\n",
            "| 에폭 597 |  반복 1 / 2 | 시간 1[s] | 손실 2.17\n",
            "| 에폭 598 |  반복 1 / 2 | 시간 1[s] | 손실 1.81\n",
            "| 에폭 599 |  반복 1 / 2 | 시간 1[s] | 손실 1.63\n",
            "| 에폭 600 |  반복 1 / 2 | 시간 1[s] | 손실 2.27\n",
            "| 에폭 601 |  반복 1 / 2 | 시간 1[s] | 손실 2.04\n",
            "| 에폭 602 |  반복 1 / 2 | 시간 1[s] | 손실 1.88\n",
            "| 에폭 603 |  반복 1 / 2 | 시간 1[s] | 손실 1.83\n",
            "| 에폭 604 |  반복 1 / 2 | 시간 1[s] | 손실 2.47\n",
            "| 에폭 605 |  반복 1 / 2 | 시간 1[s] | 손실 1.83\n",
            "| 에폭 606 |  반복 1 / 2 | 시간 1[s] | 손실 2.10\n",
            "| 에폭 607 |  반복 1 / 2 | 시간 1[s] | 손실 1.84\n",
            "| 에폭 608 |  반복 1 / 2 | 시간 1[s] | 손실 2.11\n",
            "| 에폭 609 |  반복 1 / 2 | 시간 1[s] | 손실 2.01\n",
            "| 에폭 610 |  반복 1 / 2 | 시간 1[s] | 손실 1.88\n",
            "| 에폭 611 |  반복 1 / 2 | 시간 1[s] | 손실 2.08\n",
            "| 에폭 612 |  반복 1 / 2 | 시간 1[s] | 손실 1.83\n",
            "| 에폭 613 |  반복 1 / 2 | 시간 1[s] | 손실 2.03\n",
            "| 에폭 614 |  반복 1 / 2 | 시간 1[s] | 손실 2.10\n",
            "| 에폭 615 |  반복 1 / 2 | 시간 1[s] | 손실 2.03\n",
            "| 에폭 616 |  반복 1 / 2 | 시간 1[s] | 손실 2.29\n",
            "| 에폭 617 |  반복 1 / 2 | 시간 1[s] | 손실 1.89\n",
            "| 에폭 618 |  반복 1 / 2 | 시간 1[s] | 손실 2.05\n",
            "| 에폭 619 |  반복 1 / 2 | 시간 1[s] | 손실 1.99\n",
            "| 에폭 620 |  반복 1 / 2 | 시간 1[s] | 손실 1.82\n",
            "| 에폭 621 |  반복 1 / 2 | 시간 1[s] | 손실 2.08\n",
            "| 에폭 622 |  반복 1 / 2 | 시간 1[s] | 손실 2.04\n",
            "| 에폭 623 |  반복 1 / 2 | 시간 1[s] | 손실 1.79\n",
            "| 에폭 624 |  반복 1 / 2 | 시간 1[s] | 손실 2.07\n",
            "| 에폭 625 |  반복 1 / 2 | 시간 1[s] | 손실 2.06\n",
            "| 에폭 626 |  반복 1 / 2 | 시간 1[s] | 손실 1.79\n",
            "| 에폭 627 |  반복 1 / 2 | 시간 1[s] | 손실 1.83\n",
            "| 에폭 628 |  반복 1 / 2 | 시간 1[s] | 손실 1.82\n",
            "| 에폭 629 |  반복 1 / 2 | 시간 1[s] | 손실 2.06\n",
            "| 에폭 630 |  반복 1 / 2 | 시간 1[s] | 손실 2.02\n",
            "| 에폭 631 |  반복 1 / 2 | 시간 1[s] | 손실 2.06\n",
            "| 에폭 632 |  반복 1 / 2 | 시간 1[s] | 손실 2.02\n",
            "| 에폭 633 |  반복 1 / 2 | 시간 1[s] | 손실 2.06\n",
            "| 에폭 634 |  반복 1 / 2 | 시간 1[s] | 손실 1.54\n",
            "| 에폭 635 |  반복 1 / 2 | 시간 1[s] | 손실 2.08\n",
            "| 에폭 636 |  반복 1 / 2 | 시간 1[s] | 손실 2.04\n",
            "| 에폭 637 |  반복 1 / 2 | 시간 1[s] | 손실 2.03\n",
            "| 에폭 638 |  반복 1 / 2 | 시간 1[s] | 손실 2.02\n",
            "| 에폭 639 |  반복 1 / 2 | 시간 1[s] | 손실 2.06\n",
            "| 에폭 640 |  반복 1 / 2 | 시간 1[s] | 손실 1.62\n",
            "| 에폭 641 |  반복 1 / 2 | 시간 1[s] | 손실 2.21\n",
            "| 에폭 642 |  반복 1 / 2 | 시간 1[s] | 손실 1.84\n",
            "| 에폭 643 |  반복 1 / 2 | 시간 1[s] | 손실 1.87\n",
            "| 에폭 644 |  반복 1 / 2 | 시간 1[s] | 손실 1.99\n",
            "| 에폭 645 |  반복 1 / 2 | 시간 1[s] | 손실 2.29\n",
            "| 에폭 646 |  반복 1 / 2 | 시간 1[s] | 손실 1.59\n",
            "| 에폭 647 |  반복 1 / 2 | 시간 1[s] | 손실 2.01\n",
            "| 에폭 648 |  반복 1 / 2 | 시간 1[s] | 손실 2.23\n",
            "| 에폭 649 |  반복 1 / 2 | 시간 1[s] | 손실 2.04\n",
            "| 에폭 650 |  반복 1 / 2 | 시간 1[s] | 손실 1.61\n",
            "| 에폭 651 |  반복 1 / 2 | 시간 1[s] | 손실 2.01\n",
            "| 에폭 652 |  반복 1 / 2 | 시간 1[s] | 손실 2.02\n",
            "| 에폭 653 |  반복 1 / 2 | 시간 1[s] | 손실 2.07\n",
            "| 에폭 654 |  반복 1 / 2 | 시간 1[s] | 손실 1.59\n",
            "| 에폭 655 |  반복 1 / 2 | 시간 1[s] | 손실 1.99\n",
            "| 에폭 656 |  반복 1 / 2 | 시간 1[s] | 손실 1.84\n",
            "| 에폭 657 |  반복 1 / 2 | 시간 1[s] | 손실 2.04\n",
            "| 에폭 658 |  반복 1 / 2 | 시간 1[s] | 손실 2.01\n",
            "| 에폭 659 |  반복 1 / 2 | 시간 1[s] | 손실 2.04\n",
            "| 에폭 660 |  반복 1 / 2 | 시간 1[s] | 손실 2.01\n",
            "| 에폭 661 |  반복 1 / 2 | 시간 1[s] | 손실 1.98\n",
            "| 에폭 662 |  반복 1 / 2 | 시간 1[s] | 손실 1.86\n",
            "| 에폭 663 |  반복 1 / 2 | 시간 1[s] | 손실 2.20\n",
            "| 에폭 664 |  반복 1 / 2 | 시간 1[s] | 손실 2.03\n",
            "| 에폭 665 |  반복 1 / 2 | 시간 1[s] | 손실 1.61\n",
            "| 에폭 666 |  반복 1 / 2 | 시간 1[s] | 손실 1.74\n",
            "| 에폭 667 |  반복 1 / 2 | 시간 1[s] | 손실 2.04\n",
            "| 에폭 668 |  반복 1 / 2 | 시간 1[s] | 손실 1.80\n",
            "| 에폭 669 |  반복 1 / 2 | 시간 1[s] | 손실 2.26\n",
            "| 에폭 670 |  반복 1 / 2 | 시간 1[s] | 손실 1.99\n",
            "| 에폭 671 |  반복 1 / 2 | 시간 1[s] | 손실 1.81\n",
            "| 에폭 672 |  반복 1 / 2 | 시간 1[s] | 손실 2.08\n",
            "| 에폭 673 |  반복 1 / 2 | 시간 1[s] | 손실 1.74\n",
            "| 에폭 674 |  반복 1 / 2 | 시간 1[s] | 손실 1.84\n",
            "| 에폭 675 |  반복 1 / 2 | 시간 1[s] | 손실 1.99\n",
            "| 에폭 676 |  반복 1 / 2 | 시간 1[s] | 손실 1.78\n",
            "| 에폭 677 |  반복 1 / 2 | 시간 1[s] | 손실 2.03\n",
            "| 에폭 678 |  반복 1 / 2 | 시간 1[s] | 손실 2.03\n",
            "| 에폭 679 |  반복 1 / 2 | 시간 1[s] | 손실 1.81\n",
            "| 에폭 680 |  반복 1 / 2 | 시간 1[s] | 손실 1.79\n",
            "| 에폭 681 |  반복 1 / 2 | 시간 1[s] | 손실 2.00\n",
            "| 에폭 682 |  반복 1 / 2 | 시간 1[s] | 손실 1.98\n",
            "| 에폭 683 |  반복 1 / 2 | 시간 1[s] | 손실 2.05\n",
            "| 에폭 684 |  반복 1 / 2 | 시간 1[s] | 손실 1.75\n",
            "| 에폭 685 |  반복 1 / 2 | 시간 1[s] | 손실 1.77\n",
            "| 에폭 686 |  반복 1 / 2 | 시간 1[s] | 손실 2.47\n",
            "| 에폭 687 |  반복 1 / 2 | 시간 1[s] | 손실 2.04\n",
            "| 에폭 688 |  반복 1 / 2 | 시간 1[s] | 손실 1.79\n",
            "| 에폭 689 |  반복 1 / 2 | 시간 1[s] | 손실 2.00\n",
            "| 에폭 690 |  반복 1 / 2 | 시간 1[s] | 손실 2.24\n",
            "| 에폭 691 |  반복 1 / 2 | 시간 1[s] | 손실 1.75\n",
            "| 에폭 692 |  반복 1 / 2 | 시간 1[s] | 손실 1.83\n",
            "| 에폭 693 |  반복 1 / 2 | 시간 1[s] | 손실 2.43\n",
            "| 에폭 694 |  반복 1 / 2 | 시간 1[s] | 손실 1.80\n",
            "| 에폭 695 |  반복 1 / 2 | 시간 1[s] | 손실 1.76\n",
            "| 에폭 696 |  반복 1 / 2 | 시간 1[s] | 손실 2.03\n",
            "| 에폭 697 |  반복 1 / 2 | 시간 1[s] | 손실 1.79\n",
            "| 에폭 698 |  반복 1 / 2 | 시간 1[s] | 손실 2.22\n",
            "| 에폭 699 |  반복 1 / 2 | 시간 1[s] | 손실 1.79\n",
            "| 에폭 700 |  반복 1 / 2 | 시간 1[s] | 손실 1.98\n",
            "| 에폭 701 |  반복 1 / 2 | 시간 1[s] | 손실 1.58\n",
            "| 에폭 702 |  반복 1 / 2 | 시간 1[s] | 손실 2.19\n",
            "| 에폭 703 |  반복 1 / 2 | 시간 1[s] | 손실 1.58\n",
            "| 에폭 704 |  반복 1 / 2 | 시간 1[s] | 손실 1.98\n",
            "| 에폭 705 |  반복 1 / 2 | 시간 1[s] | 손실 2.01\n",
            "| 에폭 706 |  반복 1 / 2 | 시간 1[s] | 손실 2.22\n",
            "| 에폭 707 |  반복 1 / 2 | 시간 1[s] | 손실 1.79\n",
            "| 에폭 708 |  반복 1 / 2 | 시간 1[s] | 손실 2.23\n",
            "| 에폭 709 |  반복 1 / 2 | 시간 1[s] | 손실 1.98\n",
            "| 에폭 710 |  반복 1 / 2 | 시간 1[s] | 손실 1.76\n",
            "| 에폭 711 |  반복 1 / 2 | 시간 1[s] | 손실 2.23\n",
            "| 에폭 712 |  반복 1 / 2 | 시간 1[s] | 손실 1.76\n",
            "| 에폭 713 |  반복 1 / 2 | 시간 1[s] | 손실 2.02\n",
            "| 에폭 714 |  반복 1 / 2 | 시간 1[s] | 손실 1.76\n",
            "| 에폭 715 |  반복 1 / 2 | 시간 1[s] | 손실 2.19\n",
            "| 에폭 716 |  반복 1 / 2 | 시간 1[s] | 손실 2.04\n",
            "| 에폭 717 |  반복 1 / 2 | 시간 1[s] | 손실 1.53\n",
            "| 에폭 718 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 719 |  반복 1 / 2 | 시간 1[s] | 손실 1.77\n",
            "| 에폭 720 |  반복 1 / 2 | 시간 1[s] | 손실 2.02\n",
            "| 에폭 721 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 722 |  반복 1 / 2 | 시간 1[s] | 손실 1.54\n",
            "| 에폭 723 |  반복 1 / 2 | 시간 1[s] | 손실 2.21\n",
            "| 에폭 724 |  반복 1 / 2 | 시간 1[s] | 손실 1.99\n",
            "| 에폭 725 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 726 |  반복 1 / 2 | 시간 1[s] | 손실 1.78\n",
            "| 에폭 727 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 728 |  반복 1 / 2 | 시간 1[s] | 손실 2.21\n",
            "| 에폭 729 |  반복 1 / 2 | 시간 1[s] | 손실 1.71\n",
            "| 에폭 730 |  반복 1 / 2 | 시간 1[s] | 손실 1.78\n",
            "| 에폭 731 |  반복 1 / 2 | 시간 1[s] | 손실 1.98\n",
            "| 에폭 732 |  반복 1 / 2 | 시간 1[s] | 손실 1.50\n",
            "| 에폭 733 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 734 |  반복 1 / 2 | 시간 1[s] | 손실 2.20\n",
            "| 에폭 735 |  반복 1 / 2 | 시간 1[s] | 손실 2.02\n",
            "| 에폭 736 |  반복 1 / 2 | 시간 1[s] | 손실 2.21\n",
            "| 에폭 737 |  반복 1 / 2 | 시간 1[s] | 손실 1.55\n",
            "| 에폭 738 |  반복 1 / 2 | 시간 1[s] | 손실 2.22\n",
            "| 에폭 739 |  반복 1 / 2 | 시간 1[s] | 손실 1.72\n",
            "| 에폭 740 |  반복 1 / 2 | 시간 1[s] | 손실 1.76\n",
            "| 에폭 741 |  반복 1 / 2 | 시간 1[s] | 손실 2.02\n",
            "| 에폭 742 |  반복 1 / 2 | 시간 1[s] | 손실 1.98\n",
            "| 에폭 743 |  반복 1 / 2 | 시간 1[s] | 손실 1.99\n",
            "| 에폭 744 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 745 |  반복 1 / 2 | 시간 1[s] | 손실 1.77\n",
            "| 에폭 746 |  반복 1 / 2 | 시간 1[s] | 손실 2.23\n",
            "| 에폭 747 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 748 |  반복 1 / 2 | 시간 1[s] | 손실 1.50\n",
            "| 에폭 749 |  반복 1 / 2 | 시간 1[s] | 손실 2.19\n",
            "| 에폭 750 |  반복 1 / 2 | 시간 1[s] | 손실 2.19\n",
            "| 에폭 751 |  반복 1 / 2 | 시간 1[s] | 손실 1.76\n",
            "| 에폭 752 |  반복 1 / 2 | 시간 1[s] | 손실 1.77\n",
            "| 에폭 753 |  반복 1 / 2 | 시간 1[s] | 손실 2.42\n",
            "| 에폭 754 |  반복 1 / 2 | 시간 1[s] | 손실 1.54\n",
            "| 에폭 755 |  반복 1 / 2 | 시간 1[s] | 손실 2.19\n",
            "| 에폭 756 |  반복 1 / 2 | 시간 1[s] | 손실 1.74\n",
            "| 에폭 757 |  반복 1 / 2 | 시간 1[s] | 손실 2.18\n",
            "| 에폭 758 |  반복 1 / 2 | 시간 1[s] | 손실 1.75\n",
            "| 에폭 759 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 760 |  반복 1 / 2 | 시간 1[s] | 손실 1.74\n",
            "| 에폭 761 |  반복 1 / 2 | 시간 1[s] | 손실 2.01\n",
            "| 에폭 762 |  반복 1 / 2 | 시간 1[s] | 손실 1.74\n",
            "| 에폭 763 |  반복 1 / 2 | 시간 1[s] | 손실 1.96\n",
            "| 에폭 764 |  반복 1 / 2 | 시간 1[s] | 손실 2.20\n",
            "| 에폭 765 |  반복 1 / 2 | 시간 1[s] | 손실 1.55\n",
            "| 에폭 766 |  반복 1 / 2 | 시간 1[s] | 손실 1.96\n",
            "| 에폭 767 |  반복 1 / 2 | 시간 1[s] | 손실 1.98\n",
            "| 에폭 768 |  반복 1 / 2 | 시간 1[s] | 손실 1.96\n",
            "| 에폭 769 |  반복 1 / 2 | 시간 1[s] | 손실 1.53\n",
            "| 에폭 770 |  반복 1 / 2 | 시간 1[s] | 손실 1.98\n",
            "| 에폭 771 |  반복 1 / 2 | 시간 1[s] | 손실 1.96\n",
            "| 에폭 772 |  반복 1 / 2 | 시간 1[s] | 손실 1.98\n",
            "| 에폭 773 |  반복 1 / 2 | 시간 1[s] | 손실 1.96\n",
            "| 에폭 774 |  반복 1 / 2 | 시간 1[s] | 손실 1.74\n",
            "| 에폭 775 |  반복 1 / 2 | 시간 1[s] | 손실 2.18\n",
            "| 에폭 776 |  반복 1 / 2 | 시간 1[s] | 손실 1.53\n",
            "| 에폭 777 |  반복 1 / 2 | 시간 1[s] | 손실 2.20\n",
            "| 에폭 778 |  반복 1 / 2 | 시간 1[s] | 손실 2.19\n",
            "| 에폭 779 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 780 |  반복 1 / 2 | 시간 1[s] | 손실 1.75\n",
            "| 에폭 781 |  반복 1 / 2 | 시간 1[s] | 손실 2.19\n",
            "| 에폭 782 |  반복 1 / 2 | 시간 1[s] | 손실 1.74\n",
            "| 에폭 783 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 784 |  반복 1 / 2 | 시간 1[s] | 손실 1.74\n",
            "| 에폭 785 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 786 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 787 |  반복 1 / 2 | 시간 1[s] | 손실 1.73\n",
            "| 에폭 788 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 789 |  반복 1 / 2 | 시간 1[s] | 손실 1.96\n",
            "| 에폭 790 |  반복 1 / 2 | 시간 1[s] | 손실 1.73\n",
            "| 에폭 791 |  반복 1 / 2 | 시간 1[s] | 손실 1.72\n",
            "| 에폭 792 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 793 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 794 |  반복 1 / 2 | 시간 1[s] | 손실 1.74\n",
            "| 에폭 795 |  반복 1 / 2 | 시간 1[s] | 손실 2.41\n",
            "| 에폭 796 |  반복 1 / 2 | 시간 1[s] | 손실 1.51\n",
            "| 에폭 797 |  반복 1 / 2 | 시간 1[s] | 손실 1.96\n",
            "| 에폭 798 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 799 |  반복 1 / 2 | 시간 1[s] | 손실 1.74\n",
            "| 에폭 800 |  반복 1 / 2 | 시간 1[s] | 손실 1.96\n",
            "| 에폭 801 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 802 |  반복 1 / 2 | 시간 1[s] | 손실 2.18\n",
            "| 에폭 803 |  반복 1 / 2 | 시간 1[s] | 손실 1.49\n",
            "| 에폭 804 |  반복 1 / 2 | 시간 1[s] | 손실 2.22\n",
            "| 에폭 805 |  반복 1 / 2 | 시간 1[s] | 손실 1.72\n",
            "| 에폭 806 |  반복 1 / 2 | 시간 1[s] | 손실 2.18\n",
            "| 에폭 807 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 808 |  반복 1 / 2 | 시간 1[s] | 손실 1.73\n",
            "| 에폭 809 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 810 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 811 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 812 |  반복 1 / 2 | 시간 1[s] | 손실 1.97\n",
            "| 에폭 813 |  반복 1 / 2 | 시간 1[s] | 손실 1.96\n",
            "| 에폭 814 |  반복 1 / 2 | 시간 1[s] | 손실 1.72\n",
            "| 에폭 815 |  반복 1 / 2 | 시간 1[s] | 손실 1.73\n",
            "| 에폭 816 |  반복 1 / 2 | 시간 1[s] | 손실 2.18\n",
            "| 에폭 817 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 818 |  반복 1 / 2 | 시간 1[s] | 손실 1.51\n",
            "| 에폭 819 |  반복 1 / 2 | 시간 1[s] | 손실 2.41\n",
            "| 에폭 820 |  반복 1 / 2 | 시간 1[s] | 손실 1.49\n",
            "| 에폭 821 |  반복 1 / 2 | 시간 1[s] | 손실 2.19\n",
            "| 에폭 822 |  반복 1 / 2 | 시간 1[s] | 손실 1.73\n",
            "| 에폭 823 |  반복 1 / 2 | 시간 1[s] | 손실 1.73\n",
            "| 에폭 824 |  반복 1 / 2 | 시간 1[s] | 손실 1.72\n",
            "| 에폭 825 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 826 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 827 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 828 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 829 |  반복 1 / 2 | 시간 1[s] | 손실 1.72\n",
            "| 에폭 830 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 831 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 832 |  반복 1 / 2 | 시간 1[s] | 손실 1.73\n",
            "| 에폭 833 |  반복 1 / 2 | 시간 1[s] | 손실 2.17\n",
            "| 에폭 834 |  반복 1 / 2 | 시간 1[s] | 손실 1.71\n",
            "| 에폭 835 |  반복 1 / 2 | 시간 1[s] | 손실 1.71\n",
            "| 에폭 836 |  반복 1 / 2 | 시간 1[s] | 손실 2.19\n",
            "| 에폭 837 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 838 |  반복 1 / 2 | 시간 1[s] | 손실 1.48\n",
            "| 에폭 839 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 840 |  반복 1 / 2 | 시간 1[s] | 손실 1.48\n",
            "| 에폭 841 |  반복 1 / 2 | 시간 1[s] | 손실 2.40\n",
            "| 에폭 842 |  반복 1 / 2 | 시간 1[s] | 손실 1.52\n",
            "| 에폭 843 |  반복 1 / 2 | 시간 1[s] | 손실 1.72\n",
            "| 에폭 844 |  반복 1 / 2 | 시간 1[s] | 손실 2.17\n",
            "| 에폭 845 |  반복 1 / 2 | 시간 1[s] | 손실 1.51\n",
            "| 에폭 846 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 847 |  반복 1 / 2 | 시간 1[s] | 손실 1.96\n",
            "| 에폭 848 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 849 |  반복 1 / 2 | 시간 1[s] | 손실 1.49\n",
            "| 에폭 850 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 851 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 852 |  반복 1 / 2 | 시간 1[s] | 손실 1.70\n",
            "| 에폭 853 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 854 |  반복 1 / 2 | 시간 1[s] | 손실 1.71\n",
            "| 에폭 855 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 856 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 857 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 858 |  반복 1 / 2 | 시간 1[s] | 손실 1.70\n",
            "| 에폭 859 |  반복 1 / 2 | 시간 1[s] | 손실 1.72\n",
            "| 에폭 860 |  반복 1 / 2 | 시간 1[s] | 손실 2.18\n",
            "| 에폭 861 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 862 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 863 |  반복 1 / 2 | 시간 1[s] | 손실 1.71\n",
            "| 에폭 864 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 865 |  반복 1 / 2 | 시간 1[s] | 손실 1.71\n",
            "| 에폭 866 |  반복 1 / 2 | 시간 1[s] | 손실 1.72\n",
            "| 에폭 867 |  반복 1 / 2 | 시간 1[s] | 손실 1.96\n",
            "| 에폭 868 |  반복 1 / 2 | 시간 1[s] | 손실 1.46\n",
            "| 에폭 869 |  반복 1 / 2 | 시간 1[s] | 손실 1.73\n",
            "| 에폭 870 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 871 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 872 |  반복 1 / 2 | 시간 1[s] | 손실 2.39\n",
            "| 에폭 873 |  반복 1 / 2 | 시간 1[s] | 손실 1.49\n",
            "| 에폭 874 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 875 |  반복 1 / 2 | 시간 1[s] | 손실 1.70\n",
            "| 에폭 876 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 877 |  반복 1 / 2 | 시간 1[s] | 손실 1.72\n",
            "| 에폭 878 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 879 |  반복 1 / 2 | 시간 1[s] | 손실 1.71\n",
            "| 에폭 880 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 881 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 882 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 883 |  반복 1 / 2 | 시간 1[s] | 손실 1.47\n",
            "| 에폭 884 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 885 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 886 |  반복 1 / 2 | 시간 1[s] | 손실 1.70\n",
            "| 에폭 887 |  반복 1 / 2 | 시간 1[s] | 손실 2.16\n",
            "| 에폭 888 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 889 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 890 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 891 |  반복 1 / 2 | 시간 1[s] | 손실 1.46\n",
            "| 에폭 892 |  반복 1 / 2 | 시간 1[s] | 손실 2.18\n",
            "| 에폭 893 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 894 |  반복 1 / 2 | 시간 1[s] | 손실 1.68\n",
            "| 에폭 895 |  반복 1 / 2 | 시간 1[s] | 손실 1.71\n",
            "| 에폭 896 |  반복 1 / 2 | 시간 1[s] | 손실 1.71\n",
            "| 에폭 897 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 898 |  반복 1 / 2 | 시간 1[s] | 손실 1.91\n",
            "| 에폭 899 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 900 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 901 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 902 |  반복 1 / 2 | 시간 1[s] | 손실 2.16\n",
            "| 에폭 903 |  반복 1 / 2 | 시간 1[s] | 손실 1.91\n",
            "| 에폭 904 |  반복 1 / 2 | 시간 1[s] | 손실 1.70\n",
            "| 에폭 905 |  반복 1 / 2 | 시간 1[s] | 손실 1.73\n",
            "| 에폭 906 |  반복 1 / 2 | 시간 1[s] | 손실 1.91\n",
            "| 에폭 907 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 908 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 909 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 910 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 911 |  반복 1 / 2 | 시간 1[s] | 손실 1.71\n",
            "| 에폭 912 |  반복 1 / 2 | 시간 1[s] | 손실 1.68\n",
            "| 에폭 913 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 914 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 915 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 916 |  반복 1 / 2 | 시간 1[s] | 손실 1.70\n",
            "| 에폭 917 |  반복 1 / 2 | 시간 1[s] | 손실 1.46\n",
            "| 에폭 918 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 919 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 920 |  반복 1 / 2 | 시간 1[s] | 손실 1.95\n",
            "| 에폭 921 |  반복 1 / 2 | 시간 1[s] | 손실 1.46\n",
            "| 에폭 922 |  반복 1 / 2 | 시간 1[s] | 손실 1.91\n",
            "| 에폭 923 |  반복 1 / 2 | 시간 1[s] | 손실 2.16\n",
            "| 에폭 924 |  반복 1 / 2 | 시간 1[s] | 손실 1.47\n",
            "| 에폭 925 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 926 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 927 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 928 |  반복 1 / 2 | 시간 1[s] | 손실 1.70\n",
            "| 에폭 929 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 930 |  반복 1 / 2 | 시간 1[s] | 손실 1.46\n",
            "| 에폭 931 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 932 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 933 |  반복 1 / 2 | 시간 1[s] | 손실 1.94\n",
            "| 에폭 934 |  반복 1 / 2 | 시간 1[s] | 손실 1.91\n",
            "| 에폭 935 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 936 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 937 |  반복 1 / 2 | 시간 1[s] | 손실 1.70\n",
            "| 에폭 938 |  반복 1 / 2 | 시간 1[s] | 손실 1.91\n",
            "| 에폭 939 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 940 |  반복 1 / 2 | 시간 1[s] | 손실 1.47\n",
            "| 에폭 941 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 942 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 943 |  반복 1 / 2 | 시간 1[s] | 손실 1.68\n",
            "| 에폭 944 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 945 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 946 |  반복 1 / 2 | 시간 1[s] | 손실 1.90\n",
            "| 에폭 947 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 948 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 949 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 950 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 951 |  반복 1 / 2 | 시간 1[s] | 손실 1.91\n",
            "| 에폭 952 |  반복 1 / 2 | 시간 1[s] | 손실 1.70\n",
            "| 에폭 953 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 954 |  반복 1 / 2 | 시간 1[s] | 손실 1.68\n",
            "| 에폭 955 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 956 |  반복 1 / 2 | 시간 1[s] | 손실 1.91\n",
            "| 에폭 957 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 958 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 959 |  반복 1 / 2 | 시간 1[s] | 손실 1.68\n",
            "| 에폭 960 |  반복 1 / 2 | 시간 1[s] | 손실 1.71\n",
            "| 에폭 961 |  반복 1 / 2 | 시간 1[s] | 손실 1.67\n",
            "| 에폭 962 |  반복 1 / 2 | 시간 1[s] | 손실 2.14\n",
            "| 에폭 963 |  반복 1 / 2 | 시간 1[s] | 손실 2.16\n",
            "| 에폭 964 |  반복 1 / 2 | 시간 1[s] | 손실 1.46\n",
            "| 에폭 965 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 966 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 967 |  반복 1 / 2 | 시간 1[s] | 손실 2.14\n",
            "| 에폭 968 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 969 |  반복 1 / 2 | 시간 1[s] | 손실 1.93\n",
            "| 에폭 970 |  반복 1 / 2 | 시간 1[s] | 손실 2.14\n",
            "| 에폭 971 |  반복 1 / 2 | 시간 1[s] | 손실 1.46\n",
            "| 에폭 972 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 973 |  반복 1 / 2 | 시간 1[s] | 손실 1.68\n",
            "| 에폭 974 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 975 |  반복 1 / 2 | 시간 1[s] | 손실 1.91\n",
            "| 에폭 976 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 977 |  반복 1 / 2 | 시간 1[s] | 손실 1.91\n",
            "| 에폭 978 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 979 |  반복 1 / 2 | 시간 1[s] | 손실 2.15\n",
            "| 에폭 980 |  반복 1 / 2 | 시간 1[s] | 손실 1.68\n",
            "| 에폭 981 |  반복 1 / 2 | 시간 1[s] | 손실 2.14\n",
            "| 에폭 982 |  반복 1 / 2 | 시간 1[s] | 손실 1.67\n",
            "| 에폭 983 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 984 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 985 |  반복 1 / 2 | 시간 1[s] | 손실 1.92\n",
            "| 에폭 986 |  반복 1 / 2 | 시간 1[s] | 손실 1.68\n",
            "| 에폭 987 |  반복 1 / 2 | 시간 1[s] | 손실 2.13\n",
            "| 에폭 988 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 989 |  반복 1 / 2 | 시간 1[s] | 손실 1.69\n",
            "| 에폭 990 |  반복 1 / 2 | 시간 1[s] | 손실 1.68\n",
            "| 에폭 991 |  반복 1 / 2 | 시간 1[s] | 손실 2.13\n",
            "| 에폭 992 |  반복 1 / 2 | 시간 2[s] | 손실 1.69\n",
            "| 에폭 993 |  반복 1 / 2 | 시간 2[s] | 손실 1.92\n",
            "| 에폭 994 |  반복 1 / 2 | 시간 2[s] | 손실 1.91\n",
            "| 에폭 995 |  반복 1 / 2 | 시간 2[s] | 손실 1.68\n",
            "| 에폭 996 |  반복 1 / 2 | 시간 2[s] | 손실 1.91\n",
            "| 에폭 997 |  반복 1 / 2 | 시간 2[s] | 손실 1.68\n",
            "| 에폭 998 |  반복 1 / 2 | 시간 2[s] | 손실 2.14\n",
            "| 에폭 999 |  반복 1 / 2 | 시간 2[s] | 손실 1.92\n",
            "| 에폭 1000 |  반복 1 / 2 | 시간 2[s] | 손실 2.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGwCAYAAABRgJRuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABpRUlEQVR4nO3dd3gU1foH8O+W9AZJSEIIoUiRXqWFJr3qxY5iwXIVvVf0igVFxQaCqBd7u1fwh6hcxV6QFkVaaKEXQUqAJBDSSdlsdn5/hGxmdmd3Z3dna76f58lDdubMzMlk2XlzznvO0QiCIICIiIgowGl9XQEiIiIiNTCoISIioqDAoIaIiIiCAoMaIiIiCgoMaoiIiCgoMKghIiKioMCghoiIiIKC3tcV8CaTyYSzZ88iJiYGGo3G19UhIiIiBQRBQFlZGVJTU6HV2m6PaVRBzdmzZ9GyZUtfV4OIiIhckJOTg7S0NJv7G1VQExMTA6DupsTGxvq4NkRERKREaWkpWrZsaX6O29Kogpr6LqfY2FgGNURERAHGUeoIE4WJiIgoKDCoISIioqDAoIaIiIiCAoMaIiIiCgoMaoiIiCgoMKghIiKioMCghoiIiIICgxoiIiIKCgxqiIiIKCgwqCEiIqKgwKCGiIiIggKDGiIiIgoKjWpBS0/JKaxAiE6L2Ag9IkJ0DhfcIiIiIvUxqFHBPZ9sx6G8MgBAiE6D2PAQxEaEQKsBosL0iI8KRdfUOLRtFoVmMWEY0r6Zj2tMREQUfBjUqECj0UCn1aDWJKCmVsCFiwZcuGiQlMk8fF7yenC7RPxjRDsMaJvgzaoSEREFLY0gCIKvK+EtpaWliIuLQ0lJCWJjY1U9tyAIqDDUorSqBiWVNSitNKK8ugZniipRUlmDbSeK8NuR81bHdWoei7sHt0FGu0SkxIWrWiciIqJgoPT5zaDGyw6cLcWB3FJ88PsxHMkvN2+PCtVh8U29kNEuERGhOp/UjYiIyB8xqJHhD0GN2NFz5Zj/00GsPXROsv2DW/tgdOdkJhwTERGBQY0sfwtq6u08VYRr3tlktf39W/tgbJcUH9SIiIjIfyh9fnOeGj/QO70pXr+xBx4d21Gy/dOtp3xUIyIiosDDoMZPTOmVhgeubIcxnZPN234/ch4ZL6/D+sPn7BxJREREAIMav/PBbX1x4PmxaBkfAQA4U1yJ6R9vw7pD+T6uGRERkX9jUOOHIkP1+PWhYRjSPtG87c4l23HsfLmdo4iIiBo3BjV+KiJUh//ecQVGi7qj5v90COXVRh/WioiIyH8xqPFjITotFl7b3fx6zcF8DH8lk4ENERGRDAY1fq5pVCh2PT0aCVGhAICC8mp0fXYVSipqfFwzIiIi/8KgJgA0jQrFqoeHom+rpuZt//hspw9rRERE5H8Y1ASIxOgwfHpPf/PrDX8W4MpFmXj4i2zkllT6sGZERET+gUFNAAnT63BL/3Tz6+MFF/H1rjN44qu9PqwVERGRf2BQE2Cev7orbuzbUrLttyPncSS/zEc1IiIi8g9+E9QsWLAAGo0GmZmZsvtNJhPmzJmDtLQ0JCUlYfz48Thx4oRX6+gPdFoN5l3TDQ+Nai/ZPub137HjZKGPakVEROR7fhHU7N+/H8uWLUOLFi1sllmwYAFWrlyJ7du3Izc3F507d8bEiRNhNDa+4c06rQa3D2xttf2bXWe9XxkiIiI/4fOgxmg04o477sCbb74JvV4vW0YQBCxevBhz5sxBSkoKdDodXnzxRZw6dQqrV6/2co39Q9OoULx+Yw/JtqzjhZiweAO2nWCLDRERNT4+D2peeukl9OvXD8OHD7dZ5vjx48jPz0dGRoZ5W0REBHr37o0tW7bYPK66uhqlpaWSr2AypVcaru2dZn59OL8MB3JLcffS7T6sFRERkW/4NKjZuXMnli1bhgULFtgtl59ft5hjcnKyZHtycrJ5n5z58+cjLi7O/NWyZUubZQNVk8gQq20llTU4eo7rRBERUePis6DGYDDgjjvuwHvvvYfo6Gi7ZU0mEwBAo9FItmu1WvM+ObNnz0ZJSYn5Kycnx/2K+5mOyTGy28cv/t3LNSEiIvItnwU1zz//PAYOHIiRI0c6LJuQkAAAKCyU5ooUFhYiMTFR7hAAQFhYGGJjYyVfweaa3i1w9+A2VttragXU1NoO+IiIiIKNz4KarKwsfPbZZ2jSpIn569SpU5g0aRKGDBkiKduuXTvExcVhx44d5m1GoxG7du1C7969vV11v6LXaTFnUmfsenq0ZBkFAMgrqfJRrYiIiLzPZ0HNr7/+itLSUhQXF5u/0tPT8cMPP2DNmjUYM2YMXn/9dQCAXq/Hvffei6eeegq5ubmoqanBnDlzEBUVhYkTJ/rqR/ArdaOhekq2nS3m8glERNR4+Hz0kxyj0YjDhw/j1KlT5m0vvvgihg8fjh49eiApKQnbtm3DqlWrEBER4cOa+pe0phGS2Yb/KrgIQRB8WCMiIiLv0QiN6KlXWlqKuLg4lJSUBGV+Tb1Fqw7jrfVHAQDp8ZFY9dBQRITqfFwrIiIi1yh9fvtlSw2554o28ebvTxVW4Ic9Z7F00wkmDhMRUVCTn8KXAlofi4ThR7/cAwAorqjBTIs1o4iIiIIFW2qCUHSYHld2bGa1ffXBPB/UhoiIyDsY1ASpt2/pDa10rkJoLSYvJCIiCiYMaoJUZKgeTSJDJdtyCit8VBsiIiLPY1ATxKpraiWviypqUGmotVGaiIgosDGoCWKPjbscAHDbwFaIDqvLCX9r/Z+YvXIvXvrxAM6XVfuyekRERKri6KcgdtvAVhh4WQLaJkZh87EL+PNcOd5ef8y8/+i5cnw8vZ8Pa0hERKQettQEMY1Ggw7JMdDrtOie1sRq/46TRd6vFBERkYcwqGkkxndNsdpWWmXE+78dg8nUaCaVJiKiIMagppFIi5dfI2v+z4fwxfYcL9eGiIhIfQxqGonmsbYX/tx+gt1QREQU+BjUNBKxEbZzwkurarxYEyIiIs9gUNNIaDQaNIkMkd1XWsmghoiIAh+DmkZk7b+GYfFNPa22lzCoISKiIMCgphFJiA5Dz5ZNrLaf4vIJREQUBBjUNDJpTSOttlUYanGhvBrzfzqI73ef9UGtiIiI3MegppHRaTV4/9Y+VttfWXUY7//+F/752S4f1IqIiMh9DGoaoZTYcKttn29rmKumqoaLXhIRUeBhUNMI9WjZBFGhOpv7zxZXerE2RERE6mBQ00jdkdHa5r7ckirvVYSIiEglDGoaqVbxUebvr+nVQrJvzcF8jHg1E+sO5Xu7WkRERC6zPc0sBbVrerfA8QsXMaBtAs4UVWLlrjPmfR9vPAEAuHPJdpx4eaKPakhEROQcBjWNlF6nxePjLgcAVBtr8euBPGQePu/jWhEREbmO3U+EML0OS6b3w01XtPR1VYiIiFzGoIbMwkNsj4giIiLydwxqyCzSzjBvIiIif8eghsz0Wo2vq0BEROQyBjVkZjQJVtvyOGcNEREFCAY1ZCYX1AyYvxaZh8/5oDZERETOYVBDZjW1Jtntd3y8DTtOFnq5NkRERM5hUENmITrbb4dr393sxZoQERE5j0ENmd0zpC1aJ0TinyPa+boqRERETuOMwmTWLCYMmY9eCQA4X1aNz7fl+LhGREREyrGlhmRN7Zdute3dzGP4eONxH9SGiIjIMbbUkKzuaXFomxiFvwoumrct+OUQAGDagFZ282+IiIh8gU8mkqXRaDDvmm6y+6pqar1cGyIiIscY1JBNfVo1RefmsVbbL1YzqCEiIv/DoIZsCtFp8cM/B1ttH/vv331QGyIiIvsY1JBdWpn1oEoqa7BiWw5OF1X4oEZERETyGNSQQ4MuS7Da9thXezDi1d98UBsiIiJ5DGrIof/cfgV+fXio1XaDUX5ZBSIiIl9gUEMORYTq0CE5RnZfWVWNl2tDREQkj0ENueV0UaWvq0BERASAQQ25qcLA4d1EROQfGNSQWyoZ1BARkZ9gUENuqTAYfV0FIiIiAAxqyE2VXDKBiIj8BIMaUuztm3tbbTuQWwpBEHxQGyIiIikGNaTYxO7NsfGJEZJt7//2F5ZuOoEDZ0uxcudpBjhEROQzel9XgAJLk4gQq21zvz9g/v73I+fx75t6ebNKREREAHzcUrNixQpkZGQgOTkZLVq0wIQJE7B7927ZsnPnzkV0dDRSUlIkX9XV1V6udeMWGaqzu/+b7LPYd6bES7UhIiJq4NOg5scff8SiRYuQl5eHkydPolevXhg3bhxMJvnp92fNmoW8vDzJV1hYmJdr3bhpNNYLXFrKKeRCl0RE5H0+DWqWLl2KgQMHQqPRQK/X44YbbkBeXh4KCgp8WS1yEyfkIyIiX/CbROHc3FwsWLAAY8aMQVJSkirnrK6uRmlpqeSL3PfWzb3Qs2UTm/s5dw0REfmCXwQ1Q4YMQWpqKk6ePIlPP/3UZrnFixejefPmSE9Px8SJE5GZmWn3vPPnz0dcXJz5q2XLlirXvHGa1D0V3zyQgR/+Odi8LTm2oRuwtIpBDREReZ9fBDUbNmxAfn4+OnXqhKFDh6KqqsqqzMyZM5GXl4fc3FxkZ2dj2LBhGDduHDZu3GjzvLNnz0ZJSYn5Kycnx5M/RqMTqm94+zSLaQhqCi8aAAAXyqtxKI+tY0RE5B0awY8mFjEYDIiJicHKlSsxceJEh+VHjBiBTp064e2331Z0/tLSUsTFxaGkpASxsbHuVrfREwQBj365B8mxYfjj6AXszikGAPRs2QQTuqVg3k+HAABr/jUU7ZJifFhTIiIKZEqf3z6bp6a2thY6nXR4sFarhU6ns9puS1VVFeLj4z1RPVJAo9Fg0fU9AAB//PmHeXt2TjGyLwU4ALD5r0IGNURE5HE+637au3cvrrrqKuzduxdAXSvNI488guTkZAwZMgRTp07FrFmzzOWffPJJnDx5EkBdAvDLL7+Mw4cP47777vNJ/Um52lr5IfpERERq8llLTbdu3TB69GjccccdOH36NPR6Pa644gqsXr0aUVFROHLkiGRivbi4OIwfPx4FBQUwGo0YPHgwNmzYgBYtWvjqRyCFqo0mHM4rQ4fkaEXz3BAREbnCr3JqPI05NZ5z1Vt/YM9p+zMJPz2pM+4a3MZLNSIiomCh9PntF6OfKPApCY3fWven5ytCRESNFoMaUsXcqzo7LGM0NZpGQSIi8gEGNaSKPq3iceiFcXh0bEebZcqqjMh4eR2WbjrhvYoREVGjwaCGVBMeosPk7ql2y5wprsSz3+33Uo2IiKgxYVBDqkpPiMQ1vTkijYiIvI9BDamua2qcwzIlFTVeqAkRETUmDGpIdeI1oWzJL7Ne34uIiMgdDGpIdUqCmuoaE04UXMT/tueglqOiiIhIBT6bUZiCV5iSoMZYi8mL6taLMpoETO2X7ulqERFRkGNLDalOr3X8tjIYG9aD2nmyyJPVISKiRoJBDanuosHosMyXO0+bvw8L4duQiIjcx6cJqa68ynFQs3LnGfP34XodAODNtX/i2+wztg4hIiKyizk1pLruaY6HdIuFhWix81QRXl19BABwdU/Oc0NERM5jSw2prm/reLw5tRfio0IVlQ/T61BQVu3hWhERUbBjUEMeMblHKlY/PFRR2XDm1BARkQr4NCGPiQyV9m7++vBQDGgbb1UuVKeFeKYazltDRESuYFBDHmPZAtMkIgQ1tdYBi0kABNHmmlqTVRkiIiJHGNSQx2g0GvRp1dT8OkSnRVVNrVW5upaZhqhm0pt/oLjC4I0qEhFREGFQQx41e/zl5u9D9PJBjdEkSFpqjp4rx7u/HfNG9YiIKIgwqCGPCrs0Bw0A6LUaVNVYdy2ZBAFGizyai9WO57ohIiIS4zw15FEJ0Q3DukN0WlQbrVtqPss6BZ1WI9nGXGEiInIWW2rIo1KbROCFq7tg0fU9oNNqMKWX9cR6p4sqcfJChWTb8q2nYGJkQ0RETmBQQx5368DWuK5PGgDgkTEdFR+3bOtJnLxw0VPVIiKiIMOghrwqPETnuNAlz3y7H8NeycSKbTkerBEREQULBjXkdalx4U6Vf3P9nx6qCRERBRMGNeR1y+8Z4FT5CCdad4iIqPFiUENe1zoxyqnyhRdrsPZgPpdPICIiuxjUkN8rKK/GXUu3Y/bKPb6uChER+TEGNRQwvt+d6+sqEBGRH2NQQwFDALufiIjINgY1FDAExjRERGQHgxoKGAKACoMRNbXW60cRERExqCG/MbxjM7v7DUYTOj+zClcuyvROhYiIKKAwqCGfCA+xfuvpLRa1tOV0USUAQBAEHC+4CIH9UkREBAY15CM/zxxqtU2vVf52LLxowL9W7MaVizLx0o8H1awaEREFKAY15BNtEqNwz5A2km16nbKWGgDo/cJqfL3rDADgoz+Oq1o3IiIKTAxqyGcs832Vdj858n9bTuLb7DOqnIuIiAKH3tcVoMar1iSNavQ692Ps00UVePqbfQCAq3qkQqNRJ1AiIiL/x5Ya8plaiwTfECe6nyy9+MMBAEB5tdG8zci1ooiIGhUGNeQz4gUqf/jnYOjc6H6qz6sRJxsbjJzPhoioMWFQQz6T1jTS/H3XFnFOjX6yRZyXw6CGiKhxYU4N+cxdg9sgr6QKY7okA3A/UdhyvhoDZx4mImpUGNSQz4SH6PDC37qaX+vcyKkBgJpaASZRYMOWGiKixoXdT+Q3QtzsfjLUmiRBTTWDGiKiRoVBDfmN1CYRbh1vMJokc9+wpYaIqHFhUEN+44a+aZg2IN3l47/YliPtfmJODRFRo8KghvyGXqfFi3/rhhYuttgs+OUQjp0vN79mSw0RUePCoIb8jjuT8P1j+S7z9wxqiIgaFwY15HcW39RLlfNUG2tVOQ8REQUGBjXkd3q0bKLKeSxbas4WV6LSwECHiChYMaihoCVOFD56rhyDXl6H4YvWA7CeqI+IiAKfT4OaFStWICMjA8nJyWjRogUmTJiA3bt32yz/xhtvoE2bNkhKSkJGRgays7O9V1kKOOJ5atYdygcA5JdW4++fbMeEN/5ADUdHEREFFZ8GNT/++CMWLVqEvLw8nDx5Er169cK4ceNgMlk/bD777DPMmzcPq1atwrlz53DDDTdg7NixKCkp8UHNyZemZ7TGo2M7Oiwn7n7SoCH5+NcD+TiYW4o9p/neISIKJj4NapYuXYqBAwdCo9FAr9fjhhtuQF5eHgoKCqzKvv7663jwwQfRoUMHAMDMmTMRGxuL5cuXe7va5ENXtG6K2eM7ITYixGFZSVAjM6CKXVBERMHFb3JqcnNzsWDBAowZMwZJSUmSfQaDAbt27UJGRoZk+6BBg7Blyxab56yurkZpaankiwJDfFSo7PareqQiVK9FmM7xW1ecU6ORi2qIiCio+EVQM2TIEKSmpuLkyZP49NNPrfZfuHABRqMRycnJku3JycnIz8+3ed758+cjLi7O/NWyZUvV606esfye/rLbu7SIAwCE6hUENaKWGrkFwNlOQ0QUXPwiqNmwYQPy8/PRqVMnDB06FFVVVZL99Tk2ln9ta7Va2fyberNnz0ZJSYn5KycnR/3Kk0dcnhKL12/sIdm28Lru6J3eFAAQoqSlRpJTY81kYlhDRBRM/CKoAYCkpCS88847OHbsGNauXSvZFx8fD41Gg8LCQsn2wsJCJCYm2jxnWFgYYmNjJV8UOMZ1aS55fUPfhpY2RS01DrqfaplTQ0QUVHwW1NTWWk+CptVqodPpoNPpJNsjIiLQuXNn7NixQ7I9KysLvXv39mg9yXciQnU2F7hUspSCo+4nLqNARBRcfBbU7N27F1dddRX27t0LoC4Z+JFHHkFycjKGDBmCqVOnYtasWebyDzzwABYsWIDDhw/DZDLh7bffxvHjxzFt2jRf/QjkBRrZjiNlLTXieWrkhj8xqCEiCi56X124W7duGD16NO644w6cPn0aer0eV1xxBVavXo2oqCgcOXIE1dXV5vIzZsxAQUEBRowYgYsXL6Jjx45YtWoVUlJSfPUjkBfYGrQUpiCoOVNcid05xXjkf7vRKj7San9Nre3up/Nl1fh443HcdEU60hOsjyUiIv+jERrRZB2lpaWIi4tDSUkJ82sCxLPf7sPSzScBACdenmjevud0Ma56ayMAoF/reGSdKJQ93p7Xb+yBKb3SZPfd8tEWbDx6AUkxYch6apQLNSciIrUofX77TaIwkRxb88uIu58eH3+5S+e21/205a+6IOlcWbXNMkRE5F8Y1JBfu31QawDAhG7SbkbxkG6dXBawAgY73U9ERBR4fJZTQ6REm8QoHHh+LCJCpCPiQsVBjYuzBdtrqeH8w0REgYctNeT3IkP1Vt1Q4kRhrRZ4ZHQHp8/L0U9ERMGFQQ0FJMsZhf85sj3mTOzk1DlqahnUEBEFEwY1FJDEicL1K2XcPaStU+ew11LDbBsiosDDoIYCkrilxtXlDt5afxQfbzwOAFiy8TjeWvenKnUjIiLfYFBDAUm8TEKtGwtTPvf9AdSaBMz9/gAW/XoEZ4sr1ageERH5gMPRT3v27JHdnpqaivPnz6OmpgYA0L17d5hMJixatAiPPfaYurUksiBOHA4PcS82rzbWir633SV1sdqImloTmkSGyu43GE0oqaxBs5gwt+pDRESucRjU9OzZExqNBuKJhzUaDebNm4fFixfj/PnzEAQBFRUVEAQBs2fPZlBDXvHs5M44U1SJzs3dmx26usZ64Uu5ibZ7PPcrjCYBe+eOQUx4iNX+8Yt/x7HzF7F+1nC0SYxyq05EROQ8h0GNyWT7L9fFixejpqYGWm3DX8qNaNUF8rHpGW1UOU9FTUNLjb2eLOOlnX+eK0fv9KZW+4+dvwgA+HV/Hu4ddpkqdSMiIuWcbrcXBAE//vijzf22prUn8ldlVTXm742Xhnnbex87Cty1/D9AROQTTgc1b7/9NmbOnGm3BYcokBSWG8zff7r1lGwZcSAjnt6moLwa+aVVkrKMaYiIfMOpoGbNmjV4/PHH8f7770u6nIgC2c0fbTV/v2TTCZhk+qDEI6zqvzeZBPR9cQ36z1uLSkNDFxZbK4mIfMNhTs0HH3yA5ORk7Nu3D//+97+xYsUKjBw50qqcZTIxUaDadOyC1XtZHOeYLu2rEbVWiltrXFxfk4iI3OQwqHnnnXeQn5+P/Px8jBo1CgMGDJAt995777FLivzSzJHtsXit8on1pv1nq9U2k2DdUiOOe8QhkKOYZu53+7H3TAk+//sAq+UeiIjIdQ4/UbOzs5Gbm4usrCxUVlaif//+yM3NBQAMHDgQADB06FCsXLkS33zzDYYOHerZGhP5gCSoEWSCGtELrYOmmiWbTmDHySL8dvi8upUkImrkHLbU1Ovbty/Wrl2LG264AdOmTcPatWvx1VdfAQAyMzM9VT8it6mR4iLOqTGZBOw5XYw/jhaYtznTUlPP6MZMyEREZE1xUAMAoaGhWLZsGbp3747169dDp9OxZYb8nkZxmGGbOP6oNQm47t3NMNhY5TtQEoUNRhMO5paiW4s4h61LRESBwOkOfa1Wiy1btmDQoEG48sorPVEnIpd9eFtfxITr8dFtfVU9r3hElEkQrAIa8f6TFy7ieMFFVa/vCTM/34Wr396Id3875uuqEBGpwumgZvr06fj1118BWE9CVl1dzSUSyKdGd07G7mfGYFTnZFXPa7IxT029mtqG/R9uOI4rF2Vizjd7UVJZY134El836Py8Lw8A8OGGv3xbESIilTgV1BQUFODHH39Ely5dAFg3s1dVVeHVV19Vr3ZELrDsShEvMJkeH4lXruvu1PlWbM/BLaK5bIwyo/xqZCKdZVtO4eWfDzp1LV/gTAxEFCycCmqee+45DBo0CL169eKcNOT33rq5F266oiWu75tm3pYUE4Z+beKdOs9jX+7Bobwy8+vTRZVWZWzl1xzMLZPdTkRE6lOcKLx06VJ88skn2LlzJ4DASYakxmtS91RM6p4q2WZSIRh/ZdVhq20Go+M5mmpNAnSiViT+DyIiUpfDoCYkJATdunXD4cOH8cMPP+Cyyy5D+/btvVE3ItWZBEAvmvAuJTYceRZrN7nCVlBTH0J9nnUKz31/AO/f2sfta3mSIAh4a91RdG0RhysvT/J1dYiInOKw+2nlypXo378/amtrcfBgXX7AnDlzmBBMAUkA0KJJBKb2a4npGa0xbUC6KuetttVSc6ll6ImVe1FZU4vb/pulyvXUJO5KXnfoHF5dfQTTl2zzYY2IiFzjsKVm8uTJmDx5MqZNm4Zx48ahRYsWuP3221FdXY377rvPG3UkUk39A3z+NXXJwsZaE+KjwvDk13vdOm+1sVZ2e6Blnp0tts4XIiIKFIoThTMyMrB48WLce++9qKiokOzLzMzEihUr8PXXX6teQSI1WabU6HVa3Nzf/dYamy01ASDQAi8iIlucmlH4zjvvxH//+18sXboUd955p3n7xx9/jKysumb1Dh06qFtDIhWpkSgsx2ZOjZ3L+WWyvT/WiYhIIaeCGgCYMWMGwsLCJNuWLl2qWoWIPOGK1k2x7UQRbrqipez+ri1ise9Mqcvnn/PNPtntJZU1ePVX69FSRESkPqeDmltuuQUAUFNTg86dO6teISJPWHpnPxzMLUOvlk1k93902xUY++/f7c4A7IpThRV4c91R2X0VBqOq11IF558iogDm9DIJ9UJCQrB3r3vJlUTeEhmqR59WTW0u3JgSF44HR3p3qoKZn2crmt/G4xjHEFGQcDmoIQo2vsgmyVdhjhwiIqrDoIboEubIgjeBiAIagxqiS3zxOPe7FBa/qxARkXIMaogu8cUQa0HFhBZBEFxKPmYYQ0TBgkEN0SXiHOJJ3Zt75ZpqNow8sHwnOj+zCicvXHT5HOfLDepViIjIyxjUENUTtdS8dXNvvPi3rh6/5PBFmTiY6/r8OGI/7c0DACzfesqp4+qXjnh7/VG8sfZPVepCROQLDGqILmmTECV5ndY0QrbcbQNbqXrdR7/crer59DrXutFeWcVJAokosDk9+R5RsMpol4DnruqCjikxAIC2idGy5e4f3g7fZp9VbaI+Y626WS0hOv6tQkSNEz/9iC7RaDS4fVBrDGibAABIT4jEZ/cMwKqHhkrKpcSFY8ecUapdV6tygnKITmvuUiIiakwY1BDZMfCyBHPLjZhexdYQnY1Zjl3125Hz6Pn8aqw/dE5ReYY/RBQsGNQQ+ZhWq8HFaiOKLro+8kjcMpN1vBAllTWYvmSbGtULOEs2HscjK3bDZGK4RtTYMKgh8jGdBrjuvc3oN28N1hzId+rYJRuPY/5PB2GUeYAr7dWqMNRi6gdbnLquP5v7/QF8tfM01h9W1lJFRMGDQQ2RAmp3EYlpNRoczC1FTa2Auz/ZjpIK5QnIc78/gPd//wvZOcVW+6JDlY8D2PzXBcVlA0VZlR+ugk5EHsWghkgBD8Y0Vi0qj3+1ByUVNXh99REcL1A2kd6hvDKrbdHhDUFNrUnALR9twTPf7nOrroFEzdmaiSgwMKghUsCTLTWWXUfrD5/Dk1/vxeK1f+K6dzeZt+eVVOHxL/fg8KUARpxH8/Q31sFKVFhDULP1rwvYePQCPtl8UnG9ThdVKC7rjzgAjKjxYVBDpIDOg+tCWc5TIwhA5qV8kAui5OHxi3/HF9tz8OTXewEAjvJgo0VBTXWtSbIvv7TKYb0GL1jvsIw/Y1BD1PgwqCFSQO25ZMRqLAIOkyDgoqHWqlzRpVyb82XVAOq6lOwRBzWWI4Hu+WS7S3W1Z+/pEryx9k9crPaPXBbGNESND2cUJlJA68XuJ5ODJoaUuHBF5SJDdebvLQOgPadLnKmiIpPf+gMA8NrqI3h0bEc8cGU71a/hDEf3h4iCD1tqiBTwZE6NZcDhqFsp5NLaTnLDuMVC9Q3/vb09ZYtfrCPlwZ/ZaNG6RkT+wadBTVZWFiZMmICkpCQ0b94cI0aMQHZ2tmzZuXPnIjo6GikpKZKv6upq71aaGiVPdj8ZTfYfkNtOFEoeohpo8Oy3+/D97rN2jxOvAeXJVos1B/Lx895cj53fVZ4a/fTLvlxc/vQv+GGP/ftPRN7n06Dmsccew4wZM5Cbm4szZ86gf//+uPrqq22WnzVrFvLy8iRfYWFhXqwxNVaOGmrmTOwku33elG4Oz+1oQcvr39uMypqGHJs/jhZg6eaTmL1yr93jxK1LngpqampNuPuT7Zjx6U6PnN8f3bdsJ4wmAf9YvsvXVSEiCz4NatasWYPJkydDp9NBq9Xi1ltvxalTp5Cf79ysqkSeJtf9JG68uXtIW/Rs2cSqzM3903Ft7zS7565RsEp3bonj0Ur2OEoqdpXaK4yriSk1RI2PTxOF9Xrp5Tdv3ozk5GQkJiaqcv7q6mpJ91Rpaakq56XGR677SavRoFb05OzXJh7ZOcXQaoARlyejQ3I0ACBUb7+Zp9ZB9xMAnLrg/Jwx4tYZ8QNezRW8/XmCO/+tGRF5it+Mfjp69ChmzZqFN998EzqdTrbM4sWL8f777yMkJATdunXDo48+iuHDh9s85/z58/Hcc895qMbUmMi11Gg1gHjg9cOjOiApJgyjOyejVUKUebtea79BVElrx8lC54MacexiK8Bxlz+3hvhz3YjIM/xi9FNRURGuuuoqTJ8+HdOmTZMtM3PmTOTl5SE3NxfZ2dkYNmwYxo0bh40bN9o87+zZs1FSUmL+ysnJ8dSPQEHu5v7pAID+beLN2zSQBjoRoTrcPaStJKABHI+cqjYqaalRtlyCmDiQEXc/qfmsV/Ncn2Wdwmurj6h2Pn9uRSIiz/B5S015eTnGjx+PPn364NVXX7VZrmnTpubv4+Pj8dhjj+GXX37B8uXLkZGRIXtMWFgYE4lJFfdcypnpnhZn3qZ0QFRsRIjd/QYFw4NLXVic0WSjpUbNpGE1z1Wf+Dy+awo6NY91+3yB0lJTWlWD3TnFGHRZokenDnCWySR4dH4mIk/waUtNZWUlJk2ahNTUVHz88cfQODlstqqqCvHx8Y4LErlJp9VgQNsERIpWvlY6zLtZjPuBdYXBlaCm4ameV9KQW6Zq95OdeCyvpAplVcpXHK+n1ozEAoDckkr8tDfXY4nSarjp/S249T9Z+GjDX76uitn/bTmJHs/9it0yq78T+TOfBTUGgwFTpkxBWFgYPv/8c6uk4alTp2LWrFnm108++SROnqxbjK+6uhovv/wyDh8+jPvuu8+r9Saqp/SP2CRVghrrZRMcqU8IXr71FF5f09Ct40zrSo6DXB575xowfy16Pr9a0XXEycurD+Sjqsb5n1funMMWZuL+T3dixXb/7Xo+kFs3gOHrXWd8XJMGT3+zD2XVRjy8ItvXVSFyis+6nzZv3oxVq1YhPj4e6enpkn3Lli3DkSNHJCOX4uLiMH78eBQUFMBoNGLw4MHYsGEDWrRo4e2qEwGA4pZFNVpqKl0IauoHVT37nfUK3koNWbgeB54fK2mhklzDQYBUaxIgCILDeyVuSHn/979QVVOL2wa1xtniSgxp38zpegN1LVL1XXt//FmAqf3SHRxBVvy3gYtIls+CmmHDhtkdWrpjxw7J68cffxyPP/64p6tFpJjS3tLuLeLQJTUW+8+6PqWAKy01tYKA34+ct5oHx9k8mJmfZ+PD2/rK7lPSq3PRUCtZXFOO5azKX+08g6Wb61pmf3pwCDqnOp9jo+bQdSIKDH4x+okoECnNqdHrtPjhn4Px+o09XL5WpQvdMasP5OO2/2ZZbXf2Wb/6gO3JMJUEDsUVBgBAtbHW5ppJljkv4lt7KK8hGNzw53l8se0UAMfrL/125Lzs+Ug5AXWzRm86VqBKlyCRpzGoIXLRyE5JAIDWCZEOy2o0GkUzB9viSqKwLeqOfnJcpqSyBtXGWvR9YQ2ufDVTtozl4py2YpBb/5OFx7/ai4e/yEbnZ1ch204i6/rD4qCGUY0rBEHASz8exM0fbsUjK3Z77ZpErmJQQ+Si56/uiuev7oIv7h2oqLxBwXw0trjS/WSLmitoKwmQSiprcLzgIsqqjcgprJR9aNXWWrbU2A9Cvt51BgajCU99bX/9K/P5FJUiSwKAJZtOAAB+9MKipfvPlqD/vLVYsc1/E7vJvzGoIXJRdJgetw1sjeTYcEXllUyyZ4uaTf+fXMpVUYOSoKa0UjqsW651x7KlpkbB3D2A8m4lNtS4xtuNJg99no1zZdV47Ks9bp+rvNqIpZtOIL/U8bpp6w+fw9/e3oij58rcvi75FoMaIi9xp6XGna4rT1Ly0LOcOFAuELLMqVHaMmU5q7OtrgtvxDRb/7qAA24kg/sjb8/KrOZ8Qs98sw/Pfrcf17+32WHZ6R9vQ3ZOMe5XsNo8u8f8G4MaIi+pNgZfoqWSlhqTxYNK7sFlOfpJKcsWGFvV0Wg0EAQB58qq8MOes4pbgpQ6W1yJGz/YgglvbFD1vL7m7ee3mpdbfbAuwf2UE+umFVXYnyzyk80n0H/eWqdbdIy1JpcWpSXnMagh8hJ3Wmp8yd4kg0r+sH5i5V58l31WdIz1QS7GNDh2rhyjXvsN3+2uO7+t6mgA3Pj+FvR7aS3+sXwX3ss85toFbXDmwUmB65lv9+NcWTWe/Nq5uZ/uWrodQ19Zj1/25XmoZlSPQQ2Rl6gxs7AvhOq1+M8fxzFh8QZcKK+W7FM6kuodURAhn1PjWlRz0VCLo+fK8eBnuwDY7ho4er4cWScKza/XHLQ9TN0Vao4o8ydeb6kJkPto2froSP30Aks2HfdEdUiEQQ2Rl0ztn45pAwJvVlutRoMXfjiAA7mleHPdUcm+DaK5YJSS635Skkuh5IFnq8Se0yXSDWpnDgfGs7hx8eDvxNW3j2UOGKmPQQ2Rl4TpdXjxb92w9pFhGNwuEQ+P6uDrKikijiXKLRabnPv9ARfOJ5dT4/gJpCzwUVYHtR8twRrTBErLSaDgKDzPY1BD5GWXNYvGsrv744rWTX1dFUXEXStqjE5xtaWmVlFLjbL6WeYJCYLgVs6Tku6nD3//Cwt/OeTyNXzB2yGNmtdz5VyejjkY1HgegxoiH8kva5g/44d/DvZhTewTP6/VGDWkZJ4aR/VwpwxgPbnfIyt2o8Ocn7Hgl0MorzZi49ECpwI4R0UFQcBLPx3EO5nHAmoUDBtq1MXuJ89jUEPkI0PbN0NUqA6D2yVCZ2+IkY8ZRIHMD3tyzV0SziZL1pOfp8ZxsKTmHCaWd3vlrjMAgHczj6Hb3FW45aOt+OD3vxSfz1FLjbjqwTi0Xy3BHkSxpcbzGNQQ+UhCdBi2zRmFT+7s53RQM3v85R6qlWP1IzkMLrbayAUARgWTCyrp4lHeUuP4HP/brnyqfnHuiXzOUMO9CqR1qLw9+Z6amA/UODGoIfKhyFA9tFqNzRW/bQ0D79oiDi3jIzxZNZsO5NbNmutqV5SrOTVKRn0rfQgrCiyciD3Ez0+5Z6m47n7cKGfF+5PvBXcgEkgBbaBiUEPkB+QedI+O7YjnruoiW16v1UCv9c1/39LKuhFQribWygUnSnJqFM1erOLoJ2ceP+LrylVB3FLjz12NloI7xPC+wPnNBy69rytARNIH3R+PX4nUuAhotRr8ul9+BlK9TotuLeJwvOCit6poVlpVN5W8mt1Pqo1+Uti0oKihxqKQwWjCeYvJB+uZrLqfNKiqqUV2TjH6tmoq+flstcr5o0DuwfHHqgfQrz5gsaWGyA+IH3ShOi20l4IcvU7+UzBEp7HZiuNpJZdW3Xa1pUYuOFGrpUbpg6z+fhuMJuy1nJjPXEb6eso7G5Hx8jrJtvrV08XBZX0d/vnZLtz0wRa8vuaIJKhR8mATBAGrD+Q7PVLK1eRtOzVR+XwOruaPkYiKGNN4HoMaIj+gFT1Bxd/b6oPXa7VoGhWKuwa38XjdLNU/OF0NauRaU5SMflKUU+NkovDDX2Rj8lt/yJaxbFHZL7MC98JfDmPJxuN4+eeG+Wfqg6/VB+qWYliy8YQkqFFSx8zD53HPJ3XrBSlVaajFsEXr8c9LS0Y0dv4YIDGnxvPY/UTkB8QfdTrRB5+tv7xDLrXg+CI/o75VZYGLE8nJ9Vqp1VKjtGGhfr6QH/fmKjvAhu/3nMX5MmmXlCAA+aUNcxBptRrJz2eoNUEQBLsPOPE6VUqtPpiPnMJK5BRW4s2pvZw+Xo73137y7vW8LYDSqQIWW2qI/Iy4pabGxlBnva7uv64v8jMO5ZVi3k8HsebgOZeOdzan5qKhFlU1tcqWSVA8+klJGdfv7QOf7jR/r9NqJHUf+epvuGvpdtTUmrDxaIHs8a483D0xhDmQYwz/bBTxy0oFFQY1RH5G/NecrdWr61tz9D740y+nsNKpieksOTuk++lv9mHQy+s8OqOwbBllp5Ktw/aTRebXOo3G6udbd+gcXv31CG75aKv8OVwIJ5xZKVwQBGz96wJKKmoclvMmNQMRf2z18c9AK7gwqCHyY7Ye9vUfjlFhrvUgJ8fKz3/jDfUPm482/IW319et+u2o+6nwokHh2k/KKBrS7eJUNi/+eEASmFp2P9X7v80nbJ/YpZYa5WW/35OLGz/YgnGLf7d/Tuer4RZ/DETkuFpPxjSex5waIj8Qqm/4+0KcJ2Nrpt36Lqq4iBCXrufLYcULfjmE/WdLUHSpleCqHql47Ms9Do9Tkkys7pBuRaey8unWUwjRaWC69LvTaTSyrShaO61srjwznRn49POlXKLckiq75QIlyJDjjxP5saXG8xjUEPmBxOgwzBzZHiE6DSJDG/5b9kxvIlu+/nnoalDjy8/WPyzySNYfVpabM/bfGxyWcXZIt7tl7B97KajRamSDU3vnd2VotjPdT0p/tvrh+6QOLmjpeex+IvITD4/ugH+MaC/ZdlmzaDw7ubNV2YSouu6j2AjX/i7xp6GlZVVGReUUJQr7aEZhSzrJEH3nF+N0pY1B3Er1v+059lutVPr1/7w3F7f9NwsXbExKqMTWvy7g2nc3Yf9Z+fmCgokf/bcLWmypIfJztw5ohU3HLmDXqWKsnDEIcZEh5u4qcauOM/xpqv6L1cqCGkdOXajAyl2nFZVV9HBx4wkkCWo0Gtl8IHuBjmujnxq+f/TLPYgI1WFS91TZsq62Qp0prkRCVCjCQ3QAgBmXRnn1eXENXprSFbf0b6WgngLeWncUnZrHYlTnZNz4wRYAwG3/yUKYXr2/s/2x64xBjeexpYbIz+l1Wnx4W19kPTkS6QmRki6nqDCdS+dU8uEaFerauZ2lVlBz5auZ+PeaPxWW9tzoJ8tj60Y/WecD2RrZBrg6+kn62tZMyYDrP1vGy+sw6rXfAADZOcWSfU99vU/2GEEQUFxhML/+/c8CvLr6CO7+ZLuk3IWLBstDVVVpqMV//jiOkxe8v7RIPXY/eR6DGqIAIZdYmhQT7tq5PJxT4oyLhlpVzuNMF4+Ship3fnzJWk82cmrUbKn58Pe/8MIPB6TnsFPenYa600WVAIC/vb3RbrkdJwux5kA+Hv9qD3o+v9o8J88TXzlOCrfkytByyyNeWXUYL/xwwByU+QRjGo9jUEMUwOKjQvHetN5OH3ffsLYOy9gbnaMmtVpqnOHqcG2lakQBi85G95O9YezOPsRf+ukgKmukwaHcOS6UV+NCebVXcqqufXcz7v5kO1Zsr+sSfGPtn/gzv8zuiCu5n3rHyUL0emE1vtoh7Vo8W1yJN9b+KWkFsmfLXxcA2J7Q0hsY03gegxqiADeua3Ob+9onRVttS44Nw2XNrLdb8lbeTbkvghoFjxd3WqqMorUgtFrryfcA+60xajx2Lc9vMJrQ58U16PPiGkXLUthja1TUlYsysc3GEg9ajQajX7c/L4444Cm81B11zyc7UFxRg0f+t1tS9plv9+O11XUTGJ4trnRYZ+azNA4MaoiC2NOTOqNNYpRkW3iITtFf6t7KJfZFS41WwSef+BY523Iijhl0WmVrW0mPdz+ssTxDWVVDIFJe5d5Q7R7P/Sq7/XjBRUy9lPjrrr9fyrmpkVssDMD2k3XB0/6zpRj08jrsEM3iDEBxZPh/W06avz9XVi27MvqiVYfxoMKFQutao+SDLEf/79RfZb3xYVBDFMRS4sKx8LruVtuVBCxey6mpVienxhlKWmo00JiDGWeHZIvpNBqnH1ZqjNyxPIf49+nJR6etAM7ZQG37ySLsPFUkey92nCxCscUSD1/usD3yrayqRrLK+u9Hzpu/f/obaYLz8xa5SQDw1vqj+G73WYd1PldWhdGv/46B89fJ7rf3rvt443H0eP5X7DsT/EPbPYlBDVEQC9NrrQIYDZTNU+OtoMYX3U8nCy9i0pv2J/PLOlGIwQvWo6SiBoUK8zbk2FomwR5nSruSROuLBgFXArW/f7JdNhi6/b9Zjq8nuovvZB6T7LvNzvHiUWkmk2CefVls+8ki/HW+3Gr7sXP2R1bJ/Zcqq6rBudIqPPf9AZRVGfG4C4nUaqipNeHY+XKvr/elNgY1REHMVleTkpYab+XUXDR4P6jZd6YU+86UOix3prgSy7NOod9La2X3K/n419vIqbHHmeeKrbKWw8LFycq+eHC50qVmMJpkj3P2PZNfKp+cLHcfxL+qr3edMc/FY2nEq9ajqBz9l5HbPe2jreg3r+H9JX6v7DxVhJxC6+4wsZU7T2PUa7/heIF7Q9Uf+3IPRr76m6IWKX/GoIYoiNW11Eg/SjUajbIh3V76dChXOKOwr9jrDlDynM4vrcaaA/lOXdOZoENpsCDuArO1ppgnuZonJDedj7OnslXeUay58ViB/QIWxCMG5X6H32Sfxbh//w6DseGH2m0xn1D9ffrrfDmueWcThixcb95XVVOLq9/eiIW/HDJv+9eK3Th6rhxPrtyLud/tx4xlOyTXrqqR7941mQS8vf6oeVTY17vOAABmfp5tzr8SBAF//FmAfyzfice/3BMQXWOcUZgoiIXpdQ7/eowI0VkNBwbqckG8wd2ROJ5WbbSd86Mk+DhVWIFTDv7atj6vE2UVnkPcUuNOjpAScvfF1ThKyers8nVo+N5WQCW3XVJ3Jy8t/h9jEgCdzH+hQ3llWH0gHxO7y49arP/VHMwts9r3XfZZ7M4pxu6cYjw27nLJvmpjLZZsOgEA6DdvLbKeHIm1B8+ZJzl8dGxHPHBlO3P57/ecxSurDgMA3rq5l+Rc3eb+irmTO+Pfa/+U5C59sT0HJ16eKFvvEwUXcfRcOUZ1Tpbd7y0MaoiCmGxLDaT5Mq0To3Aw17orxpcrefuTNQeVLbipJmdmFFbcUiMqZm82YzXIVcnVLi81RoLZCuLUOLeYuKu31iTY7MI11NoOlOvrJHeovT8AxNc6X1YNkwDM/LxhxNYrqw5LgpqTolFe/1huPbJr7vfWCdNAXctlQnQoDuWVoWlkKCoNtRh4WQKGL8oEADSNDMHPM4ciJc61iUHdxaCGKEiF6rXQajWyyYnODmkmeZ5q75A0FgiC3cRumzk1FjvE3U8eb6mR2eZqAOFq3CE+zNa11U4tEgcirt7j+t+Ts7lwliP6BEGw28rl6n/vKe9stJrAcMecUebviypqUOGDPLl6zKkhCkILr+uO3c+MASDT4qKRfgA+OKId5Oi9lVQTwNT+S7+e9IHsoKzNRGEp8UPW011+cvfFlcYhZ2ppLwi3dW357icnLmpVh4ZKuNoaVv+rkQtg7LaeWuwyCfbfO67OGC43I7Plul0hOt99dvBTiygIRYfpEXFpQUpHLS4D2ibg2wcyrLY7+tBrEhlid39j4KlBREryQRzt921OjfU2TwWA9q5Zz1aLhdxtcGUx0Xri/zKu9vA1dD9Z//9zIqaBSRDszo+kZkus5e311shJOQxqiILAJ3f2w/iuKebX4geIXE6N5LUGSIgOtTqnvc8lvVaDXU+PRkSId1by9ldqDo0+lFeGG9/fjGpjreS8jgIQm4nCFntMNlpqPDG8W+0WEFeIfy5bD3fVc2qgrKXG3uSP9XUVN5SatzkRiQiCdTAn2PlcUJOeQQ0RuWNoh2Z4d1of82vxZ7jc54v480wDjexfVvb+2jKahEtDw12qbtAoVXk4+tbjhfhyx2msvDS8FnAcDLiUKCxaesDTrTYN13f+Omq9vWy2ZjkaLu5kBcSBpJL7Kj9KzDqnpn6bMz3CJkGweu8IDj4XXGUZH+nZ/UREahJ/WFomHGo0Fn8nauSHbyv5S84bqz03Nqst5rSprKlFpUF+tIwg8+CqZ7lkkq2cGk/k1yjtfvLWJIC2hpPLdUu5UyXxraw/t9zPWB/8yN+nun+1FiOpLLc5rotMXpOXWmrY/UREqogOqxvQ2K9NvHmbow8vrUY+f0bJ5xJDGvVlHj4veX3lokx0f26V1SRqn249iSteWoMDZ+VnRjZaRDXiB5o4wPlG1CqkFqXdTw6ToN2og/hYW8GTw9YjJysgPl/9BIf2fkZ7gZ74/59RZkSUZZea5X9zuetKW3DZ/UREfm7rkyOxZfZINI+LMG+TW/tJ8lqjkf0QUvSh5+ZnV0wYZ5VwpKSyBjW1gmRBRgB46ut9KCg34EHRXCRilTW12HmqyBzc2GqpeWLlXpwpll9V2lVyz3G5VhFPJw+br+1ETo07icJyuVD2fka5asm1ytTW2g50lNSl4Xrilhq7h7tFLzfroJcwqCEKIlFheqtJrxyNotDARkuNgk+90W7OHhovk6BM8q59dxPWHz6HFdtz8MOehvV5zpdVy5b/YU8urnlnE15dfQSA7ZYaACgsd33BTjlyD3K5wMKTQY341LaCGvlJAl2/plz3k/2gRu6e1P0r/t9Xn3Qs1yVVzzL5WHZkl7ilxoNRjS+ng+CfSURBzmqaGpnXOpkmF7k8mz6tmmLHySL0bdUUAPD81V2xcqfr3ReNffSUs6Z/vM3pY97NPIbHx11u0TUi7ZpSuydCabDgrRFRttd+UrcCchMc2ruEbJdc/XGibQ2tNw3b6gId2/9/HOXUeLItxZc5NQxqiIKcfDKvRvSdRvGoinen9cbXO8/gmt5pAOpyeNonRePPc+Uu1S2K3U8eFxdRN5+QOI4xWGYRi7yTedT9i8oGMN5tqdFoGoIG5+apcZ10hJmrLTXWx9XI5NQ4Gl3lMKgJ0iR/fqIQBTnrnBrp0gkajXwXVYjeOtJJjArDvcMuk2xzdcFBAIgMZUuNp5VU1qD1Ez9KtlXVWAc1JpOAaqMJC3857PY17XWrONom5sxj1/ItrKT7SXb+GjeiGsncOObgxHZ5e/dJ0pUlM3zLMqfG3s9veW5A3URhb89BZA9zaoiCnKMPL40GCNFpkBQTJtkeGaLD0jv7oc+lrqb6spbszVoqNqqTdf5Nk0jncmrG+HgF4GB17Hw5+s9fi9kr96hyPrl3hCtDutV6Vtq6jtoPY+miobZbagSZwKWeXC5OfU6NkkCtoS7277eaPUTeSvhWwqdBTVZWFiZMmICkpCQ0b94cI0aMQHZ2ts3yb7zxBtq0aYOkpCRkZGTYLUtEdRzm1EADjUaDjU+MsCo3rEMzDGmfKNpm/UmodJqTXulNrLYlRCkPaiJCdJIJBkk9Mz/PxvmyanyTfdZxYQUqa6zn1ZF78Mm1GHmC7e4ndUc/SZOxLwUidn5E2S45cy6OXH6O/Ag2+brY36ZmSw2Dmksee+wxzJgxA7m5uThz5gz69++Pq6++WrbsZ599hnnz5mHVqlU4d+4cbrjhBowdOxYlJSVerjVRYJEd/ST+/tILVxehUzojbX1uh5gz60fFRuh9moBIymW8vM5qm9zb5IqX1nihNtYTEdZT2k2mlPw8Nc4N6TZ3W4nq3NDq07DNskvKap4aR6PN1Gyp8U5sqohPg5o1a9Zg8uTJ0Ol00Gq1uPXWW3Hq1Cnk5+dblX399dfx4IMPokOHDgCAmTNnIjY2FsuXL/d2tYkCivyQbu//lTagbbzVtjA9c2oaC1f+mi9TaRkK25PvOTjO6es0fO/ukG65IfhyXVL1LId0K53VWQ1sqblEr5fmKW/evBnJyclITEyUbDcYDNi1axcyMqQrCQ8aNAhbtmyxef7q6mqUlpZKvogaG7nGjZZNI5AYHYZWCZEOZ/+0t/geoOwD7eeZQ9AuKQYTuqVItoc4MUmXo3qQf5NLdlWTvbeh7XlqnM/zsUc+EFFW3npfw/dy+TmOWkgdLQGh5hIVDGpkHD16FLNmzcKiRYug00n/ertw4QKMRiOSk6VJgsnJybKtOvXmz5+PuLg481fLli09Uncif2YZDGg0Guh1WmyePQLrHhkuabWR5s8oO7+d0cFmnZrHAgDemtpbklvjzHTqQToCtdGwN4zc02w9dD06pFsmD8bqWnYvJpOfI3P+etbLJNjvflJzyS8vrYmqiF8ENUVFRbjqqqswffp0TJs2zWq/6dIv1LLJXKvVmvfJmT17NkpKSsxfOTk56lacKADYSnwM0WmtclQ+vK2v+XulLSNyH55RNoZqa7UaJMeEy+5zhDFNYKs2uh/U5JdWIfPwOaePs/XQVX3yPXHQYKelpmH0k8KWGpn8nFqTYHfkofy8QPJ1dZe3FiZVwufz1JSXl2P8+PHo06cPXn31Vdky8fHx0Gg0KCwslGwvLCy06qoSCwsLQ1hYmM39RI2B5eeNveAg3IUZfuU+HF+/sSdWbM/BmoPWD6AnJ3TCiQsXcfeQtiirqjFv12r86y8+8j/9561VVM7yISvXVXOi4CIKL1ovD+HO81ludJKzicIN++x3ZdWaBLtzRMkmIYs2sqXGAyorKzFp0iSkpqbi448/tpm8GBERgc6dO2PHjh2S7VlZWejdu7c3qkoUNNTuxpH7azEsRIde6U1lSgPpCZH45aGhuK5PmmS7o/Vibh/U2uU6UvD7LOsU+r20BusPncMz3+6X7CsVBc/1hi/KxC0fbVW1DpZBhyAIqLHT7WavpcVRTo3RJNjNq3G0UrqarStKR0B6g89aagwGA6ZMmYKwsDB8/vnnVknDU6dORYsWLbBo0SIAwAMPPIB58+Zh9OjRaN++Pd59910cP35ctruKiBrEhEv/bykNappGhSgqL/d55krcZNlNNqxDM6TEhuOGK9Kg0WjQM62JC2elxuRcWTWmL7FeH6u4wjqoscWZx/PHG49jekYb82vL1pWbPtiCrccLrY6rL2d/XSj789TUmgS7c9XIZWbIdY+pgUEN6kY6rVq1CvHx8UhPT5fsW7ZsGY4cOYLq6obVZ2fMmIGCggKMGDECFy9eRMeOHbFq1SqkpKRYnpqIRPQ6LXY/OwY9nvsVADC4XTO75d++uTe+3JGDWWM6Kjq/3F+ESif2Eh9q+bkYFabDguu6KzpPr/Qm2HWqWFFZIrsuvSmNtSZ8vcv+Yq3PfX8ASTHh+OPoeUSF6vHRH8fN+2pNgmxAAwAVhlpsOlaAlFj5/LK8kirM/a6htUlunhqjyYQj+WU26+bNRGHL4eW+5LOgZtiwYXabvyy7mgDg6aefxtNPP+3JahEFpbiIEGx8YgQyD5/Dtb3T7Jad2L05JnZvrvjccn+lKV0PSlzK8vPAmTlKYsKVT+IXDEJ1Wp+OJgpmu0+XWK2VZc8Dy3fKbj9ecNHmMc9+t9/mPgB4bfVhFIlal+pHP4mDkorqWtz8YUP3mWX6hqO1n9RMFPanlhq/GP1ERJ7XokkEbunfyqVkYHvkPhwNRpOiPnvBzl+O9wxpq7gOwzvYb31ypFlMYA0ocGZ+H/KNxWv/dPnYFdtPS14fL6jA3Uu3YfuJIvO2M8WVkjK1Fq0lL/9y0Oq8glCXh1NhMKq67pWjJRu8yeejn4gosMl9nhmMJkXN2x1TYmS3b31yJJJtNM3L0es0mDH8MrybeUzxMWKBFiLodVoA1usrUXBa8Mshq22WrT0bj16w+xoAZn25B7tzipEYHYpJ3VNVq9/FanVmflYDW2qIyC5HD3y5pudqo7IH7uB2iVh0fQ98/4/Bku3OBDS2fDVjoOKygTaxn6vrdFHjtjunGABQUG7Akk0nVDvvv1bsVu1c7uL/DCJSXV33k+NyGo0G1/VJQ7e0ONXr0KdVPKb2S3dcEOquWOwN7H4if/Xm1F4+vT6DGiJSnSuzxz53VRcAwEOj2qtdHYfkgprbBrbyej2UYksN+avxXX07Ipn/M4hIdcmx4eiSGuvUMbcPao2sp0bioVEdVKtHb9E6U/bINdTUB1n+SM+WGvJTvm71ZKIwEanm/+7qh12nijGmczI0GuC1G3qgsxPBTZKL60IBQGK09Qima3un4dEv9zg81vJz+LaBrWzOcO4PQtlSQ37K1/9t+D+DiOxy5kNqSPtmeHBke2i1Gmg0GlzTOw2XpzjXYuOqaQOs82e0Wg2ynhyJ0Z2T7R4r/uty+T39MWdiZ9Xr56ymkbbn3mH3E/krX/8xwP8ZRBQUwvQ6dJdJOE6KDZesPi5HHNQMuiwRofq6j8ZvH8jAx3dcISqnvD5tEqOUF5bx1YxBNvcxUZhIHoMaIgoarj7qbR3Xo2UTXHl5kvm1o0U3ASA9PhLZz4zGM5Pda+0JD9EhMlR+okQ9W2qIZPF/BhHZ1bd1vK+roJyLTd9KD9MpbKppEhnqUj3E9DqNzaRLV1pqbuhrf3kMomDARGEismtA2wR8end/tEqI9HVVHHK1pUbpiA29E/1PSpaJsH8trc1gy5Wcmueu6orTRZXYdMx6plnyfxqN/VW9qQ5baojIoYx2iUhr6v9BjauUBjU6J1pI3H0A6XUamy1DrgQ1EaE6TO6h3tT45F0RKq/ZFqwY1BBR0HB14IXi7icFBeuLuLvGn16rsdny5OqQbiX1J/+k9kK0wYpBDREFDZcThZW21DjR/WQrydeeBdd2M3+v1djOqXH2B116Z7+6wxjTBCy21CjDnBoicktGuwRsPHoBY7vYnwvGG1ydI8NRrPLkhMuxeM2fmDelG+7+ZLuicw5sm4Bre6chPioE+8+W2sxleXNqL5gEAZ2ax6JDcgz+On8RYSE6hIfo8NKUbrhv2Q6rY4y1zi1DEX5piLqvZ3sl14WFsA1CCQY1ROSWd27ug18P5GGcj9d8AdxpqbG//+9DL8Ndg9viQnm1eVt8VCgKLxpsHqPVavDqDT3Mr1s/8aNsudiIEAzr0Mz8evaETubvx3VNwfK7++Pmj7ZKjhneMQmr9ufbr7RFXer+VXwI+Rm21CjDtzgRuSUuMgTX922JmHDbM+D6OyUtGDqtxhwcAMDKGYNk17dyNrByVD46XPq35+UpMQjTO/fRXf/zsaUmcDGnRhkGNUQUsC5rVjdr78hOdV1fLicKKywnDgrUGl3rKNCwTBeOiwhxOjipj8UY1AQuttQow+4nIgpYvzw0FOVVRjSNqpvszvZ4IfsUJwqLgxobY7adzetxVLx1onQofUa7RKeDN7mWmkfHdsTHG0+gQNSl1pgE2rwv4cypUYRBDREFrBCd1hzQAHA5qUbpoCZxTopJUGc0kaNTxISHIOupkTh+/iJ2nirGXYPb4Jf9eU5do37Ulvjn1Gk1MJqkCcfN48KRW1Jl8zwtmkTgTHGlU9f2VzqNBsYAimrY/aQMQz8iavRcG9ItuNwy5Oy1k2LC0b9tAmYMvwyheq1TC2vWXcP6WjqNBgZjQ1Dz1s29sHn2SAzv2JC0PFK07hUAFFdIE6M/uLWPw2u/JkqW9ieB1hXHoEYZBjVEFDRcXyZBabmGgu5OrlfPlWer8zk1deXFQZlWq0GNaGj4pO51sw3nFje01FTW1ErOc9Egfa2kHrEBnEDuT5hTowyDGiIKGq7PKKzsQEmisI3uJ2er4EqLgbNHyHY/aYCaWuvI7HjBRfP3lkHNnImdJK91Wg1SYsPtXtvRMPLMWcPtFyAAzKlRineJiIKGy4nCCsuJWzpMgqBC55NrgZizychyo59szY4875q6WY1fvb4HLk+JMW/fPmcUpvRqYVEPYN2sYcicNRwnXp4oez578wSG6bVonRil5EdQnaDa+DXvYEuNMkwUJqKA8uzkznju+wNYfFNPq32uBAjju6agpLJGUVlxHKBWjqmz+TGuHFMfBInvj9bGSa7rk4YJ3VIQGarHqE7JiAkPwd96tkBidJhVTo1Wo0FkqB6tE20/SioMRpv7EsRJ3gC6pMZi/9lSRz8OosP0KK+2fd5gFO7CshuNEVtqiCigTM9ogwPPj8XVPVs4LuzAvCnd8O40x8mu9cQtJHqdRqXFlJw/h7NdVjqZnJoQrRbzptS1yjw9qbOkfGRoXZASFxmCJyd0QudLkwxathApWQvLXvARHy0NasSnf2lKV3z+9wGy3S7vTutt/n753f3RNNJ+3k67pGirbWokeXsTW2qUYVBDRAGn/qFrydkYw5UuiHuHtsU1vVqgvcyDsq4Szp3PpZYa0Se3ksBCbp6asBAtbu6fjt3PjMFdg9sou67FpZTc74rqWpv7WjaNtLmvTUIUBrRNwOqHh1ntE88XFB8dil3PjMGJlydiyfQrEG/R+gPUrd1lac6kTlbb/Jncz0XWGNQQUdBw9q/vvq3inb7G7Amd8NqNPaHRqPO3viuLcIqvrCQoahjS3bCtfqmFOAetHGKWAZSSFiO9znaZuVd1UXxtW/UQ12F4xyT88tAQtE6IxDW9W8iW6duqKTY9MQK3DWyN6LDAycBoleCb3KNAEzi/USIiB5TGB1lPjcS50mp0FCXC+oorLTXin7MuKLLf4tQw+knUUqN3vjvDMohREtTceEVLfJ6Vg8P5ZZLtLZpEINli5JQkTJQJxOqJgxrL3Ukx4ch89EqcKa7Eyp1nrMqP6ZKM1CYRAICfZw7B8qxTeDfzmHn/pO7NcdvA1ii8aEDTyBDc+MEWhz+jN7RhUKMIW2qIqNFJiglH1xZxPru+VvLsdqGlRuNcS43cPDWhTi6KWXdd+6/lRIbq8ek9/Z2+Vj25wEkS1NiohHir+Bzi+90yPhL92khb6165rgf6tYnHuK4p6N82Aa/d0AMt4yMc1vP7fww2D2//6cEhmDOxk+zCo93THL/v5H43zrSoNWYMaogoaMwYfhkAYHKPVK9cz9V5avQ69z56xdfQKYgsGoZ0N2xzdqVvuWspDcdcm4vHesSWuR6SoMbG8Rr57y3zqCzrZnm+a3qnYcE13e3W9ZsHMtAtLQ6rHh6K3c+OQefUWNw9pK1sgvJHt/e12jaqk3Tm5k7Npau/L7zO/vXl/K1nKgZdliC7z5XWwUDBoIaIgsagyxKx8+nReENmuLcnuPpsCBUFNbVujg23NTRbroxG5e4npbGK0oeo3PkcttTYOpck78h2BRQlPzuof/3vMy4iBHERDS0qcrNOJ8WEY71owsHxXVOsgvBOKTH4W8+6bU9N6IQb+raUras986/pjuevluYspcSG48kJl2PdI8PlD7Lhhr5pTpX3JebUEFFQCYRRIuLk2VqTndnpFFDSCiI3+kmN7ielYZ0rydD2rqAXDf+y9fNL5uSx0f0kd7xcd6Cje2xr1mSTjbU02lhMOCg+f5vEKDw+7nLEhOtx3/DL0DG5Ie9r7SPD8f3us3ht9RG79QGAiFAdLO/exO7N8fehda2ZK+4diBXbczBj+GUY+epvknLfPJCBv7290fz6+au74sqOSZjx6U4AwCOjO2DgZQk4mFuK+KgwvLnuTxzKK8MVrZs6rJenMaghInKRqw/r7mlN8PuR8wDsz7irhLKcGuuyrnQ/Wf68arfUyJ1b7h6Le+9sdj/ZuL5l95OSPCGHQY2N/SaFrXDiw7+8b6B55fnLU6TdUG0So/DgyPZ4fc0RRZM/is87uUcqHh3b0fy6X5t4cz7RVT1S8d3us+Z94m7Gd27pjfAQHUZ1TkaIToMQnRYPXNkOWq0GfVvXHT+xe3OcK6tCfKTv/6Bg9xMRkYqUBDrilauNbkY1ilpq5EY/qbCWkNo5NbI9P7I5NY5baiQDqexc31HLTd02m4fb3a80qHF0fVfKWJa7uV+6zZW+F1zbHRO7NTe/Fp8+Pb5uLqEQnRZ7nh2LnU+Plu3yTIoJdztXTA2+rwERUYBytVMlMTrM/L3RzeW+lQRRcg/BUBUeQK4sBKr43Bb/iilJjlY6l49VS41sGfvXs7VfyW/WMu5RcqsU5ygpPCYiVIcRlzckK9tKxI4I1dkMjPwFgxoialRUWdnAzrlsnb5+KK/lkF63E4WdeAiKA6gwFR5OSm+lO/dctuVESfeT4pYa28c11MFOBWE7aFP6qxUHYEqG+LsSTCpJKK9na3LDQMCghogahYnd65rX7xnS1qPXibCx8OBHt/XFrDEdrIb0RroZXDjTXSHu6lKjpUYpxQ9GcUKvzCKclvvsnVtj43t755J7be8aDfvlt7uSU6NR8GtxZTSZM3lNknmUAiumYaIwETUOr93QA7cPbI3e6U1UO6f4r+rFN/XEG2v/lOTLiCXFhuMfI9qbX79wdRf8ea7cavI3Zzmz9lNNbcNDNsTO8gVKeTJRuOEa9nNcbLfUyHehWJdzXAdXE4VrXehaVHKr3J2wUY64pvZGi/k7BjVE1CiE6XVuBxBWRJ/3V/ds4dTK4bcObK1KFcZ0ScbHG0+gRZMInCmulC1THwTUiFpq3BlmbT6HwgeeO10Y8l18zrXUuNuF4uhwW/uVdz+Jz6UkSFV2Xq2L3UjS7ifFh/kFdj8RUaN3/aXJxbqkxjooKdWvtcpBkkLi59NjYy/Hout74OsHBtksX/+Qql8aQo1WGst6qFJO4TbpMhPylI8QUlLGtZYal7qfFJR3ZTSZo5/TVhDI7iciogDzt54tcFmzaNlp7e35x4h2aBoVKhk54g3iZ2V4iBbX9ZHO+NoyPgLTB7XB8z8cANDw1398VCi2PTUKkTbyfjzFnVYh2Qe4koQZG11U1nGGgpYRB3/+uxvUQEHLk6S0Szk1yrufLM6i7GJ+gkENETV6Go0G3dOaOH1ceIgOdw1uo36FnCAXMKTHR6KXjdyhZjFhsttdu7Zqp7I6X8PkezLllHQ/SVo/7Ix+UqWlRn67KwPbFNVHYZ+Qqy0uriYY+wN2PxERBRhbD6hpA9IBAA+P6uDS2k6+JhcEyAUUSrprpHkq8t/bO15aB/v7bbVEKc0TdjY4dK37yf4xtrufAiuqYUsNEVGQeOHqrnhifCdEh+khCAKu75OG5NhwVa9xZ0Yb/HfjcaTGhaODaF0iNYhjAHcfpbZGP1lPdqeku8fDQ7ol51IvUVjJ0Pd6knvvZI6PP2FQQ0QUYGw9KzUaDaLD9ObvX7lefni5O56Z3BlPT+oEk6BsOLm7XB25JE27sdP9pEId3M+pEdVH0Y+rsKXGxW4krRPBkL9hUENERE7RaDRQaQCVlEwQ4MyszbaOs9v9pCinxtF+G0GNwv4nSauSgvKuLJPgcFi6i8f5G+bUEBEFmEB70LjCnCjs6vGQDxQs4yY1FpC0NQuw5+apcT5R2BFJVQP4/cWghoiIvKZ+OPmssR2s9snFAC53P9lpnXHnXHJsrv3kwrWcWcvLEWm1lN8EV9eM8gfsfiIiCjCtE6J8XQVF5J6HL1zdFYPbJ8omMItbNpJi6vZrtRo8NKo9Pvj9L1QYal2siXtDuj2eKCwJwNxPXJYr59SQbhvfBwKfBzU5OTm49tprsW3bNtTU1ECvl6/S3LlzsWjRIkRHSyfHOnnyJMLC1Jt3gYjI37WMj8Tyu/ujaVSor6sioddqYDQJ+FvPVLRPjsH4rilWZTQa2B2Rteyu/iisMKBlfKR520OjOqBjcgxmfLoTABCib+hkEH9veR25763KKVkV28F+d9d+cnpIt8I+FldbqAJtGLeYT4OarVu34rrrrsP48eOxbds2h+VnzZqFuXPner5iRER+blC7RF9Xwcrqfw3DT3tzcfug1uZRWM4QIGBwe/mfa2yXFEwbkI5eLZsiNjwEcyZ2giAAseEhsuVt5dRYlVPh+W2z+0lxTo1zlXBlnhpnrhBgPU4SPg1q2rVrh4MHD2L79u348MMPfVkVIiJyU5vEKDxwZTuXj7cXBGi1Grz4t27m13cPaav4vPZaHtQJauS3uzKkW9n1XJlRWPkPGmgrc4v5NKhJSEjw6Pmrq6tRXV1tfl1aWurR6xERkX0RId6Z6VgcaMSG237UxUe634Xn9jw1TsYQai8kanVcAA8hCqiqL168GM2bN0d6ejomTpyIzMxMu+Xnz5+PuLg481fLli29U1EiIpJ4YvzlGN05GaM7J9ss01HFGYr1Oi3+c3tfvHNLbyRE2867TIoNx5Udmzl17tdv7IEBbRtWaLcVPDi/nKUyyrufxJPoKT9/4LbTBFBQM3PmTOTl5SE3NxfZ2dkYNmwYxo0bh40bN9o8Zvbs2SgpKTF/5eTkeLHGRERU775hl+HD2/pCr7N+7Hz/j8G4Y1BrPDO5s6rXHNkpGRO6NQcAXNasbsTY2C7WycvX9XHuD94pvdLw3FVdza9tde14qPfJpSHdznQpMVHYC5o2bWr+Pj4+Ho899hh++eUXLF++HBkZGbLHhIWFcWQUEZGf65YWh25pcR69xs8zh6K0qgaJMq02Y7okY3C7RPS2sbK5Xmb65A7J0RhxeRISFI5AG9I+EWPstFI5I9CWLvCmgAlq5FRVVSE+Pt5xQSIiatRC9VrZgAYAQnRaLLu7v81jk2LCMbVfOj7LOoW7B7cBUNea8d87rlB8/f+7y/b5nW0ZcaUlpbHEQX7b/TR16lTMmjXL/PrJJ5/EyZMnAdQlAL/88ss4fPgw7rvvPl9VkYiIGon513TDiZcnYs4kdbrIoi7NrDy8YzMXcmrkt4dadO2Fh+gwpnNdK1Ra0wgXahl4/Lal5siRI5KRS3FxcRg/fjwKCgpgNBoxePBgbNiwAS1atPBhLYmIiOQlx4Yhv7Radt/6WcORnVOMUZ2ScTi/zKnzzhrbEdM/ls7ttuqhoaisqcVz3+/HnIkNgdcHt/VVdE5xN5o4aAq0ri6NIHgqlcn/lJaWIi4uDiUlJYiNjfV1dYiIKIgdyS/D/J8O4uHRHdA9rYndsks3nUBKXLhsIrOcoosG7DhZhBd+PIDXbuiJPq2aOj7IDkEQ8NKPB9GlRSym9ErD7JV7UV1Ti9du7OnWedWi9PnNoIaIiIj8mtLnt9/m1BARERE5g0ENERERBQUGNURERBQUGNQQERFRUGBQQ0REREGBQQ0REREFBQY1REREFBQY1BAREVFQYFBDREREQYFBDREREQUFBjVEREQUFBjUEBERUVBgUENERERBgUENERERBQW9ryvgTYIgAKhbwpyIiIgCQ/1zu/45bkujCmrKysoAAC1btvRxTYiIiMhZZWVliIuLs7lfIzgKe4KIyWTC2bNnERMTA41Go9p5S0tL0bJlS+Tk5CA2Nla185IU77P38F57B++zd/A+e4+n7rUgCCgrK0Nqaiq0WtuZM42qpUar1SItLc1j54+NjeV/GC/gffYe3mvv4H32Dt5n7/HEvbbXQlOPicJEREQUFBjUEBERUVBgUKOCsLAwPPvsswgLC/N1VYIa77P38F57B++zd/A+e4+v73WjShQmIiKi4MWWGiIiIgoKDGqIiIgoKDCoISIioqDAoMZNVVVVmDFjBpo3b47k5GTcdNNNuHDhgq+rFXCysrIwYcIEJCUloXnz5hgxYgSys7MB1E2aOGfOHKSlpSEpKQnjx4/HiRMnJMe/8cYbaNOmDZKSkpCRkWE+lmxbsGABNBoNMjMzATh+Lyv5PZDUsWPHMGXKFDRv3hyJiYkYOHAgAL6n1bRt2zaMGzcOaWlpSE1NxbBhw7Bu3ToAvM/uysnJQb9+/aDRaGA0Gs3b1bivX3zxBS6//HIkJyejZ8+e5t+Z2wRyy9///ndh6NChQklJiVBVVSXceOONwpgxY3xdrYAzbNgw4bvvvhOMRqNQW1srPPHEE0J6erogCIIwb948oVOnTkJubq5gNBqFf/3rX0Lnzp2FmpoaQRAEYfny5UJycrJw+PBhQRAE4d///reQlJQkFBcX++zn8Xf79u0TunbtKrRo0UJYv369IAiO38uOfg8kderUKaFly5bCe++9J9TU1Agmk0n47bffBEHge1otxcXFQtOmTYWnnnpKMBgMQm1trfDGG28I4eHhwl9//cX77IYtW7YIaWlpwj333CMAkPw/d/e+btiwQYiKihI2b94sCIIgfPXVV0JkZKRw7Ngxt+vNoMYNxcXFQkhIiLBx40bztpycHAGAcODAAR/WLPBYPhj3798vABDy8vKE5ORk4dNPPzXvq6ioEKKjo4WffvpJEARBuOKKK4SXXnpJcny7du2Ed955x/MVD0A1NTVC3759hfXr1wutWrUS1q9f7/C9bDKZHP4eSGratGnCwoULrbYruZd8TyuzZcsWAYBVEBIVFSV8+eWXvM9uKCgoEMrKyoT169dLgho13r/XX3+9cM8990j2jxw5Unjsscfcrje7n9ywY8cOCIKAfv36mbelpaUhPT0dW7Zs8WHNAo9eL12xY/PmzUhOTkZ5eTny8/ORkZFh3hcREYHevXtjy5YtMBgM2LVrl2Q/AAwaNIi/Axteeukl9OvXD8OHDzdvc/RePn78uN3fA0nV1NRg5cqV6NatGwYNGoSkpCQMHz4c+/fvd3gv+Z5WrkePHujSpQteeOEFXLx4EQaDAQsXLkR8fDxat27N++yGhIQEREdHW21X4/27ZcsWq/0ZGRmq3HcGNW7Iz89HQkKC1QM5OTkZ+fn5PqpV4Dt69ChmzZqFRYsW4dy5cwDq7qlY/T2+cOECjEajzf0ktXPnTixbtgwLFiyQbHf0Xq6/l7zPyuTk5EAQBLz++uv47LPPcOLECQwcOBAjRoxATk4OAL6n1RAeHo5169Zhw4YNiI2NRXR0ND766COsX78eBoMBAO+z2hx9Fii5r/n5+R677wxq3GAymWRX+9ZqtTCZTD6oUeArKirCVVddhenTp2PatGnm+2h5n+vvsaP91MBgMOCOO+7Ae++9Z/UXmKP3Mu+zc/Ly8lBZWYkXX3wRrVq1QmRkJF544QWYTCb88ccfAPieVkNFRQVGjx6Nfv36obCwEMXFxZg+fTpGjBjBzw4PUeO+yn3eqHXfGdS4ISEhAcXFxRAsJmUuLCxEYmKij2oVuMrLyzF+/Hj06dMHr776KoC6ewzU3VOx+nscHx8PjUZjcz81eP755zFw4ECMHDnSap+j97Kj3wNJxcbGQqPRoFevXuZter0erVq1gk6nA8D3tBr+97//oaioCG+88Qbi4uIQGRmJ2bNnIz09HYsXLwbA+6w2NT6TExISPHbfGdS4oVevXjAYDNi/f795W2FhIY4dO4bevXv7sGaBp7KyEpMmTUJqaio+/vhjcxTfrl07xMXFYceOHeayRqMRu3btQu/evREREYHOnTtL9gN1Q8T5O5DKysrCZ599hiZNmpi/Tp06hUmTJuHhhx+2+1529Hsgqfbt2yMmJgbHjh0zbzMYDDh+/DhSU1P5nlZJUVERwsLCrP7qj4yMRGJiIu+zB6jxmdy3b1/P3Xe3U40buRtuuEEYNWqUUFxcLFRUVAi33HKL0K9fP19XK6BUV1cLY8eOFcaMGSNUV1db7X/ssceE7t27C2fPnhUMBoPw+OOPC61atRIqKioEQRCEd955R0hLSxMOHTok1NbWCm+99ZYQGxsr5ObmevtHCTj1o58EwfF72dHvgaTuv/9+YeTIkUJhYaFQWVkpPPTQQ0L79u2FqqoqvqdVcuDAASE8PFx44YUXzEO6//Of/wg6nU5Yu3Yt77MKLEc/CYL7n8k//fSTEBcXJ2zatEkwmUzCypUrhfDwcGH37t1u15dBjZtKSkqEW2+9VYiPjxeaNGkiTJkyRTh79qyvqxVQMjMzBQBCfHy8kJycLPlavXq1YDAYhAcffFBo1qyZ0KRJE2HEiBHCoUOHJOd4/vnnhdTUVCEuLk7o16+fef4Dsk8c1Dh6Lyv5PVCDiooK4f777xeaNWsmxMXFCRMnThSOHz8uCIKye8n3tDKZmZnC8OHDhZSUFCEuLk7o3bu3sHLlSkEQeJ/VIBfUqHFf33//faFNmzZCbGys0KVLF+H7779Xpb5cpZuIiIiCAnNqiIiIKCgwqCEiIqKgwKCGiIiIggKDGiIiIgoKDGqIiIgoKDCoISIioqDAoIaIiIiCAoMaIiIiCgoMaojIq5o0aYLMzEzJtkWLFqFv376y5adNm4b77rvPCzUjokDHoIaIVLNv3z6rxQXnzJmDadOmOTxWEARUVVVZfZlMJkXXFgQBV199NX788UeHZYuKijBr1iykp6cjJSUFHTt2xNtvvy0ps27dOvTs2RPJycno2LEjvvjiC/O+J554AgsXLlRULyLyHr2vK0BEwWfRokXm77ds2YKUlBSHx+zcuRMRERGy++69915F14yMjMTEiRMdll28eDFiY2ORnZ2N+Ph47Ny5E4MHD0anTp0wYsQI/PXXX5g8eTKWLVuGKVOmYPPmzRg9ejTS0tKQkZGBuXPnokePHhg6dCgGDBjg8HpE5B1sqSEi1aWkpJi/oqOjrfZfeeWV0Gg0km6o/v37Q6hbZFfydcsttzi83vnz5/HSSy/hmWeeUVS/OXPm4JlnnkF8fDwAoHfv3ujatSu2bdsGAHjvvfeQkZGBKVOmAAAGDhyIm266CW+88QYAIDw8HP/617/w8MMPK7oeEXkHgxoiUt20adPMX127drXa//3336OoqAiDBw82b9u6dSs0Go3V16effurwekuXLkWPHj3QqVMnAMDnn3+O5s2b49y5c+YyN954I26++WYAgF4vbaQuKirC4cOHzXXdsmULMjIyJGUyMjKwZcsW8+vbbrsN2dnZ2LFjh8P6EZF3MKghItWJc2Jqa2ut9kdHR6NJkybm4GLmzJkoKytDWVkZTp8+DQDIysoyb1u8eLHd63377bcYM2aM+fVNN92EqVOn4tZbb4UgCHjrrbdw8OBBfPTRR1bHCoKA22+/Hf379zd3XeXn5yM5OVlSLjk5Gfn5+ebXERERGDp0KL755htlN4WIPI45NUSkOsvcGFtdSLW1taipqQHQ0HpS/69OpzN/X59ErNPpEBISYnWew4cP45///Kdk28KFCzFy5EhMnz4dP//8MzZu3IjIyEirY2fPno1Dhw5h06ZN5m0mk8kq4Vmr1VolLXfo0AGHDh2S/dmIyPsY1BCRarp27WoOUsS0WvlG4ddffx2PPvqo7L4+ffpYbbv99tuxZMkSq+3l5eWIioqSbNPr9Vi4cCEGDBiAJ554Au3atbM67pVXXsGyZcuwYcMGJCYmmrcnJCSgsLBQUrawsFBSBqhrcSotLZWtPxF5H7ufiEhVer3e6stWUDNr1iwIggCDwYDKykqbXwaDAYIgyAY0AJCWlibpGgLqusDuv/9+zJo1C//5z3+QnZ0t2f/OO+9g0aJFWLNmDdq0aSPZ17dvX6tcmaysLPTu3VuyLTc3F+np6QruChF5A4MaIlLd559/jiFDhsju++WXX9CrVy/JtsmTJyMiIsLm16233mr3eoMGDcLmzZsl2x544AG0bdsWr7zyChYtWoRrr70WRUVFAIAlS5bg6aefxqpVq3D55Zdbne/ee+/FTz/9hG+//RaCIGDjxo3473//i/vvv19SbtOmTZJkZyLyLY0gCIKvK0FEwWXJkiVYtGgR9u3b5/a5nnjiCZw4cQKff/65zTK//vorbr/9dpw8eRKhoaH46KOPMG/ePOzatQtxcXEAgOnTp+PcuXP44Ycf0LZtW+Tn5yM2NlZynsmTJ+PDDz8EUJd8PHv2bJw5cwaJiYl46qmncOedd5rL7t27FwMGDMDZs2fN1yAi32JQQ0SqW7JkCRYuXIisrCzZ/aGhoQgNDVV0LiVBDQAMHToUt99+O+666y6n6+uK22+/Ha1atcLzzz/vlesRkWPsfiIijzh48CBiYmJkv5ROkgcAUVFRVi0qct577z0sXLgQxcXFbtRama1bt2L37t144oknPH4tIlKOLTVEFDQyMzMRGhqKQYMGefQ6n332GQYMGGCVYExEvsWghoiIiIICu5+IiIgoKDCoISIioqDAoIaIiIiCAoMaIiIiCgoMaoiIiCgoMKghIiKioMCghoiIiIICgxoiIiIKCgxqiIiIKCj8P7EsvdzOmXk7AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding 계층 구현 (가중치 행렬에서 특정 단어 ID에 해당하는 행을 추출)\n",
        "import numpy as np\n",
        "W = np.arange(15).reshape(5, 3)\n",
        "print(W, \"\\n\")\n",
        "print(W[2], \"\\n\") # 2번째 행을 추출\n",
        "index_list = [1, 3, 4]\n",
        "print(W[index_list])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJBQdZTwT0IV",
        "outputId": "f00e6716-3618-4895-aced-726ee603c583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2]\n",
            " [ 3  4  5]\n",
            " [ 6  7  8]\n",
            " [ 9 10 11]\n",
            " [12 13 14]] \n",
            "\n",
            "[6 7 8] \n",
            "\n",
            "[[ 3  4  5]\n",
            " [ 9 10 11]\n",
            " [12 13 14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding: # MatMul 대신에 Embedding 계층을 사용함으로써 시간/공간 복잡도를 모두 줄인다!\n",
        "  def __init__(self, W): # 초기화 함수\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.idx = None\n",
        "\n",
        "  def forward(self, idx): # 순전파\n",
        "    W, = self.params\n",
        "    self.idx = idx\n",
        "    out = W[idx]\n",
        "    return out\n",
        "\n",
        "  def backward_bad(self, dout): # 역전파 (불완전)\n",
        "    dW, = self.grads\n",
        "    dW[...] = 0 # 우선 모두 0으로 초기화\n",
        "    dW[self.idx] = dout # 특정 행에만 기울기 전달 ('할당')\n",
        "    return None\n",
        "\n",
        "  def backward(self, dout): # 역전파 (idx에 중복된 원소가 존재할 수 있으므로 이를 핸들링)\n",
        "    dW, = self.grads\n",
        "    dW[...] = 0\n",
        "    for i, word_id in enumerate(self.idx):\n",
        "      dW[word_id] += dout[i] # 이렇게 덧셈을 해주면 된다!\n",
        "    return None\n",
        "    # for문 대신에 그냥 np.add.at(dW, self.idx, dout)으로 해결할 수도 있다!\n",
        "    # dW의 self.idx 인덱스(행)에 dout 벡터를 더해준다."
      ],
      "metadata": {
        "id": "c-Aci245T0BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding 계층과 Dot 계층을 합치기\n",
        "class EmbeddingDot:\n",
        "  def __init__(self, W):\n",
        "    self.embed = Embedding(W)\n",
        "    self.params = self.embed.params\n",
        "    self.grads = self.embed.grads\n",
        "    self.cache = None # 순전파 당시 계산 결과 임시 저장용\n",
        "\n",
        "  def forward(self, h, idx):\n",
        "    target_W = self.embed.forward(idx)\n",
        "    out = np.sum(target_W * h, axis=1) # 내적 계산을 이렇게 표현 가능\n",
        "    self.cache = h, target_W\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    h, target_W = self.cache\n",
        "    dout = dout.reshape(dout.shape[0], 1)\n",
        "    dtarget_W = dout * h\n",
        "    self.embed.backward(dtarget_W)\n",
        "    dh = dout * target_W\n",
        "    return dh"
      ],
      "metadata": {
        "id": "au9tdOgZTz5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 변형된 확률분포 in Negative Sampling 예시\n",
        "import numpy as np\n",
        "\n",
        "p = [0.63, 0.29, 0.08]\n",
        "p_ = np.power(p, 0.75)\n",
        "p_ /= np.sum(p_)\n",
        "print(p_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR9p5kBfgic3",
        "outputId": "8775d348-27bc-4058-c615-2bbbd73b46a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.56447137 0.31545325 0.12007539]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NegativeSamplingLoss:\n",
        "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
        "        self.sample_size = sample_size\n",
        "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
        "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.embed_dot_layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, h, target):\n",
        "        batch_size = target.shape[0]\n",
        "        negative_sample = self.sampler.get_negative_sample(target)\n",
        "\n",
        "        # 긍정적 예 순전파\n",
        "        score = self.embed_dot_layers[0].forward(h, target)\n",
        "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
        "        loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "        # 부정적 예 순전파\n",
        "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "        for i in range(self.sample_size):\n",
        "            negative_target = negative_sample[:, i]\n",
        "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
        "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dh = 0\n",
        "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "            dscore = l0.backward(dout)\n",
        "            dh += l1.backward(dscore)\n",
        "\n",
        "        return dh"
      ],
      "metadata": {
        "id": "WADcSczEhdyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW:\n",
        "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "        V, H = vocab_size, hidden_size\n",
        "\n",
        "        # 가중치 초기화\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.in_layers = []\n",
        "        for i in range(2 * window_size):\n",
        "            layer = Embedding(W_in)  # Embedding 계층 사용\n",
        "            self.in_layers.append(layer)\n",
        "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "\n",
        "        # 모든 가중치와 기울기를 배열에 모은다.\n",
        "        layers = self.in_layers + [self.ns_loss]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h = 0\n",
        "        for i, layer in enumerate(self.in_layers):\n",
        "            h += layer.forward(contexts[:, i])\n",
        "        h *= 1 / len(self.in_layers)\n",
        "        loss = self.ns_loss.forward(h, target)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.ns_loss.backward(dout)\n",
        "        dout *= 1 / len(self.in_layers)\n",
        "        for layer in self.in_layers:\n",
        "            layer.backward(dout)\n",
        "        return None"
      ],
      "metadata": {
        "id": "AeziQhBxhd1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구현한 CBOW 클래스를 기반으로 학습 진행\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common import config\n",
        "import numpy as np\n",
        "import pickle\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from common.util import create_contexts_target\n",
        "from common.layers import Embedding, SigmoidWithLoss\n",
        "from dataset import ptb\n",
        "import collections\n",
        "\n",
        "class UnigramSampler:\n",
        "    def __init__(self, corpus, power, sample_size):\n",
        "        self.sample_size = sample_size\n",
        "        self.vocab_size = None\n",
        "        self.word_p = None\n",
        "\n",
        "        counts = collections.Counter()\n",
        "        for word_id in corpus:\n",
        "            counts[word_id] += 1\n",
        "\n",
        "        vocab_size = len(counts)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.word_p = np.zeros(vocab_size)\n",
        "        for i in range(vocab_size):\n",
        "            self.word_p[i] = counts[i]\n",
        "\n",
        "        self.word_p = np.power(self.word_p, power)\n",
        "        self.word_p /= np.sum(self.word_p)\n",
        "\n",
        "    def get_negative_sample(self, target):\n",
        "        batch_size = target.shape[0]\n",
        "        negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            p = self.word_p.copy()\n",
        "            target_idx = target[i]\n",
        "            p[target_idx] = 0\n",
        "            p /= p.sum()\n",
        "            negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
        "\n",
        "        return negative_sample\n",
        "\n",
        "# Hyperparameters Setting\n",
        "W, H, N, max_epoch = 5, 100, 100, 10 # window size, hidden size, batch size\n",
        "\n",
        "# Data importing\n",
        "corpus, word2id, id2word = ptb.load_data('train')\n",
        "V = len(word2id) # vocabulary size\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, W)\n",
        "model = CBOW(V, H, W, corpus)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "trainer.fit(contexts, target, max_epoch, N) # 본격적인 Training\n",
        "trainer.plot()\n",
        "\n",
        "# 추후에 사용이 가능하도록 데이터를 저장\n",
        "word_vecs = model.word_vecs\n",
        "params = {}\n",
        "params['word_vecs'] = word_vecs.astype(np.float16) # 16비트 부동소수점\n",
        "params['word2id'] = word2id\n",
        "params['id2word'] = id2word\n",
        "pkl_file = 'cbow_params.pkl'\n",
        "with open(pkl_file, 'wb') as f:\n",
        "  pickle.dump(params, f, -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Q6ztuoRpiXJQ",
        "outputId": "4d1f2465-5e83-4872-f407-ef64cb0ae6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ab23c3fb8d31>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_contexts_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-df272cd29b15>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, hidden_size, window_size, corpus)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_in\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Embedding 계층 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNegativeSamplingLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# 모든 가중치와 기울기를 배열에 모은다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2506ad6a359e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, W, corpus, power, sample_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnigramSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSigmoidWithLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dot_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEmbeddingDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2506ad6a359e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnigramSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSigmoidWithLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dot_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEmbeddingDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EmbeddingDot' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CBOW 모델 평가\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.util import most_similar\n",
        "import pickle\n",
        "\n",
        "pkl_file = 'cbow_params.pkl'\n",
        "\n",
        "with open(pkl_file, 'rb') as f:\n",
        "  params = pickle.load(f)\n",
        "  word_vecs = params['word_vecs']\n",
        "  word2id = params['word2id']\n",
        "  id2word = params['id2word']\n",
        "\n",
        "querys = ['I', 'like', 'school']\n",
        "for query in querys:\n",
        "  most_similar(query, word2id, id2word, word_vecs, top=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-gkhjs7iXhc",
        "outputId": "04afffa2-b656-4fff-d507-cb98e1aac270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I(을)를 찾을 수 없습니다.\n",
            "\n",
            "[query] like\n",
            " vicious: 0.54638671875\n",
            " mirror: 0.533203125\n",
            " beat: 0.51904296875\n",
            "\n",
            "[query] school\n",
            " university: 0.6103515625\n",
            " college: 0.5947265625\n",
            " yale: 0.58984375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch 튜토리얼\n",
        "https://tutorials.pytorch.kr/beginner/basics/quickstart_tutorial.html"
      ],
      "metadata": {
        "id": "oH8uXP-LAKDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "파이토치(PyTorch)에는 데이터 작업을 위한 기본 요소로 두가지가 있습니다.\n",
        "\n",
        "torch.utils.data.Dataset 그리고 torch.utils.data.DataLoader\n",
        "\n",
        "Dataset 은 샘플과 정답(label)을 저장하며,\n",
        "DataLoader 는 Dataset 을 iterable(순회 가능한) 객체로 감쌉니다.\n",
        "'''"
      ],
      "metadata": {
        "id": "CAdiWgu2k7IY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "39f49e04-1b39-443d-81d6-420732daa06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n파이토치(PyTorch)에는 데이터 작업을 위한 기본 요소로 두가지가 있습니다.\\n\\ntorch.utils.data.Dataset 그리고 torch.utils.data.DataLoader\\n\\nDataset 은 샘플과 정답(label)을 저장하며,\\nDataLoader 는 Dataset 을 iterable(순회 가능한) 객체로 감쌉니다.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn # import torch.nn 이렇게 하면 nn 사용할 때 계속 torch.nn이라고 명시해줘야 함 (귀찮)\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# import만 사용하면 모듈 안의 함수를 사용할 때 모듈명.함수명( )으로 하고,\n",
        "# from을 사용하면 바로 함수명( )으로 해당 함수를 사용가능"
      ],
      "metadata": {
        "id": "XVk5bj05ArM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(torch), type(nn), type(DataLoader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em2kfQhtBB6R",
        "outputId": "8088be68-2651-4fac-ed6e-c602eee1ffad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'module'> <class 'module'> <class 'type'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch는 TorchText, TorchVision, TorchAudio 와 같이 도메인 특화 라이브러리를 데이터셋과 함께 제공하고 있습니다.\n",
        "# 이 튜토리얼에서는 TorchVision 데이터셋을 사용하도록 하겠습니다.\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor # ToTensor() 함수를 사용하기 위해 이를 불러옵니다.\n",
        "\n",
        "# torchvision.datasets 모듈은 CIFAR, COCO 등과 같은 다양한 실제 비전(vision) 데이터에 대한 Dataset을 포함하고 있습니다.\n",
        "# 이 튜토리얼에서는 FasionMNIST 데이터셋을 사용합니다.\n",
        "# 모든 TorchVision Dataset 은 샘플과 정답을 각각 변경하기 위한 \"transform\" 과 \"target_transform\" 의 두 인자를 포함합니다."
      ],
      "metadata": {
        "id": "j49zsryEBsCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(datasets.FashionMNIST)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gxLBtmQC_0e",
        "outputId": "705797db-39d4-4492-dae6-b12eb7f892e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class FashionMNIST in module torchvision.datasets.mnist:\n",
            "\n",
            "class FashionMNIST(MNIST)\n",
            " |  FashionMNIST(root: str, train: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None\n",
            " |  \n",
            " |  `Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ Dataset.\n",
            " |  \n",
            " |  Args:\n",
            " |      root (string): Root directory of dataset where ``FashionMNIST/raw/train-images-idx3-ubyte``\n",
            " |          and  ``FashionMNIST/raw/t10k-images-idx3-ubyte`` exist.\n",
            " |      train (bool, optional): If True, creates dataset from ``train-images-idx3-ubyte``,\n",
            " |          otherwise from ``t10k-images-idx3-ubyte``.\n",
            " |      download (bool, optional): If True, downloads the dataset from the internet and\n",
            " |          puts it in root directory. If dataset is already downloaded, it is not\n",
            " |          downloaded again.\n",
            " |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
            " |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
            " |      target_transform (callable, optional): A function/transform that takes in the\n",
            " |          target and transforms it.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      FashionMNIST\n",
            " |      MNIST\n",
            " |      torchvision.datasets.vision.VisionDataset\n",
            " |      torch.utils.data.dataset.Dataset\n",
            " |      typing.Generic\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {}\n",
            " |  \n",
            " |  __parameters__ = ()\n",
            " |  \n",
            " |  classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'San...\n",
            " |  \n",
            " |  mirrors = ['http://fashion-mnist.s3-website.eu-central-1.amazonaws.com...\n",
            " |  \n",
            " |  resources = [('train-images-idx3-ubyte.gz', '8d4fb7e6c68d591d4c3dfef9e...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from MNIST:\n",
            " |  \n",
            " |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
            " |      Args:\n",
            " |          index (int): Index\n",
            " |      \n",
            " |      Returns:\n",
            " |          tuple: (image, target) where target is index of the target class.\n",
            " |  \n",
            " |  __init__(self, root: str, train: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |  \n",
            " |  download(self) -> None\n",
            " |      Download the MNIST data if it doesn't exist already.\n",
            " |  \n",
            " |  extra_repr(self) -> str\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from MNIST:\n",
            " |  \n",
            " |  class_to_idx\n",
            " |  \n",
            " |  processed_folder\n",
            " |  \n",
            " |  raw_folder\n",
            " |  \n",
            " |  test_data\n",
            " |  \n",
            " |  test_labels\n",
            " |  \n",
            " |  train_data\n",
            " |  \n",
            " |  train_labels\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from MNIST:\n",
            " |  \n",
            " |  test_file = 'test.pt'\n",
            " |  \n",
            " |  training_file = 'training.pt'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
            " |  \n",
            " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
            " |  \n",
            " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __class_getitem__(params) from builtins.type\n",
            " |  \n",
            " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
            " |      This method is called when a class is subclassed.\n",
            " |      \n",
            " |      The default implementation does nothing. It may be\n",
            " |      overridden to extend subclasses.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 공개 데이터셋에서 학습 데이터를 내려받습니다.\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True, # Train Data\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# 공개 데이터셋에서 테스트 데이터를 내려받습니다.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # Test Data\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHydUpk9CqY8",
        "outputId": "dac7a72c-1d91-44ac-f94e-a93279b0f703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 13208732.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 209058.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3864263.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 8391868.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 다운받은 Dataset은 DataLoader 의 인자로 전달합니다.\n",
        "# 데이터셋을 iterable객체로 감싸고, 자동화된 \"배치\", \"샘플링\", \"셔플\" 및 \"다중 프로세스로 데이터 불러오기\"를 지원합니다.\n",
        "# 본 튜토리얼에서는 배치 크기(batch size)를 32로 정의하겠습니다.\n",
        "# 이말은 즉슨, 데이터로더 객체의 각 요소는 32개의 특징(feature)과 정답(label)을 묶음(batch)으로 반환합니다.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# 데이터로더를 생성합니다.\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\") # Batch, Channel, Height, Width\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-QKVDNZCqOx",
        "outputId": "84cfad5d-3430-45d6-91be-28302fc6b383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([32, 1, 28, 28])\n",
            "Shape of y: torch.Size([32]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(X[0][0], cmap='gray') # 0번째 인덱스에 담긴 사진 정보 표시하기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "4MlihrPOE7pz",
        "outputId": "695718f3-4bac-46f5-977e-d4e95b7d227b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7e1c5c605c60>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch에서 신경망 모델은 nn.Module 을 상속받는 클래스(class)를 생성하여 정의합니다. (매우 중요)\n",
        "# __init__ 함수에서는 신경망의 계층(layer)들을 정의합니다.\n",
        "# forward 함수에서는 정의된 신경망에 데이터를 어떻게 전달할지 지정합니다.\n",
        "# 가능한 경우, GPU 또는 MPS로 신경망을 이동시켜 연산을 가속(accelerate)시키는 것을 권장합니다.\n",
        "\n",
        "# 학습에 사용할 CPU나 GPU, MPS 장치를 얻습니다. (GPU 최우선, 그다음 MPS, CPU)\n",
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\" # 삼항연산 두번 활용\n",
        ")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 모델을 정의합니다. (nn.Module 상속 필수!!)\n",
        "class NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() # 이것도 필수!!\n",
        "        # 이하 신경망 정의\n",
        "        self.flatten = nn.Flatten() # 1D화\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512), # 선형회귀 모델\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        # Logit 함수(혹은 log-odds 함수)는 0에서 1까지의 확률값과 -∞에서 ∞ 사이의 확률값을 표현합니다.\n",
        "        return logits\n",
        "\n",
        "model = NN().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "zuendSySFfNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CH 04"
      ],
      "metadata": {
        "id": "0y3Z_6k_ANDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5장 순환 신경망(RNN)"
      ],
      "metadata": {
        "id": "bwi-04QhBZ27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 한 단계 RNN 연산\n",
        "import numpy as np\n",
        "\n",
        "# Shape 정리\n",
        "# h_prev:(N*H), Wh:(H*H), x:(N*D), Wx:(D*H), h_next:(N*H)\n",
        "\n",
        "class RNN:\n",
        "  def __init__(self, Wx, Wh, b):\n",
        "    self.params = [Wx, Wh, b]\n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "    self.cache = None # 역전파 계산을 위해 값들을 임시로 저장해두기 위한 변수\n",
        "\n",
        "  def forward(self, x, h_prev):\n",
        "    Wx, Wh, b = self.params\n",
        "    t = h_prev@Wh + x@Wx + b # temporary\n",
        "    h_next = np.tanh(t)\n",
        "    self.cache = (x, h_prev, h_next)\n",
        "    return h_next\n",
        "\n",
        "  def backward(self, dh_next):\n",
        "    Wx, Wh, b = self.params\n",
        "    x, h_prev, h_next = self.cache\n",
        "\n",
        "    dt = dh_next * (1-h_next**2) # tanh 미분; y=tanh(x) = 1-y**2\n",
        "    db = np.sum(dt, axis=0)\n",
        "    dWh = np.matmul(h_prev.T, dt)\n",
        "    dh_prev = dt @ Wh.T\n",
        "    dWx = x.T @ dt\n",
        "    dx = dt @ Wx.T\n",
        "\n",
        "    self.grads[0][...], self.grads[1][...], self.grads[2][...] = dWx, dWh, db\n",
        "\n",
        "    return dx, dh_prev"
      ],
      "metadata": {
        "id": "3UxT8OjnUI7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Time RNN 계층: T개의 RNN 계층으로 구성된다. (T는 하이퍼파라미터)\n",
        "\n",
        "class TimeRNN:\n",
        "\n",
        "  # 초기화 함수\n",
        "  def __init__(self, Wx, Wh, b, stateful=False):\n",
        "    self.params = [Wx, Wh, b]\n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "    self.layers = None\n",
        "    self.h, self.dh = None, None\n",
        "    self.stateful = stateful\n",
        "\n",
        "  def set_state(self, h): self.h = h\n",
        "  def reset_state(self): self.h = None\n",
        "\n",
        "  # 순전파\n",
        "  def forwad(self, xs):\n",
        "    Wx, Wh, b = self.params\n",
        "    N, T, D = xs.shape\n",
        "    D, H = Wx.shape\n",
        "\n",
        "    self.layers = []\n",
        "    hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "    if not self.stateful or self.h is None: self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "    for t in range(T):\n",
        "      layer = RNN(*self.params) # 리스트 내부 아이템 끄집어 내서 전달\n",
        "      self.h = layer.forward(xs[:, t, :], self.h) # t 시각\n",
        "      hs[:, t, :] = self.h\n",
        "      self.layers.append(layer)\n",
        "\n",
        "    return hs\n",
        "\n",
        "  # 역전파\n",
        "  def backward(self, dhs):\n",
        "    Wx, Wh, b = self.params\n",
        "    N, T, H = dhs.shape\n",
        "    D, H = Wx.shape\n",
        "    dxs = np.empty((N, T, D), dtype='f')\n",
        "    dh, grads = 0, [0]*3\n",
        "    for t in range(T-1,-1,-1):\n",
        "      layer = self.layers[t]\n",
        "      dx, dh = layer.backward(dhs[:,t:]+dh) #dh_t + dh_next\n",
        "      dxs[:, t, :] = dx\n",
        "      for i, grad in enumerate(layer.grads):\n",
        "        grads[i] += grad\n",
        "\n",
        "    for i, grad in enumerate(grads):\n",
        "      self.grads[i][...] = grad\n",
        "\n",
        "    self.dh = dh\n",
        "    return dxs"
      ],
      "metadata": {
        "id": "FhPe-DjZBbvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '밑바닥부터 시작하는 딥러닝2'의 ch05/simple_rnnlm.py\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.time_layers import *\n",
        "\n",
        "\n",
        "class SimpleRnnlm:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # 가중치 초기화\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
        "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
        "        rnn_b = np.zeros(H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
        "            TimeAffine(affine_W, affine_b)\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.rnn_layer = self.layers[1]\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        loss = self.loss_layer.forward(xs, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.rnn_layer.reset_state()"
      ],
      "metadata": {
        "id": "zDsqeDOelM4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import"
      ],
      "metadata": {
        "id": "jR_KhG2eAIHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.array([1])\n",
        "b +"
      ],
      "metadata": {
        "id": "ap2tyexNxC-w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}